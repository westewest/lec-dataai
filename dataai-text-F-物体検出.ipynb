{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxtxhDSWtMykiW7DQPXOJP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n",">「見えるぞ！私にも敵が見える！」\\\n",">シャア・アズナブル\n","---"],"metadata":{"id":"kIeTGMf-5FLz"}},{"cell_type":"markdown","source":["# 物体検出\n"],"metadata":{"id":"L60c_B8JzPsZ"}},{"cell_type":"markdown","source":["## 物体検出手法\n","\n","画像の中の複数の物体を認識し、その場所をBounding Boxと呼ばれる四角で示す\n","- さらにセグメンテーションという手法もあり、こちらは塗り分ける\n","\n","物体検出手法には著名な手法が多く、シンプルにまとめると次のようになる\n","\n","- Yolo\n","  - 一連の物体検出手法YOLO（You Only Look Once）の1つで、v2と比較して特に小さいものの検出精度が改善された\n","  - Yolo V1からYolo V3まで同一作者により設計され、その後は異なる作者によりYolo V4およびYolo V5が提案されている\n","    - 従って、Yolo V3までがYoloという研究者もいる\n","  - 傾向として、FPS(認識フレームレート)が速く、その割に精度が高いことを特徴としている\n","  - 小さい解像度から大きい解像度に情報を伝搬して物体を検出する構造を有する\n","\n","- SSD\n","  - YOLOと並び称されることが多いが、SSDではそれぞれの解像度から直接物体を検出する構造を有する\n","  - 構造的にはSSDもYOLO V3も大きな違いはなく、SSDは情報を一方向のみにしか伝えていないが、YoloV3はGlobal情報をLocal情報の組み合わせを実現している\n","\n","論文記載のそれぞれのネットワーク構造は次のとおりである\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/ssd-yolo.png\" width=500>\n","\n","ここでは、YoloV3を扱い、実際に実装する\n"],"metadata":{"id":"6FAe2Jv6WpaL"}},{"cell_type":"markdown","source":["# YoLoの設計"],"metadata":{"id":"yc1lcfMf6STm"}},{"cell_type":"markdown","source":["## YoLoの原理\n","\n","画像ごと異なる対象物体の大きさの違いに対応するため、小スケール(Scale1)$13 \\times 13$、中スケール$26 \\times 26$、大スケール$52 \\times 52$の3つの対象の異なる3次元テンソルを得るように学習する\n","- 例えば、図のScale 1に相当する$13 \\times 13$のスケールでは、$13 \\times 13$であることから入力画像を$13\\times 13$のグリッドで分割する\n","- さらに各グリッドで3つのBoxを予測するため、これをBox1, Box2, Box3とする\n","- 各Boxで得られる(得るべき)情報は、Bounding Boxやラベルなどの情報である\n","  - Bouding boxは、$(tx, ty, tw, th)$で要素数4\n","  - 検出のObjectness Score(検出対称であるかどうかを表すフラグ)で要素数1\n","  - 80クラス分類でワンホットのため要素数80\n","- Box一つで$3\\times(4+1+80)$つまり255次元の情報が獲得され、これが$13 \\times 13$個存在するため、結果的に$13 \\times 13 \\times 255$となる\n","- 他も同様である\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yololearn.png\" width=800>\n","\n","上図において、仮にGround TruthのBouding Boxの中心があるグリッドセルの中、例えば、鳥の画像の赤いBoxにあったばあい、該当する物体のBoxの予測はこの赤いグリッドセルが担当する\n","- つまり、同じグリッドセルは3つの物質以上を担当できないという制限がある\n","- Ground Truth とは学習やテストに使用する実データのこと\n","\n","赤いセルは鳥のBoxを予測することになり、Objectness Scoreが1に、ほかのセルは0になる\n","- 各セルは予めサイズの異なる3つのPrior Box(S, M, L)が割り当てられており、Ground Truthと一番大きさの近いPrior Boxを選択する\n","  - 実際は、Ground truthであるBounding Boxと重なった面積が一番大きいPrior Boxを選択する\n","- 更に座標オフセットの調整も行う"],"metadata":{"id":"1nEH4XeruUIL"}},{"cell_type":"markdown","source":["## Yoloのモデル\n","\n","### 全体の構造\n","まず、YoLoV3のモデル構造を示す\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yolov3.png\" witdh=600>\n","\n","このように、主に75層の畳み込み層から構成されており、先に述べた物体の特徴量を抽出する\n","- Fully connected層を利用していないため、入力画像のサイズを任意に設定できる\n","- Pooling Layerを用いず、Strideが2の畳み込み層を用いることでダウンサンプリングを行う\n","  - スケール不変な特徴量を次の層までへ伝播できる\n","- ResNetとFPN構造を利用して検出精度を向上している\n","  - 図のResidual BlockがResNet構造に相当し、Shortcut Pathを有する構造である\n","  - 層が深くなると学習が困難となるが、Shortcut Pathによりある層における最適な出力を学習するのではなく、前層の入力を参照した残差関数を学習するようになり、特徴量の学習をしやすくなる\n","  - これにより、複雑な特徴量H(x)が古い特徴量xに新しく学習した残差F(x)を足し合わせる形で構成される(階差数列と似た概念)\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yoloresnet.png\" width=500>\n","\n","### 異なるスケールへの対応\n","特徴マップはネットワークの深さとともに縮小するため、小さい物体の検出もそれに呼応して難易度が上がる\n","- 直感的に大きさによって異なる段階で物体を検出するとよいと考えるのがSSD\n","\n","しかしながら、各特徴マップにおける特徴量は深さによって変化しているため、そう簡単ではない\n","- 最初は低レベルなエッジ、カラー、物体の大まかな位置などの特徴量を学習しているが、層の深くなるにつれてハイレベルな物体情報である犬、猫、車といった特徴量の学習に変化するため、検出にはどうしてもハイレベルな特徴量が必要となる\n","- これがSSDの弱点\n","\n","そこで、YOLOv3では、スケール対策としてFeature Pylamid Network(FPN)構造を利用する\n","- 全体ネットワーク図において、Conv Blockの$13\\times 13$から、$26 \\times 26$にConcatenateするパスや、さらにそれを$52 \\times 52$にConcatenateするパスが存在するなど、大から小のパスと、小から大の双方向のパスが存在する\n","\n","### Softmaxを利用しない\n","\n","多値分類では、ワンホット化しSoftmaxを利用する場合が殆どである\n","- しかしながら、分類数が多くなると例えばwomanとpersonのようにラベルが重なることがあり、完全にワンホットにすることが困難となる場合も想定される\n","- そこで、logistic関数を利用している"],"metadata":{"id":"-xbyVz9RW47W"}},{"cell_type":"markdown","source":["# YoLoの実装"],"metadata":{"id":"J08CAylYHfX4"}},{"cell_type":"markdown","source":["## データの準備\n","\n","今回は、COCO128を利用する\n","- データセットとしてはそれほど大きくない"],"metadata":{"id":"Ko6azS8vHjAK"}},{"cell_type":"code","source":["import os\n","if not os.path.exists('COCO128.zip'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=141bVQDo-x421CMN0o9vS3SkIY75kbi54\" -O COCO128.zip\n","  !wget https://keio.box.com/shared/static/64r487vugf8l385r1lflmp35g0zd63nv -O COCO128.zip\n","  !unzip COCO128.zip"],"metadata":{"id":"N7fE2yGJrUXu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692094143256,"user_tz":-540,"elapsed":3208,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"5413e85a-d319-438a-ccf3-b5296d261907"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-15 10:08:59--  https://keio.box.com/shared/static/64r487vugf8l385r1lflmp35g0zd63nv\n","Resolving keio.box.com (keio.box.com)... 74.112.186.144\n","Connecting to keio.box.com (keio.box.com)|74.112.186.144|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/64r487vugf8l385r1lflmp35g0zd63nv [following]\n","--2023-08-15 10:09:00--  https://keio.box.com/public/static/64r487vugf8l385r1lflmp35g0zd63nv\n","Reusing existing connection to keio.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://keio.app.box.com/public/static/64r487vugf8l385r1lflmp35g0zd63nv [following]\n","--2023-08-15 10:09:00--  https://keio.app.box.com/public/static/64r487vugf8l385r1lflmp35g0zd63nv\n","Resolving keio.app.box.com (keio.app.box.com)... 74.112.186.144\n","Connecting to keio.app.box.com (keio.app.box.com)|74.112.186.144|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!swPQbjRRhXbmFutncWD_8PfTZX0WOzm8PHEOPE91jMWcBKnCLjz4Sx2rguWhU8m21ZIQDXlL932mPWopmcYBygLNth7EFEwEGF48zBz1LirizVjdfFUiSCLuAXdJoo97jBuZMJWr42_BJ9YqDoFRyGnTUyisBYWNjzMo12z_ihZ9RiqSBarS0vb50SENdKdnP8bBV09JPxhDUKrAqPKyVUb3Y-q1LsM1a0F4ZiZvrYUg_bSqcMydPveHVbqI6VNLYX2fMGKMF5TVEFyZ__vMitCyVNIx1n-rrJ18gLq1XH96mJlk8Ml3MFg_4Vv8F4iTRkeAaPUFXogD7ak64_sIaW_bitRZFCSRH7G7NXTmLYA8qUSXR3COHW8kpPRp0U16uAKGyEgZbIJdeocPA4T20U3zC3wBRZ5fec-QSGMKsbjJNMUksLNdouxwlBw5YBauR6X0Rrn6r8EColwRatu7VkwEiS5dSjbStv7qcCRy0Xtg3wjapHHGIERfX32AmgSw3KKVpOTkfc1Gjd7mRyP3jNbeXBZsYeChArC16x-cAhXp1sS7O_ZPoMcBxmpirBu-PnWIlA0qCYnMbM4oqNfSodwHCYhureuqr5Ecd9KTIAfYsFaGfjack2nH2dWRl8LVZ4stH86Jbg5D8a3H0FvbOEc9JbfbQxDfppRzIDBKPaZs2S9j_kv7VkORPogC9nXNEfetj4ZY9kI0kM81cOy7YnU65JHzLegn88mfyaRJhZanAxZf49RBgXi-MIim27h94z8grp1w1DnbshGPGJjxhaPTfr1U1F5SWdVJybopBgzXWay_fd-MxhY9BdsJoPS5_Dq2ThgLzzee7geYpEcwRKu9cjkKNhg8RwFH0qaPlJnktkHVhVXQ2b776Kplan6ut8zv-gc_8357zgUsGflmw4beFkJwsQpjGLvF-rVLTd29x0SCJo_TmAJfcgLEgVbZ4-Cdn0RG2Lm-LmHhtsmSYDRsqbZP3lkkNwU5osGMCeVfIT9MoZMZW2_LRcnZi4L188N7lZ3nmNKKweZKrGA-Gk4liGbkD0D-CB02sAMDa3VphxYHrvdO0S7G49T9fBPcMoB6se1KwjOvvbLiSFZziQ4FberzfvS0RudIJW_Egp6hL43M_jxY5D3RVKRryzFi6ZBZvZbLA29-kOi2EBtGKmSLnyIDs1aI_Jm8mdX0kYBja_9b98BYXDi46BOp47YbS6EKoNdpug-AANSzVholOyImB0_ZeaQZmTqnS2XM4pijONkJ7KgQLAqYCI_TPnKfpOcoG-mBwU9eERIprZ4bWgsdiPdAEIK1Z7MigSlHPZpT5TSeYu99Jjm_g649RdBo2dHTwvNW3ZHbiduzSOcuYxpHUYXN47Klarmo/download [following]\n","--2023-08-15 10:09:00--  https://public.boxcloud.com/d/1/b1!swPQbjRRhXbmFutncWD_8PfTZX0WOzm8PHEOPE91jMWcBKnCLjz4Sx2rguWhU8m21ZIQDXlL932mPWopmcYBygLNth7EFEwEGF48zBz1LirizVjdfFUiSCLuAXdJoo97jBuZMJWr42_BJ9YqDoFRyGnTUyisBYWNjzMo12z_ihZ9RiqSBarS0vb50SENdKdnP8bBV09JPxhDUKrAqPKyVUb3Y-q1LsM1a0F4ZiZvrYUg_bSqcMydPveHVbqI6VNLYX2fMGKMF5TVEFyZ__vMitCyVNIx1n-rrJ18gLq1XH96mJlk8Ml3MFg_4Vv8F4iTRkeAaPUFXogD7ak64_sIaW_bitRZFCSRH7G7NXTmLYA8qUSXR3COHW8kpPRp0U16uAKGyEgZbIJdeocPA4T20U3zC3wBRZ5fec-QSGMKsbjJNMUksLNdouxwlBw5YBauR6X0Rrn6r8EColwRatu7VkwEiS5dSjbStv7qcCRy0Xtg3wjapHHGIERfX32AmgSw3KKVpOTkfc1Gjd7mRyP3jNbeXBZsYeChArC16x-cAhXp1sS7O_ZPoMcBxmpirBu-PnWIlA0qCYnMbM4oqNfSodwHCYhureuqr5Ecd9KTIAfYsFaGfjack2nH2dWRl8LVZ4stH86Jbg5D8a3H0FvbOEc9JbfbQxDfppRzIDBKPaZs2S9j_kv7VkORPogC9nXNEfetj4ZY9kI0kM81cOy7YnU65JHzLegn88mfyaRJhZanAxZf49RBgXi-MIim27h94z8grp1w1DnbshGPGJjxhaPTfr1U1F5SWdVJybopBgzXWay_fd-MxhY9BdsJoPS5_Dq2ThgLzzee7geYpEcwRKu9cjkKNhg8RwFH0qaPlJnktkHVhVXQ2b776Kplan6ut8zv-gc_8357zgUsGflmw4beFkJwsQpjGLvF-rVLTd29x0SCJo_TmAJfcgLEgVbZ4-Cdn0RG2Lm-LmHhtsmSYDRsqbZP3lkkNwU5osGMCeVfIT9MoZMZW2_LRcnZi4L188N7lZ3nmNKKweZKrGA-Gk4liGbkD0D-CB02sAMDa3VphxYHrvdO0S7G49T9fBPcMoB6se1KwjOvvbLiSFZziQ4FberzfvS0RudIJW_Egp6hL43M_jxY5D3RVKRryzFi6ZBZvZbLA29-kOi2EBtGKmSLnyIDs1aI_Jm8mdX0kYBja_9b98BYXDi46BOp47YbS6EKoNdpug-AANSzVholOyImB0_ZeaQZmTqnS2XM4pijONkJ7KgQLAqYCI_TPnKfpOcoG-mBwU9eERIprZ4bWgsdiPdAEIK1Z7MigSlHPZpT5TSeYu99Jjm_g649RdBo2dHTwvNW3ZHbiduzSOcuYxpHUYXN47Klarmo/download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n","Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 22088551 (21M) [application/zip]\n","Saving to: ‘COCO128.zip’\n","\n","COCO128.zip         100%[===================>]  21.06M  33.9MB/s    in 0.6s    \n","\n","2023-08-15 10:09:02 (33.9 MB/s) - ‘COCO128.zip’ saved [22088551/22088551]\n","\n","Archive:  COCO128.zip\n","  inflating: coco128/LICENSE         \n","  inflating: coco128/README.txt      \n","  inflating: coco128/images/train2017/000000000009.jpg  \n","  inflating: coco128/images/train2017/000000000025.jpg  \n","  inflating: coco128/images/train2017/000000000030.jpg  \n","  inflating: coco128/images/train2017/000000000034.jpg  \n","  inflating: coco128/images/train2017/000000000036.jpg  \n","  inflating: coco128/images/train2017/000000000042.jpg  \n","  inflating: coco128/images/train2017/000000000049.jpg  \n","  inflating: coco128/images/train2017/000000000061.jpg  \n","  inflating: coco128/images/train2017/000000000064.jpg  \n","  inflating: coco128/images/train2017/000000000071.jpg  \n","  inflating: coco128/images/train2017/000000000072.jpg  \n","  inflating: coco128/images/train2017/000000000073.jpg  \n","  inflating: coco128/images/train2017/000000000074.jpg  \n","  inflating: coco128/images/train2017/000000000077.jpg  \n","  inflating: coco128/images/train2017/000000000078.jpg  \n","  inflating: coco128/images/train2017/000000000081.jpg  \n","  inflating: coco128/images/train2017/000000000086.jpg  \n","  inflating: coco128/images/train2017/000000000089.jpg  \n","  inflating: coco128/images/train2017/000000000092.jpg  \n","  inflating: coco128/images/train2017/000000000094.jpg  \n","  inflating: coco128/images/train2017/000000000109.jpg  \n","  inflating: coco128/images/train2017/000000000110.jpg  \n","  inflating: coco128/images/train2017/000000000113.jpg  \n","  inflating: coco128/images/train2017/000000000127.jpg  \n","  inflating: coco128/images/train2017/000000000133.jpg  \n","  inflating: coco128/images/train2017/000000000136.jpg  \n","  inflating: coco128/images/train2017/000000000138.jpg  \n","  inflating: coco128/images/train2017/000000000142.jpg  \n","  inflating: coco128/images/train2017/000000000143.jpg  \n","  inflating: coco128/images/train2017/000000000144.jpg  \n","  inflating: coco128/images/train2017/000000000149.jpg  \n","  inflating: coco128/images/train2017/000000000151.jpg  \n","  inflating: coco128/images/train2017/000000000154.jpg  \n","  inflating: coco128/images/train2017/000000000164.jpg  \n","  inflating: coco128/images/train2017/000000000165.jpg  \n","  inflating: coco128/images/train2017/000000000192.jpg  \n","  inflating: coco128/images/train2017/000000000194.jpg  \n","  inflating: coco128/images/train2017/000000000196.jpg  \n","  inflating: coco128/images/train2017/000000000201.jpg  \n","  inflating: coco128/images/train2017/000000000208.jpg  \n","  inflating: coco128/images/train2017/000000000241.jpg  \n","  inflating: coco128/images/train2017/000000000247.jpg  \n","  inflating: coco128/images/train2017/000000000250.jpg  \n","  inflating: coco128/images/train2017/000000000257.jpg  \n","  inflating: coco128/images/train2017/000000000260.jpg  \n","  inflating: coco128/images/train2017/000000000263.jpg  \n","  inflating: coco128/images/train2017/000000000283.jpg  \n","  inflating: coco128/images/train2017/000000000294.jpg  \n","  inflating: coco128/images/train2017/000000000307.jpg  \n","  inflating: coco128/images/train2017/000000000308.jpg  \n","  inflating: coco128/images/train2017/000000000309.jpg  \n","  inflating: coco128/images/train2017/000000000312.jpg  \n","  inflating: coco128/images/train2017/000000000315.jpg  \n","  inflating: coco128/images/train2017/000000000321.jpg  \n","  inflating: coco128/images/train2017/000000000322.jpg  \n","  inflating: coco128/images/train2017/000000000326.jpg  \n","  inflating: coco128/images/train2017/000000000328.jpg  \n","  inflating: coco128/images/train2017/000000000332.jpg  \n","  inflating: coco128/images/train2017/000000000338.jpg  \n","  inflating: coco128/images/train2017/000000000349.jpg  \n","  inflating: coco128/images/train2017/000000000357.jpg  \n","  inflating: coco128/images/train2017/000000000359.jpg  \n","  inflating: coco128/images/train2017/000000000360.jpg  \n","  inflating: coco128/images/train2017/000000000368.jpg  \n","  inflating: coco128/images/train2017/000000000370.jpg  \n","  inflating: coco128/images/train2017/000000000382.jpg  \n","  inflating: coco128/images/train2017/000000000384.jpg  \n","  inflating: coco128/images/train2017/000000000387.jpg  \n","  inflating: coco128/images/train2017/000000000389.jpg  \n","  inflating: coco128/images/train2017/000000000394.jpg  \n","  inflating: coco128/images/train2017/000000000395.jpg  \n","  inflating: coco128/images/train2017/000000000397.jpg  \n","  inflating: coco128/images/train2017/000000000400.jpg  \n","  inflating: coco128/images/train2017/000000000404.jpg  \n","  inflating: coco128/images/train2017/000000000415.jpg  \n","  inflating: coco128/images/train2017/000000000419.jpg  \n","  inflating: coco128/images/train2017/000000000428.jpg  \n","  inflating: coco128/images/train2017/000000000431.jpg  \n","  inflating: coco128/images/train2017/000000000436.jpg  \n","  inflating: coco128/images/train2017/000000000438.jpg  \n","  inflating: coco128/images/train2017/000000000443.jpg  \n","  inflating: coco128/images/train2017/000000000446.jpg  \n","  inflating: coco128/images/train2017/000000000450.jpg  \n","  inflating: coco128/images/train2017/000000000459.jpg  \n","  inflating: coco128/images/train2017/000000000471.jpg  \n","  inflating: coco128/images/train2017/000000000472.jpg  \n","  inflating: coco128/images/train2017/000000000474.jpg  \n","  inflating: coco128/images/train2017/000000000486.jpg  \n","  inflating: coco128/images/train2017/000000000488.jpg  \n","  inflating: coco128/images/train2017/000000000490.jpg  \n","  inflating: coco128/images/train2017/000000000491.jpg  \n","  inflating: coco128/images/train2017/000000000502.jpg  \n","  inflating: coco128/images/train2017/000000000508.jpg  \n","  inflating: coco128/images/train2017/000000000510.jpg  \n","  inflating: coco128/images/train2017/000000000514.jpg  \n","  inflating: coco128/images/train2017/000000000520.jpg  \n","  inflating: coco128/images/train2017/000000000529.jpg  \n","  inflating: coco128/images/train2017/000000000531.jpg  \n","  inflating: coco128/images/train2017/000000000532.jpg  \n","  inflating: coco128/images/train2017/000000000536.jpg  \n","  inflating: coco128/images/train2017/000000000540.jpg  \n","  inflating: coco128/images/train2017/000000000542.jpg  \n","  inflating: coco128/images/train2017/000000000544.jpg  \n","  inflating: coco128/images/train2017/000000000560.jpg  \n","  inflating: coco128/images/train2017/000000000562.jpg  \n","  inflating: coco128/images/train2017/000000000564.jpg  \n","  inflating: coco128/images/train2017/000000000569.jpg  \n","  inflating: coco128/images/train2017/000000000572.jpg  \n","  inflating: coco128/images/train2017/000000000575.jpg  \n","  inflating: coco128/images/train2017/000000000581.jpg  \n","  inflating: coco128/images/train2017/000000000584.jpg  \n","  inflating: coco128/images/train2017/000000000589.jpg  \n","  inflating: coco128/images/train2017/000000000590.jpg  \n","  inflating: coco128/images/train2017/000000000595.jpg  \n","  inflating: coco128/images/train2017/000000000597.jpg  \n","  inflating: coco128/images/train2017/000000000599.jpg  \n","  inflating: coco128/images/train2017/000000000605.jpg  \n","  inflating: coco128/images/train2017/000000000612.jpg  \n","  inflating: coco128/images/train2017/000000000620.jpg  \n","  inflating: coco128/images/train2017/000000000623.jpg  \n","  inflating: coco128/images/train2017/000000000625.jpg  \n","  inflating: coco128/images/train2017/000000000626.jpg  \n","  inflating: coco128/images/train2017/000000000629.jpg  \n","  inflating: coco128/images/train2017/000000000634.jpg  \n","  inflating: coco128/images/train2017/000000000636.jpg  \n","  inflating: coco128/images/train2017/000000000641.jpg  \n","  inflating: coco128/images/train2017/000000000643.jpg  \n","  inflating: coco128/images/train2017/000000000650.jpg  \n","  inflating: coco128/labels/train2017/000000000009.txt  \n","  inflating: coco128/labels/train2017/000000000025.txt  \n","  inflating: coco128/labels/train2017/000000000030.txt  \n","  inflating: coco128/labels/train2017/000000000034.txt  \n","  inflating: coco128/labels/train2017/000000000036.txt  \n","  inflating: coco128/labels/train2017/000000000042.txt  \n","  inflating: coco128/labels/train2017/000000000049.txt  \n","  inflating: coco128/labels/train2017/000000000061.txt  \n","  inflating: coco128/labels/train2017/000000000064.txt  \n","  inflating: coco128/labels/train2017/000000000071.txt  \n","  inflating: coco128/labels/train2017/000000000072.txt  \n","  inflating: coco128/labels/train2017/000000000073.txt  \n","  inflating: coco128/labels/train2017/000000000074.txt  \n","  inflating: coco128/labels/train2017/000000000077.txt  \n","  inflating: coco128/labels/train2017/000000000078.txt  \n","  inflating: coco128/labels/train2017/000000000081.txt  \n","  inflating: coco128/labels/train2017/000000000086.txt  \n","  inflating: coco128/labels/train2017/000000000089.txt  \n","  inflating: coco128/labels/train2017/000000000092.txt  \n","  inflating: coco128/labels/train2017/000000000094.txt  \n","  inflating: coco128/labels/train2017/000000000109.txt  \n","  inflating: coco128/labels/train2017/000000000110.txt  \n","  inflating: coco128/labels/train2017/000000000113.txt  \n","  inflating: coco128/labels/train2017/000000000127.txt  \n","  inflating: coco128/labels/train2017/000000000133.txt  \n","  inflating: coco128/labels/train2017/000000000136.txt  \n","  inflating: coco128/labels/train2017/000000000138.txt  \n","  inflating: coco128/labels/train2017/000000000142.txt  \n","  inflating: coco128/labels/train2017/000000000143.txt  \n","  inflating: coco128/labels/train2017/000000000144.txt  \n","  inflating: coco128/labels/train2017/000000000149.txt  \n","  inflating: coco128/labels/train2017/000000000151.txt  \n","  inflating: coco128/labels/train2017/000000000154.txt  \n","  inflating: coco128/labels/train2017/000000000164.txt  \n","  inflating: coco128/labels/train2017/000000000165.txt  \n","  inflating: coco128/labels/train2017/000000000192.txt  \n","  inflating: coco128/labels/train2017/000000000194.txt  \n","  inflating: coco128/labels/train2017/000000000196.txt  \n","  inflating: coco128/labels/train2017/000000000201.txt  \n","  inflating: coco128/labels/train2017/000000000208.txt  \n","  inflating: coco128/labels/train2017/000000000241.txt  \n","  inflating: coco128/labels/train2017/000000000247.txt  \n","  inflating: coco128/labels/train2017/000000000250.txt  \n","  inflating: coco128/labels/train2017/000000000257.txt  \n","  inflating: coco128/labels/train2017/000000000260.txt  \n","  inflating: coco128/labels/train2017/000000000263.txt  \n","  inflating: coco128/labels/train2017/000000000283.txt  \n","  inflating: coco128/labels/train2017/000000000294.txt  \n","  inflating: coco128/labels/train2017/000000000307.txt  \n","  inflating: coco128/labels/train2017/000000000308.txt  \n","  inflating: coco128/labels/train2017/000000000309.txt  \n","  inflating: coco128/labels/train2017/000000000312.txt  \n","  inflating: coco128/labels/train2017/000000000315.txt  \n","  inflating: coco128/labels/train2017/000000000321.txt  \n","  inflating: coco128/labels/train2017/000000000322.txt  \n","  inflating: coco128/labels/train2017/000000000326.txt  \n","  inflating: coco128/labels/train2017/000000000328.txt  \n","  inflating: coco128/labels/train2017/000000000332.txt  \n","  inflating: coco128/labels/train2017/000000000338.txt  \n","  inflating: coco128/labels/train2017/000000000349.txt  \n","  inflating: coco128/labels/train2017/000000000357.txt  \n","  inflating: coco128/labels/train2017/000000000359.txt  \n","  inflating: coco128/labels/train2017/000000000360.txt  \n","  inflating: coco128/labels/train2017/000000000368.txt  \n","  inflating: coco128/labels/train2017/000000000370.txt  \n","  inflating: coco128/labels/train2017/000000000382.txt  \n","  inflating: coco128/labels/train2017/000000000384.txt  \n","  inflating: coco128/labels/train2017/000000000387.txt  \n","  inflating: coco128/labels/train2017/000000000389.txt  \n","  inflating: coco128/labels/train2017/000000000394.txt  \n","  inflating: coco128/labels/train2017/000000000395.txt  \n","  inflating: coco128/labels/train2017/000000000397.txt  \n","  inflating: coco128/labels/train2017/000000000400.txt  \n","  inflating: coco128/labels/train2017/000000000404.txt  \n","  inflating: coco128/labels/train2017/000000000415.txt  \n","  inflating: coco128/labels/train2017/000000000419.txt  \n","  inflating: coco128/labels/train2017/000000000428.txt  \n","  inflating: coco128/labels/train2017/000000000431.txt  \n","  inflating: coco128/labels/train2017/000000000436.txt  \n","  inflating: coco128/labels/train2017/000000000438.txt  \n","  inflating: coco128/labels/train2017/000000000443.txt  \n","  inflating: coco128/labels/train2017/000000000446.txt  \n","  inflating: coco128/labels/train2017/000000000450.txt  \n","  inflating: coco128/labels/train2017/000000000459.txt  \n","  inflating: coco128/labels/train2017/000000000471.txt  \n","  inflating: coco128/labels/train2017/000000000472.txt  \n","  inflating: coco128/labels/train2017/000000000474.txt  \n","  inflating: coco128/labels/train2017/000000000486.txt  \n","  inflating: coco128/labels/train2017/000000000488.txt  \n","  inflating: coco128/labels/train2017/000000000490.txt  \n","  inflating: coco128/labels/train2017/000000000491.txt  \n","  inflating: coco128/labels/train2017/000000000502.txt  \n","  inflating: coco128/labels/train2017/000000000508.txt  \n","  inflating: coco128/labels/train2017/000000000510.txt  \n","  inflating: coco128/labels/train2017/000000000514.txt  \n","  inflating: coco128/labels/train2017/000000000520.txt  \n","  inflating: coco128/labels/train2017/000000000529.txt  \n","  inflating: coco128/labels/train2017/000000000531.txt  \n","  inflating: coco128/labels/train2017/000000000532.txt  \n","  inflating: coco128/labels/train2017/000000000536.txt  \n","  inflating: coco128/labels/train2017/000000000540.txt  \n","  inflating: coco128/labels/train2017/000000000542.txt  \n","  inflating: coco128/labels/train2017/000000000544.txt  \n","  inflating: coco128/labels/train2017/000000000560.txt  \n","  inflating: coco128/labels/train2017/000000000562.txt  \n","  inflating: coco128/labels/train2017/000000000564.txt  \n","  inflating: coco128/labels/train2017/000000000569.txt  \n","  inflating: coco128/labels/train2017/000000000572.txt  \n","  inflating: coco128/labels/train2017/000000000575.txt  \n","  inflating: coco128/labels/train2017/000000000581.txt  \n","  inflating: coco128/labels/train2017/000000000584.txt  \n","  inflating: coco128/labels/train2017/000000000589.txt  \n","  inflating: coco128/labels/train2017/000000000590.txt  \n","  inflating: coco128/labels/train2017/000000000595.txt  \n","  inflating: coco128/labels/train2017/000000000597.txt  \n","  inflating: coco128/labels/train2017/000000000599.txt  \n","  inflating: coco128/labels/train2017/000000000605.txt  \n","  inflating: coco128/labels/train2017/000000000612.txt  \n","  inflating: coco128/labels/train2017/000000000620.txt  \n","  inflating: coco128/labels/train2017/000000000623.txt  \n","  inflating: coco128/labels/train2017/000000000625.txt  \n","  inflating: coco128/labels/train2017/000000000626.txt  \n","  inflating: coco128/labels/train2017/000000000629.txt  \n","  inflating: coco128/labels/train2017/000000000634.txt  \n","  inflating: coco128/labels/train2017/000000000636.txt  \n","  inflating: coco128/labels/train2017/000000000641.txt  \n","  inflating: coco128/labels/train2017/000000000643.txt  \n","  inflating: coco128/labels/train2017/000000000650.txt  \n"]}]},{"cell_type":"code","source":["  obj_labels = (\n","    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n","    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n","    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n","    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n","    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n","    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',\n","    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')"],"metadata":{"id":"H6pLjEuMnCqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["必要なライブラリを読み込む"],"metadata":{"id":"ce_fxoG_Hw1J"}},{"cell_type":"code","source":["from torch.nn.modules.batchnorm import BatchNorm2d\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset,DataLoader\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as FF\n","from torchvision.ops.boxes import box_iou\n","from torchvision.io import read_image\n","from torchvision.utils import draw_bounding_boxes\n","import glob\n","import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from six import b\n","import cv2\n","from PIL import Image\n","import math\n","import matplotlib.pyplot as plt"],"metadata":{"id":"bDiYYmULHwUo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["各種設定"],"metadata":{"id":"mC84g_0uLLtF"}},{"cell_type":"code","source":["image_path = \"coco128/images/train2017\"\n","label_path = \"coco128/labels/train2017\"\n","img_list = sorted(glob.glob(os.path.join(image_path,\"*\")))\n","label_list = sorted(glob.glob(os.path.join(label_path,\"*\")))\n","img_size = 416\n","class_n = 80"],"metadata":{"id":"0lUpOKdqLLJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## データの前処理\n","\n","YoLov3で用いるデータセットを準備する\n","- 様々な工夫が施されている\n","- 前処理の説明であるが、YoLov3の本質が詰まっている"],"metadata":{"id":"X_pJu-LbKUTR"}},{"cell_type":"markdown","source":["### AnchorBoxの定義\n","YoLoは通常AnchorBoxを利用するため、これを設定する\n","- アンカーボックスは、高さと幅が事前に決められた境界ボックスのセットのことで、一般にメッシュを複数個組み合わせた矩形となる\n","  - 検出する特定の物体のスケールおよび縦横比を取得するため利用し、学習データセットに含まれるオブジェクトサイズに基づいて選択、事前作成されるのが一般的である\n","- ここではk-Meansを利用してデータセットのアノテーションをクラスタリングし、その結果をAnchorBoxとする\n","  - 3つのPrior Boxの入手にこのAncorBoxが関わっている\n","  - K-means clusteringを利用してCOCOデータセットにある全てのBounding Boxを9個に分類する\n","    - スケールが3つ、BOXが3つであるため9個\n","\n","データからクラスタリングを行い、各データがどのクラスに属するか決定しておく\n","- YoLov3はAnchorBoxが9個あるため、9つのクラスタリングを行う"],"metadata":{"id":"rqeK8iwxMSuw"}},{"cell_type":"code","source":["bbox_dict = {'width':[] , 'height':[]}\n","bbox_list = []\n","for path in label_list:\n","  with open(path , 'r',newline='\\n') as f:\n","      for s_line in f: # Bounding Boxが順に記述されたファイル\n","        bbox = [float(x) for x in s_line.rstrip('\\n').split(' ')] # 改行で行を分けて各Bounding Boxの行を取得、さらにスペースで分割する\n","        bbox_dict['width'].append(bbox[3])\n","        bbox_dict['height'].append(bbox[4])\n","        bbox_list.append(bbox[3:5])\n","df = pd.DataFrame(bbox_dict)\n","print(df.head()) # 先頭5このファイルについてwidthとhightの抽出結果を表示\n","km = KMeans(n_clusters=9,\n","  init='random',\n","  n_init=10,\n","  max_iter=300,\n","  tol=1e-04,\n","  random_state=0)\n","y_km = km.fit_predict(bbox_list)\n","df['cluster'] = y_km\n","print(df.head()) # 先頭5このファイルについてどこのクラスタに配属されたかを表示"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcbUgD3RDfMM","executionInfo":{"status":"ok","timestamp":1692094153365,"user_tz":-540,"elapsed":402,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"312d6f16-9fe5-47ff-ad1d-d0639aee4619"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      width    height\n","0  0.955609  0.595500\n","1  0.498875  0.476417\n","2  0.494125  0.510583\n","3  0.678875  0.781500\n","4  0.118047  0.096937\n","      width    height  cluster\n","0  0.955609  0.595500        7\n","1  0.498875  0.476417        4\n","2  0.494125  0.510583        4\n","3  0.678875  0.781500        0\n","4  0.118047  0.096937        2\n"]}]},{"cell_type":"markdown","source":["9個のAnchorBoxを決定\n","- 大きいAnchorBox(先頭行)は大きな物体の検出に用いるため、大きな物体を検出する出力(Scale 3)に割り当てる\n","- 逆に小さいAnchorBoxは小さな物体の検出に用いるため、小さな物体を検出する出力(Scale 1)に割り当てる"],"metadata":{"id":"BSDSq96xQBS4"}},{"cell_type":"code","source":["anchor_dict = {\"width\":[],\"height\":[],\"area\":[]}\n","for i in range(9):\n","  anchor_dict[\"width\"].append(df[df[\"cluster\"] == i].mean()[\"width\"])\n","  anchor_dict[\"height\"].append(df[df[\"cluster\"] == i].mean()[\"height\"])\n","  anchor_dict[\"area\"].append(df[df[\"cluster\"] == i].mean()[\"width\"]*df[df[\"cluster\"] == i].mean()[\"height\"])\n","anchor = pd.DataFrame(anchor_dict).sort_values('area', ascending=False)\n","anchor[\"type\"] = [int(img_size/32) ,int(img_size/32) ,int(img_size/32) ,  int(img_size/16) ,int(img_size/16) ,int(img_size/16) , int(img_size/8), int(img_size/8), int(img_size/8)]\n","print(anchor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIpImq0eDhMR","executionInfo":{"status":"ok","timestamp":1692094153366,"user_tz":-540,"elapsed":7,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"9edda6bd-3d35-46cc-bc37-4cd0560749a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      width    height      area  type\n","7  0.953643  0.856002  0.816320    13\n","0  0.591571  0.828589  0.490169    13\n","6  0.777525  0.379511  0.295079    13\n","4  0.414800  0.443691  0.184043    26\n","3  0.245450  0.694372  0.170434    26\n","1  0.152484  0.359089  0.054755    26\n","5  0.245576  0.165619  0.040672    52\n","2  0.087694  0.152691  0.013390    52\n","8  0.040143  0.047590  0.001910    52\n"]}]},{"cell_type":"markdown","source":["### グリッド、アンカーボックス、Bouding Boxの関係\n","\n","YoLov3におけるグリッドとAnchor Box、Bounding Boxは、次の図のように定義される\n","\n","ここで\n","- $C_x, C_y$は1ピクセルの幅、\n","- $t_x , t_y , t_w , t_h$はYOLOv3の予測\n","- $P_w , P_h$はAnchorBoxサイズ\n","\n","をそれぞれ表す\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yologrid.png\" width=300>\n"],"metadata":{"id":"z6j5qV-8RoNy"}},{"cell_type":"markdown","source":["### 幅と高さの推定\n","\n","YoLoV3では次の式で幅と高さを推定する\n","- $b_w = P_w e^{t_w}$\n","- $b_h = P_h e^{t_h}$\n","\n","YOLOv3の出力にある$t_w,_h$の表現として、小さいときは正確に、大きい場合は例えば1ドットのずれなどは殆ど気にならないため、指数関数を用いてAnchorBoxを表現する\n","- 直接物体の幅や高さをピクセル値予測せず、AnchorBoxを調整するという表現にすることで、学習コストを削減できる"],"metadata":{"id":"QMFKQ5IRTqyr"}},{"cell_type":"markdown","source":["### 中心位置の推定方法\n","YoLov3では次の式で中心位置を推定する\n","- $b_x = \\sigma(t_x) + C_x$\n","- $b_y = \\sigma(t_y) + C_y$\n","\n","直接的に推定してもよいが、なぜか複雑な式で予測している\n","\n","- 例えばサイズが$416 \\times 416$の画像に物体が写っており、その物体の中心が$(200 ,250)$であったとする\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yoloapple.png\" width=300>\n","\n","- 対称物体が大きい場合、小さい特徴マップで予測することになる\n","  - $13 \\times 13$の特徴マップでは画像サイズに対する中心の位置は$\\frac{200}{416} = 0.481, \\frac{250}{416} = 0.600$となるため、$13 \\times 13$pxでは最も近いのは(6.253 , 7.800)となり、結果として小数点を省いた$13\\times 13$メッシュにおける座標値としての(6, 7)が中央となる\n","\n","このままでは、特徴マップが小さくなるにつれて誤差が大きくなる\n","- これを防ぐため、YoLov3ではピクセル内のどこに中心があるかを予測する\n","- YoLov3の出力$t_x , t_y$をシグモイド関数に入力して0から1の値に変換\n","  - メッシュよりも細かい場所の指定(メッシュに対して何割のところに中心があるか)を知ることができる\n","- $(6.253 , 7.800)$は、対象とするメッシュでピクセル(7 , 6)であり、そこから(0.253 , 0.800)の場所に中心があると考えることができる\n","- これを受けて、$0.253 = \\sigma(t_x)$となるような$x$を推論する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yoloapplegrid.png\" width=300>\n"],"metadata":{"id":"7Yi_oqd_UhH5"}},{"cell_type":"markdown","source":["### YoLov3の本質\n","\n","YoLov3の予測はAnchorBoxの微調整やピクセル間の誤差補正予測など、微調整をふんだんに行っていることがわかる\n","- そのために、少々値が加工された入力を必要とする\n","\n","アノテーションデータは真の中心位置と幅・高さを保持しているため、これをYoLov3用に変換する必要がある\n","\n","- 幅と高さについて、\n","  - $b_w = P_w e^{t_w}$\n","  - $b_h = P_h e^{t_h}$\n","  \n","  であるため、\n","\n","  $t_w , t_h$は\n","  - $t_w = \\ln\\frac{b_w}{P_w}$\n","  - $t_h = \\ln\\frac{b_h}{P_h}$\n","\n","  となる\n","  - この処理は`def wh2twth`で記述されている\n","\n","- 中心位置についても、\n","  - $b_x = \\sigma(t_x) + C_x$\n","  - $b_y = \\sigma(t_y) + C_y$\n","\n","  であるため、\n","  - $b_x - C_x = \\sigma(t_x)$\n","  - $b_y - C_y = \\sigma(t_y)$\n","\n","  となり、この逆関数を求めると$t_x$と$t_y$を得ることができる\n","\n","  シグモイド関数の逆関数はロジット関数であるため、\n","  - $t_x = \\ln\\frac{b_x - C_x}{1-(b_x - C_x)}$\n","  - $t_y = \\ln\\frac{b_y - C_y}{1-(b_y- C_y)}$\n","\n","  となる\n","  - この処理は`def cxcy2txty`で記述されている\n","\n","以上を踏まえて実際にデータセットの前処理部分を記述する\n","- 基本的にはDataset関数の準備であり、その核心は、`__getitem__`によりデータを取得することと、`__len__`により全データ数を返すところ\n","- その他はすべて`__getitem__`のための副次的な処理関数"],"metadata":{"id":"pJrp9g5YXOHT"}},{"cell_type":"code","source":["class YOLOv3_Dataset(Dataset):\n","  def __init__(self, img_list, label_list, class_n, img_size, anchor_dict, transform):\n","    super().__init__()\n","    self.img_list = img_list\n","    self.label_list = label_list\n","    self.anchor_dict = anchor_dict\n","    self.class_n = class_n\n","    self.img_size = img_size\n","    self.transform = transform\n","    self.anchor_iou = torch.cat([torch.zeros(9,2) , torch.tensor(self.anchor_dict[[\"width\",\"height\"]].values)] ,dim = 1)\n","    self.map_size = [int(self.img_size/32) , int(self.img_size/16) , int(self.img_size/8)]\n","\n","  def get_label(self, path):  # coco128/labelsにあるラベルとBBoxがセットになったファイルを読み出す\n","    bbox_list = []\n","    with open(path , 'r',newline='\\n') as f:\n","      for s_line in f:\n","        bbox = [float(x) for x in s_line.rstrip('\\n').split(' ')]\n","        bbox_list.append(bbox)\n","    return bbox_list\n","\n","  def wh2twth(self, wh):\n","    twth = []\n","    for i in range(9):\n","      anchor = self.anchor_dict.iloc[i]\n","      aw = anchor[\"width\"]\n","      ah = anchor[\"height\"]\n","      twth.append([math.log(wh[0]/aw) , math.log(wh[1]/ah)])\n","    return twth\n","\n","  def cxcy2txty(self,cxcy):\n","    txty = []\n","    for size in self.map_size:\n","      grid_x = int(cxcy[0]*size)\n","      grid_y = int(cxcy[1]*size)\n","      tx = math.log((cxcy[0]*size - grid_x + 1e-10) / (1 - cxcy[0]*size +grid_x+ 1e-10))\n","      ty = math.log((cxcy[1]*size - grid_y+ 1e-10) / (1 - cxcy[1]*size + grid_y+ 1e-10))\n","      txty.append([grid_x , tx , grid_y ,ty])\n","    return txty\n","\n","  def label2tensor(self, bbox_list):\n","    tensor_list = []\n","    for size in self.map_size:\n","      for x in range(3):\n","        tensor_list.append(torch.zeros((4 + 1 + self.class_n,size,size)))\n","    for bbox in bbox_list:\n","      cls_n = int(bbox[0])\n","      txty_list = self.cxcy2txty(bbox[1:3])\n","      twth_list = self.wh2twth(bbox[3:])\n","      label_iou = torch.cat([torch.zeros((1,2)), torch.tensor(bbox[3:]).unsqueeze(0)],dim=1)\n","      iou = box_iou(label_iou, self.anchor_iou)[0]\n","      obj_idx = torch.argmax(iou).item()\n","#      print(cls_n, txty_list, twth_list, label_iou, iou, obj_idx)\n","      for i, twth in enumerate(twth_list):\n","        tensor = tensor_list[i]\n","        txty = txty_list[int(i/3)]\n","        if i == obj_idx:\n","          tensor[0, txty[2], txty[0]] = txty[1]\n","          tensor[1, txty[2], txty[0]] = txty[3]\n","          tensor[2, txty[2], txty[0]] = twth[0]\n","          tensor[3, txty[2], txty[0]] = twth[1]\n","          tensor[4, txty[2], txty[0]] = 1\n","          tensor[5+cls_n, txty[2], txty[0]] = 1\n","    scale3_label = torch.cat(tensor_list[0:3] , dim = 0)\n","    scale2_label = torch.cat(tensor_list[3:6] , dim = 0)\n","    scale1_label = torch.cat(tensor_list[6:] , dim = 0)\n","    return scale3_label , scale2_label , scale1_label\n","\n","  def __getitem__(self, idx):\n","    img_path = self.img_list[idx]\n","    label_path = self.label_list[idx]\n","    bbox_list = self.get_label(label_path)\n","    img = cv2.imread(img_path)\n","    img = cv2.resize(img , (self.img_size , self.img_size))\n","    img = Image.fromarray(img)\n","    img = self.transform(img)\n","    scale3_label , scale2_label , scale1_label = self.label2tensor(bbox_list)\n","    return img , scale3_label , scale2_label , scale1_label\n","\n","  def __len__(self):\n","    return len(self.img_list)"],"metadata":{"id":"12a-peb9DxYi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["学習のためにBouding Boxや中心位置などを操作しているため、元に戻す関数`visualization`を定義する"],"metadata":{"id":"zM6wGkGJ3WaE"}},{"cell_type":"code","source":["def visualization(y_pred, anchor, img_size,conf = 0.5, is_label = False):\n","  size = y_pred.shape[2]\n","  anchor_size = anchor[anchor[\"type\"] == size]\n","  bbox_list = []\n","  obj_list = []\n","  for i in range(3):\n","    a = anchor_size.iloc[i]\n","    grid = img_size/size\n","    y_pred_cut = y_pred[0,i*(4 + 1 + class_n) :(i+1)*(4 + 1 + class_n) ].cpu()\n","    if is_label:\n","      y_pred_conf = y_pred_cut[4,:,:].numpy()\n","    else:\n","      y_pred_conf = torch.sigmoid(y_pred_cut[4,:,:]).numpy()\n","    index = np.where(y_pred_conf > conf)\n","    for y,x in zip(index[0],index[1]):\n","      cx = x*grid + torch.sigmoid(y_pred_cut[0,y,x]).numpy()*grid\n","      cy = y*grid + torch.sigmoid(y_pred_cut[1,y,x]).numpy()*grid\n","      width = a[\"width\"]*torch.exp(y_pred_cut[2,y,x]).numpy()*img_size\n","      height = a[\"height\"]*torch.exp(y_pred_cut[3,y,x]).numpy()*img_size\n","      xmin,ymin,xmax,ymax = cx - width/2 , cy - height/2 ,cx + width/2 , cy + height/2\n","      bbox_list.append([xmin,ymin,xmax,ymax])\n","      obj_list.append(y_pred_cut[5:,y,x].argmax()) # 正解ラベル\n","  return bbox_list, obj_list"],"metadata":{"id":"bX7GENGx3WCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["画像を表示するための関数`show`を定義する"],"metadata":{"id":"FVKznFmG3jZG"}},{"cell_type":"code","source":["def show(imgs):\n","    if not isinstance(imgs, list):\n","        imgs = [imgs]\n","    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False, dpi=150)\n","    for i, img in enumerate(imgs):\n","        img = img.detach()\n","        img = FF.to_pil_image(img)\n","        axs[0, i].imshow(np.asarray(img))\n","        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"],"metadata":{"id":"1CFAgxo-3nO8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["試しに、学習データを表示させる\n","- Bounding Boxの色は、Scaleの違いを表している\n","  - 赤：大サイズとして予測（Scale3）\n","  - 緑：中サイズとして予測（Scale2）\n","  - 青：小サイズとして予測（Scale1）"],"metadata":{"id":"u0mi53iZ5It1"}},{"cell_type":"code","source":["transform = T.Compose([T.ToTensor()])\n","train_data = YOLOv3_Dataset(img_list, label_list, 80, img_size , anchor, transform)\n","train_loader = DataLoader(train_data, batch_size = 1)\n","\n","for n, (img, scale3_label, scale2_label, scale1_label) in enumerate(train_loader):\n","  path = img_list[n]\n","  img = cv2.imread(path)[:,:,::-1]\n","  img = cv2.resize(img, (img_size, img_size))\n","  img = torch.tensor(img.transpose(2,0,1))\n","  preds = [scale3_label, scale2_label, scale1_label]\n","  for color,pred in zip([\"red\",\"green\",\"blue\"],preds):\n","    bbox_list, obj_list = visualization(pred, anchor, img_size, conf=0.9, is_label=True)\n","    if len(bbox_list) != 0:\n","      obj_label_list = []\n","      for obj_item in obj_list:\n","        obj_label_list.append(obj_labels[obj_item])\n","      img = draw_bounding_boxes(img, torch.tensor(bbox_list), colors=color, width=1, labels=obj_label_list)\n","  show(img)\n","  if n == 10:\n","    break\n","print(scale3_label.shape, scale2_label.shape, scale1_label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1w6hXKz-Idd--FnhGk1jACu7iDCqZsA20"},"id":"BBLm_UTt9q2d","executionInfo":{"status":"ok","timestamp":1692094162297,"user_tz":-540,"elapsed":8935,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"4bad6cbd-a3fd-4373-e853-f75c6745605e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## YoLov3ネットワーク\n"],"metadata":{"id":"SKEZhBI5HpmQ"}},{"cell_type":"markdown","source":["ちょっと厄介ではあるが、図の通りに実装する\n","- n=80つまり、80クラス分類であるが、nであるため自由に変更できるようにしている"],"metadata":{"id":"s0r6a5fvI6kU"}},{"cell_type":"code","source":["class YOLOv3(nn.Module):\n","  def __init__(self,class_n = 80):\n","    super(YOLOv3 , self).__init__()\n","    self.class_n = class_n\n","    self.first_block = nn.Sequential(\n","        nn.Conv2d(3 , 32 , 3 , 1 , 1),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(),\n","        nn.Conv2d(32 , 64 , 3 , 2 , 1),\n","        nn.BatchNorm2d(64),\n","        nn.LeakyReLU(),\n","      )\n","    self.residual_block_1 = self.MakeResidualBlock(64)\n","    self.conv_1 = nn.Conv2d(64 , 128  , 3 , 2 , 1)\n","    self.residual_block_2 = nn.Sequential(self.MakeResidualBlock(128),self.MakeResidualBlock(128))\n","    self.conv_2 = nn.Conv2d(128 , 256  , 3 , 2 , 1)\n","    self.residual_block_3 = nn.Sequential(self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256))\n","    self.conv_3 = nn.Conv2d(256 , 512  , 3 , 2 , 1)\n","    self.residual_block_4 = nn.Sequential(self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512))\n","    self.conv_4 = nn.Conv2d(512 , 1024  , 3 , 2 , 1)\n","    self.residual_block_5 = nn.Sequential(self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),)\n","    self.conv_block = nn.Sequential(self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),)\n","    self.scale3_output = nn.Conv2d(1024 , (3 * (4 + 1 +self.class_n)) , 1 , 1 )\n","    self.scale2_upsample = nn.Conv2d(1024 , 256 , 1 , 1 )\n","    self.scale2_convblock = nn.Sequential(nn.Sequential(nn.Conv2d(768 , 256 , 1 , 1),\n","                          nn.BatchNorm2d(256),\n","                          nn.LeakyReLU(),\n","                          nn.Conv2d(256 , 512 , 3 , 1 ,  1),\n","                          nn.BatchNorm2d(512),\n","                          nn.LeakyReLU(),\n","                          ),self.MakeResidualBlock(512),self.MakeResidualBlock(512),)\n","    self.scale2_output = nn.Conv2d(512 , (3 * (4 + 1 +self.class_n)) , 1 , 1 )\n","    self.scale1_upsample = nn.Conv2d(512 , 128 , 1 , 1 )\n","    self.scale1_convblock = nn.Sequential(nn.Sequential(nn.Conv2d(384 , 128 , 1 , 1),\n","                          nn.BatchNorm2d(128),\n","                          nn.LeakyReLU(),\n","                          nn.Conv2d(128 , 256 , 3 , 1 ,  1),\n","                          nn.BatchNorm2d(256),\n","                          nn.LeakyReLU(),\n","                          ),self.MakeResidualBlock(256),self.MakeResidualBlock(256),)\n","    self.scale1_output =  nn.Conv2d(256 , (3 * (4 + 1 +self.class_n)) , 1 , 1 )\n","    self.upsample = nn.Upsample(scale_factor = 2)\n","  def MakeResidualBlock(self,fn):\n","    block = nn.Sequential(nn.Conv2d(fn , int(fn/2) , 1 , 1),\n","                          nn.BatchNorm2d(int(fn/2)),\n","                          nn.LeakyReLU(),\n","                          nn.Conv2d(int(fn/2) , fn , 3 , 1 ,  1),\n","                          nn.BatchNorm2d(fn),\n","                          nn.LeakyReLU(),\n","                          )\n","    return block\n","  def forward(self,x):\n","    x = self.first_block(x)\n","    x_res = self.residual_block_1(x)\n","    x = x + x_res\n","    x = self.conv_1(x)\n","    for layer in self.residual_block_2:\n","      x_res = layer(x)\n","      x = x + x_res\n","    x = self.conv_2(x)\n","    for layer in self.residual_block_3:\n","      x_res = layer(x)\n","      x = x + x_res\n","    x1 = x\n","    x = self.conv_3(x)\n","    for layer in self.residual_block_4:\n","      x_res = layer(x)\n","      x = x + x_res\n","    x2 = x\n","    x = self.conv_4(x)\n","    for layer in self.residual_block_5:\n","      x_res = layer(x)\n","      x = x + x_res\n","    for layer in self.conv_block:\n","      x = layer(x)\n","    scale3_result = self.scale3_output(x)\n","    scale2_up = self.upsample(self.scale2_upsample(x))\n","    x = torch.cat([x2 , scale2_up],dim = 1)\n","    for layer in self.scale2_convblock :\n","      x = layer(x)\n","    x2 = x\n","    scale2_result = self.scale2_output(x)\n","    scale1_up = self.upsample(self.scale1_upsample(x2))\n","    x = torch.cat([x1 , scale1_up],dim = 1)\n","    for layer in self.scale1_convblock :\n","      x = layer(x)\n","    scale1_result = self.scale1_output(x)\n","    return  scale3_result , scale2_result , scale1_result\n","model =YOLOv3().cuda()"],"metadata":{"id":"KaUMrYESDSoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  output = model(torch.zeros((1,3,416,416)).cuda())\n","for i in range(3):\n","  print(output[i].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zD5pRNdfDYQg","executionInfo":{"status":"ok","timestamp":1692094162686,"user_tz":-540,"elapsed":7,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"20e8637d-eb99-4bee-ac91-2c0ddfdcb05b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 255, 13, 13])\n","torch.Size([1, 255, 26, 26])\n","torch.Size([1, 255, 52, 52])\n"]}]},{"cell_type":"markdown","source":["modelとしてインスタンス化\n","- ここで、実際に出力を確認してみる\n","- $13 \\times 13$、$26 \\times 26$、$52 \\times 52$のグリッドそれぞれについて、255要素のデータが出力されており、設計通りであることがわかる"],"metadata":{"id":"toC0stGeJjO2"}},{"cell_type":"markdown","source":["## ロス関数\n","\n","YoLov3のロス関数はBounding Boxをラベルの両方を考慮しており、かなり複雑\n","\n","論文では次の式が与えられている\n","- 説明のため、順番を並び替えている\n","\n","$$\\lambda_{coord} \\sum^{S^2}_{i=0} \\sum^B_{j=0}\\mathfrak{1}^{obj}_{i,j}[(t_x−\\hat{t}_x)^2+(t_y−\\hat{t}_y)^2+(t_w−\\hat{t}_w)^2+(_th−\\hat{t}_h)^2]\\\\ +\\sum^{S^2}_{i=0} \\sum^B_{j=0}\\mathfrak{1}^{obj}_{i,j}[-log(\\sigma(t_o))+\\lambda_{noobj}\\sum^{S^2}_{i=0} \\sum^B_{j=0}\\mathfrak{1}^{noobj}_{i,j}[-log(1-\\sigma(t_o))]\\\\+\\sum^C_{k=1}BCE(\\hat{y}_k,\\sigma(s_k))] $$\n","\n","簡単に説明する\n","- 1行目はBounding Boxの$x, y, w, h$それぞれの誤差を表しており、単純に二乗誤差が用いられている\n","- 2行目はobjective scoreに沿うとして、オブジェクトがセルの中に存在するかどうかで2つの項に分かれている\n","  - 存在することを正しく判定できたか、また、存在しないことを正しく判定できたかをそれぞれ表している\n","  - 検出対象の物体がない矩形は objectness score が0になるように学習する必要があり、これに対応する\n","- 3行目はクラスつまり物体のラベルを正しく推定できたかを意味しており、BCEロスを利用している\n","  - なお、コード上では`BCEWithLogitsLoss`を利用している\n","  - `BCELoss`はシグモイドをとった後の値を入力として受け付け、`BCEWithLogitsLoss`はシグモイドをとる前の値を入力として受け付けつける点に注意\n","\n","なお、論文では、$t_x, t_y$の損失は二乗誤差で計算すると書いてあり、ここではその通りに実装している\n","- 論文で示されているdarknet実装では、シグモイド関数適用後の$\\sigma(t_x), \\sigma(t_y)$の値との差分を求めているため、論文とは異なっている\n","- この実装に従う場合は、$t_x, t_y$はバイナリクロスエントロピーで損失を計算することになる"],"metadata":{"id":"Ku1AYBmW6A8A"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters())\n","criterion_bce = torch.nn.BCEWithLogitsLoss()\n","def bbox_metric(y_pred , y_true,class_n = 80):\n","  for i in range(3):\n","    y_pred_cut = y_pred[:,i*(4 + 1 + class_n) :(i+1)*(4 + 1 + class_n) ]\n","    y_true_cut = y_true[:,i*(4 + 1 + class_n) :(i+1)*(4 + 1 + class_n) ]\n","    loss_coord = torch.sum(torch.square(y_pred_cut[:,0:4] - y_true_cut[:,0:4])*y_true_cut[:,4])\n","    loss_obj = torch.sum((-1 * torch.log(torch.sigmoid(y_pred_cut[:,4] )+ 1e-10) + criterion_bce(y_pred_cut[:,5:],y_true_cut[:,5:]))*y_true_cut[:,4])\n","    loss_noobj =  torch.sum((-1 * torch.log(1 - torch.sigmoid(y_pred_cut[:,4])+ 1e-10))*(1 - y_true_cut[:,4]))\n","    return loss_coord , loss_obj , loss_noobj"],"metadata":{"id":"krF9tUElD1Se"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 学習\n","最新GPU(RTX3090Ti)でも1時間弱かかる\n","- Google Colaboratoryでは、ほぼ実行時間限界の半分(10時間)程度必要となる可能性がある\n"],"metadata":{"id":"Jrm4G74JV92F"}},{"cell_type":"code","source":["from tqdm import tqdm\n","lambda_coord = 1\n","lambda_obj = 10\n","lambda_noobj = 1\n","conf = 0.5\n","best_loss = 99999\n","for epoch in range(300):\n","  total_train_loss = 0\n","  total_train_loss_coord = 0\n","  total_train_loss_obj = 0\n","  total_train_loss_noobj = 0\n","  with tqdm(train_loader) as pbar:\n","    pbar.set_description(\"[train] Epoch %d\" % epoch)\n","    for n , (img , scale3_label , scale2_label ,scale1_label) in enumerate(pbar):\n","      optimizer.zero_grad()\n","      img = img.cuda()\n","      scale1_label = scale1_label.cuda()\n","      scale2_label = scale2_label.cuda()\n","      scale3_label = scale3_label.cuda()\n","      labels = [scale3_label , scale2_label ,scale1_label]\n","      preds  = list(model(img))\n","      loss_coord = 0\n","      loss_obj = 0\n","      loss_noobj = 0\n","      for label , pred in zip(labels , preds):\n","        _loss_coord , _loss_obj , _loss_noobj = bbox_metric(pred , label)\n","        loss_coord += _loss_coord\n","        loss_obj += _loss_obj\n","        loss_noobj += _loss_noobj\n","      loss = lambda_coord*loss_coord + lambda_obj*loss_obj + lambda_noobj*loss_noobj\n","      total_train_loss += loss.item()\n","      total_train_loss_coord += loss_coord.item()\n","      total_train_loss_obj += loss_obj.item()\n","      total_train_loss_noobj += loss_noobj.item()\n","      loss.backward()\n","      optimizer.step()\n","      pbar.set_description(\"[train] Epoch %d loss %f loss_coord %f loss_obj %f loss_noobj %f\" % (epoch ,total_train_loss/(n+1),total_train_loss_coord/(n+1) , total_train_loss_obj/(n+1),total_train_loss_noobj/(n+1)))\n","      if best_loss > total_train_loss/(n+1):\n","        model_path = 'model.pth'\n","        torch.save(model.state_dict(), model_path)\n","        best_loss = total_train_loss/(n+1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yh7LqG-4D5MS","outputId":"7779005c-a1b2-43eb-c756-80effc4ad99c","executionInfo":{"status":"ok","timestamp":1692098753494,"user_tz":-540,"elapsed":4590812,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[train] Epoch 0 loss 290.834387 loss_coord 12.445968 loss_obj 5.408790 loss_noobj 224.300513: 100%|██████████| 128/128 [04:09<00:00,  1.95s/it]\n","[train] Epoch 1 loss 102.526837 loss_coord 10.894247 loss_obj 6.703145 loss_noobj 24.601143: 100%|██████████| 128/128 [00:26<00:00,  4.80it/s]\n","[train] Epoch 2 loss 100.849999 loss_coord 10.994535 loss_obj 7.062128 loss_noobj 19.234183: 100%|██████████| 128/128 [00:13<00:00,  9.29it/s]\n","[train] Epoch 3 loss 99.716162 loss_coord 9.858202 loss_obj 7.268399 loss_noobj 17.173968: 100%|██████████| 128/128 [00:12<00:00, 10.63it/s]\n","[train] Epoch 4 loss 92.835085 loss_coord 8.399418 loss_obj 6.959899 loss_noobj 14.836675: 100%|██████████| 128/128 [00:12<00:00, 10.05it/s]\n","[train] Epoch 5 loss 92.025763 loss_coord 7.582071 loss_obj 7.004475 loss_noobj 14.398947: 100%|██████████| 128/128 [00:12<00:00, 10.52it/s]\n","[train] Epoch 6 loss 95.748282 loss_coord 10.721276 loss_obj 7.059851 loss_noobj 14.428496: 100%|██████████| 128/128 [00:12<00:00, 10.11it/s]\n","[train] Epoch 7 loss 92.990771 loss_coord 9.157987 loss_obj 7.017506 loss_noobj 13.657729: 100%|██████████| 128/128 [00:12<00:00, 10.45it/s]\n","[train] Epoch 8 loss 93.281981 loss_coord 8.574481 loss_obj 7.060090 loss_noobj 14.106597: 100%|██████████| 128/128 [00:12<00:00, 10.40it/s]\n","[train] Epoch 9 loss 91.077172 loss_coord 7.874818 loss_obj 6.881965 loss_noobj 14.382703: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 10 loss 88.872320 loss_coord 7.515551 loss_obj 6.718500 loss_noobj 14.171767: 100%|██████████| 128/128 [00:18<00:00,  6.98it/s]\n","[train] Epoch 11 loss 87.472409 loss_coord 7.306136 loss_obj 6.664396 loss_noobj 13.522313: 100%|██████████| 128/128 [00:14<00:00,  8.85it/s]\n","[train] Epoch 12 loss 84.562305 loss_coord 7.048776 loss_obj 6.361816 loss_noobj 13.895364: 100%|██████████| 128/128 [00:14<00:00,  9.03it/s]\n","[train] Epoch 13 loss 81.573712 loss_coord 6.663935 loss_obj 6.079839 loss_noobj 14.111384: 100%|██████████| 128/128 [00:13<00:00,  9.27it/s]\n","[train] Epoch 14 loss 84.648953 loss_coord 6.967301 loss_obj 6.466624 loss_noobj 13.015414: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 15 loss 79.106210 loss_coord 6.508965 loss_obj 5.919729 loss_noobj 13.399956: 100%|██████████| 128/128 [00:12<00:00,  9.95it/s]\n","[train] Epoch 16 loss 81.549047 loss_coord 6.935897 loss_obj 6.101770 loss_noobj 13.595452: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 17 loss 78.137293 loss_coord 6.334720 loss_obj 5.819455 loss_noobj 13.608025: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 18 loss 73.990832 loss_coord 6.040688 loss_obj 5.485318 loss_noobj 13.096964: 100%|██████████| 128/128 [00:14<00:00,  9.14it/s]\n","[train] Epoch 19 loss 71.204250 loss_coord 5.940582 loss_obj 5.247128 loss_noobj 12.792391: 100%|██████████| 128/128 [00:12<00:00,  9.85it/s]\n","[train] Epoch 20 loss 72.198415 loss_coord 6.077867 loss_obj 5.358970 loss_noobj 12.530846: 100%|██████████| 128/128 [00:13<00:00,  9.62it/s]\n","[train] Epoch 21 loss 72.362649 loss_coord 6.432839 loss_obj 5.308853 loss_noobj 12.841281: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 22 loss 64.278895 loss_coord 5.222823 loss_obj 4.690657 loss_noobj 12.149497: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 23 loss 60.236472 loss_coord 5.467181 loss_obj 4.349450 loss_noobj 11.274791: 100%|██████████| 128/128 [00:13<00:00,  9.81it/s]\n","[train] Epoch 24 loss 58.769701 loss_coord 4.983015 loss_obj 4.270828 loss_noobj 11.078403: 100%|██████████| 128/128 [00:12<00:00,  9.87it/s]\n","[train] Epoch 25 loss 58.295959 loss_coord 5.724320 loss_obj 4.163266 loss_noobj 10.938984: 100%|██████████| 128/128 [00:12<00:00,  9.86it/s]\n","[train] Epoch 26 loss 56.488597 loss_coord 4.804600 loss_obj 4.030220 loss_noobj 11.381797: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 27 loss 54.885426 loss_coord 4.328325 loss_obj 3.931458 loss_noobj 11.242517: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 28 loss 50.833410 loss_coord 4.183442 loss_obj 3.647889 loss_noobj 10.171075: 100%|██████████| 128/128 [00:14<00:00,  8.93it/s]\n","[train] Epoch 29 loss 45.278769 loss_coord 4.445433 loss_obj 3.100605 loss_noobj 9.827283: 100%|██████████| 128/128 [00:14<00:00,  9.05it/s]\n","[train] Epoch 30 loss 44.715947 loss_coord 3.985897 loss_obj 3.149903 loss_noobj 9.231023: 100%|██████████| 128/128 [00:14<00:00,  9.07it/s]\n","[train] Epoch 31 loss 37.626102 loss_coord 3.469762 loss_obj 2.598982 loss_noobj 8.166518: 100%|██████████| 128/128 [00:16<00:00,  7.74it/s]\n","[train] Epoch 32 loss 46.551600 loss_coord 3.997640 loss_obj 3.202819 loss_noobj 10.525773: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 33 loss 35.030439 loss_coord 3.191964 loss_obj 2.376521 loss_noobj 8.073267: 100%|██████████| 128/128 [00:26<00:00,  4.91it/s]\n","[train] Epoch 34 loss 32.631568 loss_coord 3.118437 loss_obj 2.169934 loss_noobj 7.813787: 100%|██████████| 128/128 [00:23<00:00,  5.46it/s]\n","[train] Epoch 35 loss 30.592085 loss_coord 2.875733 loss_obj 2.004917 loss_noobj 7.667185: 100%|██████████| 128/128 [00:14<00:00,  8.86it/s]\n","[train] Epoch 36 loss 25.301844 loss_coord 2.709888 loss_obj 1.589511 loss_noobj 6.696842: 100%|██████████| 128/128 [00:27<00:00,  4.59it/s]\n","[train] Epoch 37 loss 22.448463 loss_coord 2.153882 loss_obj 1.454441 loss_noobj 5.750168: 100%|██████████| 128/128 [00:19<00:00,  6.68it/s]\n","[train] Epoch 38 loss 19.783515 loss_coord 1.930376 loss_obj 1.240219 loss_noobj 5.450950: 100%|██████████| 128/128 [00:12<00:00, 10.13it/s]\n","[train] Epoch 39 loss 17.863826 loss_coord 2.135396 loss_obj 1.117091 loss_noobj 4.557521: 100%|██████████| 128/128 [00:32<00:00,  3.91it/s]\n","[train] Epoch 40 loss 25.705048 loss_coord 2.621546 loss_obj 1.679844 loss_noobj 6.285059: 100%|██████████| 128/128 [00:12<00:00, 10.18it/s]\n","[train] Epoch 41 loss 22.669270 loss_coord 2.617711 loss_obj 1.363681 loss_noobj 6.414747: 100%|██████████| 128/128 [00:12<00:00, 10.11it/s]\n","[train] Epoch 42 loss 19.587958 loss_coord 2.402925 loss_obj 1.173379 loss_noobj 5.451240: 100%|██████████| 128/128 [00:12<00:00, 10.12it/s]\n","[train] Epoch 43 loss 16.668973 loss_coord 2.034136 loss_obj 1.004426 loss_noobj 4.590581: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 44 loss 14.497686 loss_coord 1.538140 loss_obj 0.902141 loss_noobj 3.938139: 100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n","[train] Epoch 45 loss 12.959048 loss_coord 1.532576 loss_obj 0.801552 loss_noobj 3.410949: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 46 loss 15.404617 loss_coord 1.402591 loss_obj 0.976360 loss_noobj 4.238421: 100%|██████████| 128/128 [00:12<00:00, 10.13it/s]\n","[train] Epoch 47 loss 12.569305 loss_coord 1.417681 loss_obj 0.734413 loss_noobj 3.807496: 100%|██████████| 128/128 [00:25<00:00,  4.99it/s]\n","[train] Epoch 48 loss 12.572155 loss_coord 1.370266 loss_obj 0.791351 loss_noobj 3.288375: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 49 loss 11.231356 loss_coord 1.463364 loss_obj 0.650561 loss_noobj 3.262380: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 50 loss 8.766471 loss_coord 1.130571 loss_obj 0.508723 loss_noobj 2.548671: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 51 loss 9.166340 loss_coord 1.138006 loss_obj 0.528630 loss_noobj 2.742033: 100%|██████████| 128/128 [00:25<00:00,  4.95it/s]\n","[train] Epoch 52 loss 19.706268 loss_coord 1.486923 loss_obj 1.389182 loss_noobj 4.327521: 100%|██████████| 128/128 [00:22<00:00,  5.59it/s]\n","[train] Epoch 53 loss 31.507730 loss_coord 2.436823 loss_obj 2.064026 loss_noobj 8.430651: 100%|██████████| 128/128 [00:12<00:00, 10.17it/s]\n","[train] Epoch 54 loss 15.753670 loss_coord 1.763131 loss_obj 0.906132 loss_noobj 4.929220: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 55 loss 9.150148 loss_coord 1.278134 loss_obj 0.498878 loss_noobj 2.883233: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 56 loss 7.570285 loss_coord 1.249109 loss_obj 0.408427 loss_noobj 2.236905: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 57 loss 6.603117 loss_coord 1.131082 loss_obj 0.367550 loss_noobj 1.796534: 100%|██████████| 128/128 [00:22<00:00,  5.65it/s]\n","[train] Epoch 58 loss 5.994382 loss_coord 0.750579 loss_obj 0.350744 loss_noobj 1.736364: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 59 loss 3.936085 loss_coord 0.695124 loss_obj 0.205978 loss_noobj 1.181181: 100%|██████████| 128/128 [00:27<00:00,  4.67it/s]\n","[train] Epoch 60 loss 3.847996 loss_coord 0.703348 loss_obj 0.204467 loss_noobj 1.099979: 100%|██████████| 128/128 [00:12<00:00, 10.17it/s]\n","[train] Epoch 61 loss 3.669541 loss_coord 0.768334 loss_obj 0.179115 loss_noobj 1.110053: 100%|██████████| 128/128 [00:27<00:00,  4.61it/s]\n","[train] Epoch 62 loss 3.713819 loss_coord 0.796574 loss_obj 0.192029 loss_noobj 0.996951: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 63 loss 5.237088 loss_coord 0.747571 loss_obj 0.263056 loss_noobj 1.858960: 100%|██████████| 128/128 [00:12<00:00, 10.11it/s]\n","[train] Epoch 64 loss 7.510081 loss_coord 1.022111 loss_obj 0.416678 loss_noobj 2.321190: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 65 loss 5.328820 loss_coord 0.788810 loss_obj 0.260048 loss_noobj 1.939533: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 66 loss 5.570145 loss_coord 1.141796 loss_obj 0.284081 loss_noobj 1.587536: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 67 loss 4.109408 loss_coord 0.824113 loss_obj 0.178630 loss_noobj 1.498996: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 68 loss 6.679965 loss_coord 0.938219 loss_obj 0.349648 loss_noobj 2.245267: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 69 loss 17.220951 loss_coord 1.231643 loss_obj 1.037100 loss_noobj 5.618303: 100%|██████████| 128/128 [00:12<00:00, 10.10it/s]\n","[train] Epoch 70 loss 7.708924 loss_coord 0.998970 loss_obj 0.381819 loss_noobj 2.891768: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 71 loss 3.083774 loss_coord 0.774601 loss_obj 0.118201 loss_noobj 1.127166: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 72 loss 1.313752 loss_coord 0.449091 loss_obj 0.036406 loss_noobj 0.500601: 100%|██████████| 128/128 [00:24<00:00,  5.18it/s]\n","[train] Epoch 73 loss 0.746392 loss_coord 0.311571 loss_obj 0.020034 loss_noobj 0.234482: 100%|██████████| 128/128 [01:30<00:00,  1.42it/s]\n","[train] Epoch 74 loss 0.458394 loss_coord 0.236067 loss_obj 0.008949 loss_noobj 0.132834: 100%|██████████| 128/128 [00:23<00:00,  5.43it/s]\n","[train] Epoch 75 loss 0.336273 loss_coord 0.172053 loss_obj 0.008246 loss_noobj 0.081763: 100%|██████████| 128/128 [00:34<00:00,  3.67it/s]\n","[train] Epoch 76 loss 0.313092 loss_coord 0.178555 loss_obj 0.006932 loss_noobj 0.065212: 100%|██████████| 128/128 [00:16<00:00,  7.57it/s]\n","[train] Epoch 77 loss 0.344673 loss_coord 0.196895 loss_obj 0.005470 loss_noobj 0.093076: 100%|██████████| 128/128 [00:28<00:00,  4.50it/s]\n","[train] Epoch 78 loss 0.362975 loss_coord 0.254482 loss_obj 0.005754 loss_noobj 0.050955: 100%|██████████| 128/128 [00:12<00:00, 10.17it/s]\n","[train] Epoch 79 loss 0.471437 loss_coord 0.351322 loss_obj 0.004741 loss_noobj 0.072704: 100%|██████████| 128/128 [00:12<00:00, 10.11it/s]\n","[train] Epoch 80 loss 3.093486 loss_coord 0.472604 loss_obj 0.178719 loss_noobj 0.833690: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 81 loss 33.177840 loss_coord 2.548905 loss_obj 2.278122 loss_noobj 7.847715: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 82 loss 34.462794 loss_coord 2.994234 loss_obj 2.345051 loss_noobj 8.018048: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 83 loss 14.833094 loss_coord 1.589744 loss_obj 0.866551 loss_noobj 4.577842: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 84 loss 5.998565 loss_coord 1.046258 loss_obj 0.253183 loss_noobj 2.420479: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 85 loss 3.205279 loss_coord 0.822917 loss_obj 0.113692 loss_noobj 1.245444: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 86 loss 1.494116 loss_coord 0.533669 loss_obj 0.040185 loss_noobj 0.558599: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 87 loss 1.218707 loss_coord 0.396201 loss_obj 0.036617 loss_noobj 0.456335: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 88 loss 1.079962 loss_coord 0.397991 loss_obj 0.030783 loss_noobj 0.374144: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 89 loss 0.860025 loss_coord 0.435655 loss_obj 0.019803 loss_noobj 0.226336: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 90 loss 0.444730 loss_coord 0.215971 loss_obj 0.010690 loss_noobj 0.121862: 100%|██████████| 128/128 [00:12<00:00, 10.00it/s]\n","[train] Epoch 91 loss 0.372152 loss_coord 0.200743 loss_obj 0.008582 loss_noobj 0.085592: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 92 loss 0.361741 loss_coord 0.203673 loss_obj 0.004961 loss_noobj 0.108456: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 93 loss 0.372399 loss_coord 0.257378 loss_obj 0.005916 loss_noobj 0.055856: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 94 loss 0.427982 loss_coord 0.318472 loss_obj 0.004700 loss_noobj 0.062506: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 95 loss 0.498983 loss_coord 0.404838 loss_obj 0.005501 loss_noobj 0.039138: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 96 loss 0.620588 loss_coord 0.516865 loss_obj 0.004473 loss_noobj 0.058994: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 97 loss 0.621927 loss_coord 0.531997 loss_obj 0.005395 loss_noobj 0.035978: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 98 loss 0.638846 loss_coord 0.561535 loss_obj 0.004154 loss_noobj 0.035768: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 99 loss 0.479938 loss_coord 0.400365 loss_obj 0.004655 loss_noobj 0.033025: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 100 loss 0.539640 loss_coord 0.466719 loss_obj 0.004114 loss_noobj 0.031784: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 101 loss 0.510500 loss_coord 0.427237 loss_obj 0.004562 loss_noobj 0.037638: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 102 loss 0.485217 loss_coord 0.373968 loss_obj 0.004232 loss_noobj 0.068931: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 103 loss 0.677413 loss_coord 0.483098 loss_obj 0.014937 loss_noobj 0.044941: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 104 loss 23.934596 loss_coord 2.006496 loss_obj 1.664818 loss_noobj 5.279921: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 105 loss 33.666280 loss_coord 2.460443 loss_obj 2.306828 loss_noobj 8.137557: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 106 loss 17.575609 loss_coord 1.801168 loss_obj 1.041741 loss_noobj 5.357033: 100%|██████████| 128/128 [00:12<00:00, 10.16it/s]\n","[train] Epoch 107 loss 5.156908 loss_coord 1.033754 loss_obj 0.208966 loss_noobj 2.033493: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 108 loss 2.975669 loss_coord 0.658689 loss_obj 0.122759 loss_noobj 1.089385: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 109 loss 1.247065 loss_coord 0.451489 loss_obj 0.033796 loss_noobj 0.457616: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 110 loss 0.775856 loss_coord 0.319399 loss_obj 0.020226 loss_noobj 0.254199: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 111 loss 0.435053 loss_coord 0.192832 loss_obj 0.009876 loss_noobj 0.143458: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 112 loss 0.298830 loss_coord 0.122779 loss_obj 0.007782 loss_noobj 0.098229: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 113 loss 0.230084 loss_coord 0.086902 loss_obj 0.006474 loss_noobj 0.078446: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 114 loss 0.217427 loss_coord 0.089618 loss_obj 0.005331 loss_noobj 0.074498: 100%|██████████| 128/128 [00:17<00:00,  7.26it/s]\n","[train] Epoch 115 loss 0.202591 loss_coord 0.087888 loss_obj 0.005596 loss_noobj 0.058747: 100%|██████████| 128/128 [00:20<00:00,  6.39it/s]\n","[train] Epoch 116 loss 0.251119 loss_coord 0.134630 loss_obj 0.004154 loss_noobj 0.074950: 100%|██████████| 128/128 [00:12<00:00, 10.17it/s]\n","[train] Epoch 117 loss 0.275390 loss_coord 0.196668 loss_obj 0.003412 loss_noobj 0.044604: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 118 loss 0.430202 loss_coord 0.353506 loss_obj 0.003945 loss_noobj 0.037245: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 119 loss 0.674126 loss_coord 0.604604 loss_obj 0.003388 loss_noobj 0.035639: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 120 loss 0.642297 loss_coord 0.532109 loss_obj 0.005618 loss_noobj 0.054010: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 121 loss 0.586014 loss_coord 0.508768 loss_obj 0.003785 loss_noobj 0.039396: 100%|██████████| 128/128 [00:12<00:00, 10.17it/s]\n","[train] Epoch 122 loss 0.535552 loss_coord 0.468628 loss_obj 0.003534 loss_noobj 0.031584: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 123 loss 0.496983 loss_coord 0.432348 loss_obj 0.003354 loss_noobj 0.031099: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 124 loss 0.442968 loss_coord 0.382857 loss_obj 0.003088 loss_noobj 0.029233: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 125 loss 0.389205 loss_coord 0.331713 loss_obj 0.003212 loss_noobj 0.025375: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 126 loss 0.356769 loss_coord 0.289633 loss_obj 0.003542 loss_noobj 0.031713: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 127 loss 0.700835 loss_coord 0.411458 loss_obj 0.013214 loss_noobj 0.157239: 100%|██████████| 128/128 [00:12<00:00, 10.38it/s]\n","[train] Epoch 128 loss 17.460474 loss_coord 1.083329 loss_obj 1.260065 loss_noobj 3.776493: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 129 loss 34.007637 loss_coord 2.595668 loss_obj 2.203721 loss_noobj 9.374762: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 130 loss 10.487088 loss_coord 1.346883 loss_obj 0.532092 loss_noobj 3.819284: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 131 loss 2.606229 loss_coord 0.850680 loss_obj 0.082332 loss_noobj 0.932234: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 132 loss 0.906575 loss_coord 0.327042 loss_obj 0.022077 loss_noobj 0.358758: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 133 loss 0.480662 loss_coord 0.165248 loss_obj 0.012448 loss_noobj 0.190938: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 134 loss 0.319231 loss_coord 0.086317 loss_obj 0.010135 loss_noobj 0.131565: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 135 loss 0.238304 loss_coord 0.063525 loss_obj 0.007721 loss_noobj 0.097568: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 136 loss 0.189252 loss_coord 0.048071 loss_obj 0.006179 loss_noobj 0.079388: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 137 loss 0.158964 loss_coord 0.041766 loss_obj 0.005330 loss_noobj 0.063901: 100%|██████████| 128/128 [00:18<00:00,  7.00it/s]\n","[train] Epoch 138 loss 0.138398 loss_coord 0.038573 loss_obj 0.004400 loss_noobj 0.055821: 100%|██████████| 128/128 [00:31<00:00,  4.08it/s]\n","[train] Epoch 139 loss 0.145181 loss_coord 0.060774 loss_obj 0.003782 loss_noobj 0.046584: 100%|██████████| 128/128 [00:12<00:00, 10.11it/s]\n","[train] Epoch 140 loss 0.147152 loss_coord 0.073634 loss_obj 0.003213 loss_noobj 0.041389: 100%|██████████| 128/128 [00:23<00:00,  5.36it/s]\n","[train] Epoch 141 loss 0.160481 loss_coord 0.096078 loss_obj 0.002890 loss_noobj 0.035507: 100%|██████████| 128/128 [00:15<00:00,  8.40it/s]\n","[train] Epoch 142 loss 0.231565 loss_coord 0.173415 loss_obj 0.002591 loss_noobj 0.032244: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 143 loss 0.339025 loss_coord 0.283430 loss_obj 0.002490 loss_noobj 0.030695: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 144 loss 0.480532 loss_coord 0.420784 loss_obj 0.003202 loss_noobj 0.027729: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 145 loss 0.481998 loss_coord 0.415268 loss_obj 0.002270 loss_noobj 0.044029: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 146 loss 0.372997 loss_coord 0.315966 loss_obj 0.003302 loss_noobj 0.024010: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 147 loss 0.357223 loss_coord 0.306916 loss_obj 0.001840 loss_noobj 0.031903: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 148 loss 0.338665 loss_coord 0.295017 loss_obj 0.002340 loss_noobj 0.020247: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 149 loss 0.360814 loss_coord 0.318872 loss_obj 0.001858 loss_noobj 0.023363: 100%|██████████| 128/128 [00:12<00:00, 10.08it/s]\n","[train] Epoch 150 loss 0.315493 loss_coord 0.275027 loss_obj 0.002179 loss_noobj 0.018671: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 151 loss 0.256823 loss_coord 0.221993 loss_obj 0.001750 loss_noobj 0.017326: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 152 loss 0.264776 loss_coord 0.234564 loss_obj 0.001627 loss_noobj 0.013937: 100%|██████████| 128/128 [00:18<00:00,  6.96it/s]\n","[train] Epoch 153 loss 0.255475 loss_coord 0.226607 loss_obj 0.001514 loss_noobj 0.013728: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 154 loss 0.256126 loss_coord 0.228588 loss_obj 0.001370 loss_noobj 0.013843: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 155 loss 0.285488 loss_coord 0.258913 loss_obj 0.001518 loss_noobj 0.011396: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 156 loss 0.328717 loss_coord 0.301431 loss_obj 0.001444 loss_noobj 0.012842: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 157 loss 0.422020 loss_coord 0.394806 loss_obj 0.001453 loss_noobj 0.012685: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 158 loss 0.438842 loss_coord 0.409094 loss_obj 0.001610 loss_noobj 0.013643: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 159 loss 0.362342 loss_coord 0.334237 loss_obj 0.001617 loss_noobj 0.011940: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 160 loss 0.319568 loss_coord 0.286253 loss_obj 0.001880 loss_noobj 0.014514: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 161 loss 0.358391 loss_coord 0.329110 loss_obj 0.001627 loss_noobj 0.013006: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 162 loss 0.436477 loss_coord 0.408012 loss_obj 0.001485 loss_noobj 0.013617: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 163 loss 0.445098 loss_coord 0.407316 loss_obj 0.002141 loss_noobj 0.016369: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 164 loss 0.397110 loss_coord 0.338513 loss_obj 0.002308 loss_noobj 0.035515: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 165 loss 32.550979 loss_coord 2.146915 loss_obj 2.659804 loss_noobj 3.806028: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 166 loss 51.994948 loss_coord 3.638144 loss_obj 3.795743 loss_noobj 10.399378: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 167 loss 8.670323 loss_coord 1.536332 loss_obj 0.390361 loss_noobj 3.230380: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 168 loss 2.780315 loss_coord 0.657106 loss_obj 0.100923 loss_noobj 1.113975: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 169 loss 1.308598 loss_coord 0.271414 loss_obj 0.048429 loss_noobj 0.552898: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 170 loss 0.694323 loss_coord 0.137658 loss_obj 0.022386 loss_noobj 0.332806: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 171 loss 0.525488 loss_coord 0.095681 loss_obj 0.019334 loss_noobj 0.236471: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 172 loss 0.349454 loss_coord 0.067400 loss_obj 0.012218 loss_noobj 0.159876: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 173 loss 0.248118 loss_coord 0.044501 loss_obj 0.007120 loss_noobj 0.132413: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 174 loss 0.208980 loss_coord 0.034912 loss_obj 0.007510 loss_noobj 0.098972: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 175 loss 0.164607 loss_coord 0.031589 loss_obj 0.005272 loss_noobj 0.080296: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 176 loss 0.153861 loss_coord 0.042840 loss_obj 0.004325 loss_noobj 0.067775: 100%|██████████| 128/128 [00:12<00:00, 10.18it/s]\n","[train] Epoch 177 loss 0.174104 loss_coord 0.079393 loss_obj 0.003842 loss_noobj 0.056287: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 178 loss 0.146441 loss_coord 0.063328 loss_obj 0.003210 loss_noobj 0.051013: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 179 loss 0.157656 loss_coord 0.085139 loss_obj 0.003020 loss_noobj 0.042316: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 180 loss 0.179611 loss_coord 0.114598 loss_obj 0.002539 loss_noobj 0.039623: 100%|██████████| 128/128 [00:19<00:00,  6.63it/s]\n","[train] Epoch 181 loss 0.220261 loss_coord 0.161174 loss_obj 0.002518 loss_noobj 0.033910: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 182 loss 0.246118 loss_coord 0.192077 loss_obj 0.002199 loss_noobj 0.032053: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 183 loss 0.271218 loss_coord 0.221587 loss_obj 0.002043 loss_noobj 0.029199: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 184 loss 0.254587 loss_coord 0.207802 loss_obj 0.001889 loss_noobj 0.027893: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 185 loss 0.198194 loss_coord 0.156192 loss_obj 0.001844 loss_noobj 0.023564: 100%|██████████| 128/128 [00:12<00:00, 10.18it/s]\n","[train] Epoch 186 loss 0.219662 loss_coord 0.180397 loss_obj 0.001571 loss_noobj 0.023551: 100%|██████████| 128/128 [00:18<00:00,  6.94it/s]\n","[train] Epoch 187 loss 0.265201 loss_coord 0.228974 loss_obj 0.001604 loss_noobj 0.020183: 100%|██████████| 128/128 [00:14<00:00,  9.07it/s]\n","[train] Epoch 188 loss 0.235885 loss_coord 0.201253 loss_obj 0.001482 loss_noobj 0.019813: 100%|██████████| 128/128 [00:12<00:00, 10.09it/s]\n","[train] Epoch 189 loss 0.235003 loss_coord 0.201797 loss_obj 0.001502 loss_noobj 0.018182: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 190 loss 0.229734 loss_coord 0.196743 loss_obj 0.001603 loss_noobj 0.016958: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 191 loss 0.343060 loss_coord 0.310513 loss_obj 0.001471 loss_noobj 0.017839: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 192 loss 0.371823 loss_coord 0.311116 loss_obj 0.002910 loss_noobj 0.031607: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 193 loss 24.052698 loss_coord 1.568287 loss_obj 1.741676 loss_noobj 5.067649: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 194 loss 20.482082 loss_coord 1.866876 loss_obj 1.224495 loss_noobj 6.370255: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 195 loss 4.805590 loss_coord 0.865276 loss_obj 0.193155 loss_noobj 2.008768: 100%|██████████| 128/128 [00:12<00:00, 10.09it/s]\n","[train] Epoch 196 loss 2.039276 loss_coord 0.435404 loss_obj 0.065976 loss_noobj 0.944117: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 197 loss 0.677929 loss_coord 0.285374 loss_obj 0.013138 loss_noobj 0.261172: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 198 loss 0.364919 loss_coord 0.144897 loss_obj 0.008578 loss_noobj 0.134248: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 199 loss 0.218204 loss_coord 0.079746 loss_obj 0.005605 loss_noobj 0.082411: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 200 loss 0.163824 loss_coord 0.053229 loss_obj 0.004595 loss_noobj 0.064645: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 201 loss 0.130954 loss_coord 0.038617 loss_obj 0.003872 loss_noobj 0.053618: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 202 loss 0.110533 loss_coord 0.032117 loss_obj 0.003317 loss_noobj 0.045242: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 203 loss 0.091703 loss_coord 0.023981 loss_obj 0.002875 loss_noobj 0.038972: 100%|██████████| 128/128 [00:29<00:00,  4.33it/s]\n","[train] Epoch 204 loss 0.086584 loss_coord 0.027769 loss_obj 0.002469 loss_noobj 0.034123: 100%|██████████| 128/128 [00:23<00:00,  5.37it/s]\n","[train] Epoch 205 loss 0.083488 loss_coord 0.031743 loss_obj 0.002191 loss_noobj 0.029836: 100%|██████████| 128/128 [00:27<00:00,  4.69it/s]\n","[train] Epoch 206 loss 0.097056 loss_coord 0.050932 loss_obj 0.001957 loss_noobj 0.026550: 100%|██████████| 128/128 [00:19<00:00,  6.55it/s]\n","[train] Epoch 207 loss 0.100171 loss_coord 0.058875 loss_obj 0.001745 loss_noobj 0.023850: 100%|██████████| 128/128 [00:12<00:00, 10.03it/s]\n","[train] Epoch 208 loss 0.086282 loss_coord 0.049380 loss_obj 0.001550 loss_noobj 0.021404: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 209 loss 0.138058 loss_coord 0.103905 loss_obj 0.001542 loss_noobj 0.018733: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 210 loss 0.212082 loss_coord 0.180247 loss_obj 0.001306 loss_noobj 0.018771: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 211 loss 0.387326 loss_coord 0.355953 loss_obj 0.001554 loss_noobj 0.015836: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 212 loss 0.377075 loss_coord 0.344751 loss_obj 0.001513 loss_noobj 0.017195: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 213 loss 0.423403 loss_coord 0.391761 loss_obj 0.001498 loss_noobj 0.016666: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 214 loss 0.425598 loss_coord 0.394306 loss_obj 0.001563 loss_noobj 0.015660: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 215 loss 0.306503 loss_coord 0.279285 loss_obj 0.001184 loss_noobj 0.015381: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 216 loss 0.204897 loss_coord 0.180215 loss_obj 0.001131 loss_noobj 0.013368: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 217 loss 0.155622 loss_coord 0.133362 loss_obj 0.001116 loss_noobj 0.011097: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 218 loss 0.148113 loss_coord 0.128008 loss_obj 0.000913 loss_noobj 0.010978: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 219 loss 0.123277 loss_coord 0.105054 loss_obj 0.000876 loss_noobj 0.009464: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 220 loss 0.137993 loss_coord 0.120987 loss_obj 0.000818 loss_noobj 0.008828: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 221 loss 0.189753 loss_coord 0.172754 loss_obj 0.000895 loss_noobj 0.008050: 100%|██████████| 128/128 [00:12<00:00, 10.12it/s]\n","[train] Epoch 222 loss 0.237784 loss_coord 0.221352 loss_obj 0.000949 loss_noobj 0.006941: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 223 loss 0.268294 loss_coord 0.250146 loss_obj 0.000953 loss_noobj 0.008619: 100%|██████████| 128/128 [00:12<00:00, 10.16it/s]\n","[train] Epoch 224 loss 0.274154 loss_coord 0.255843 loss_obj 0.000968 loss_noobj 0.008628: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 225 loss 0.293333 loss_coord 0.272383 loss_obj 0.001226 loss_noobj 0.008690: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 226 loss 0.310271 loss_coord 0.287126 loss_obj 0.001440 loss_noobj 0.008743: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 227 loss 0.302969 loss_coord 0.283066 loss_obj 0.001068 loss_noobj 0.009228: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 228 loss 0.359849 loss_coord 0.338471 loss_obj 0.001218 loss_noobj 0.009194: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 229 loss 5.422374 loss_coord 0.580561 loss_obj 0.347964 loss_noobj 1.362170: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 230 loss 39.245273 loss_coord 2.596438 loss_obj 2.837096 loss_noobj 8.277872: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 231 loss 7.612853 loss_coord 1.151979 loss_obj 0.333905 loss_noobj 3.121823: 100%|██████████| 128/128 [00:12<00:00, 10.16it/s]\n","[train] Epoch 232 loss 3.078201 loss_coord 0.648833 loss_obj 0.124080 loss_noobj 1.188571: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 233 loss 1.473800 loss_coord 0.378098 loss_obj 0.063955 loss_noobj 0.456153: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 234 loss 0.435410 loss_coord 0.179715 loss_obj 0.010777 loss_noobj 0.147922: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 235 loss 0.249436 loss_coord 0.082129 loss_obj 0.005931 loss_noobj 0.107996: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 236 loss 0.180924 loss_coord 0.057698 loss_obj 0.004897 loss_noobj 0.074255: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 237 loss 0.133972 loss_coord 0.033030 loss_obj 0.004068 loss_noobj 0.060265: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 238 loss 0.106716 loss_coord 0.021641 loss_obj 0.003437 loss_noobj 0.050703: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 239 loss 0.088646 loss_coord 0.015737 loss_obj 0.002978 loss_noobj 0.043132: 100%|██████████| 128/128 [00:12<00:00, 10.11it/s]\n","[train] Epoch 240 loss 0.076324 loss_coord 0.013049 loss_obj 0.002576 loss_noobj 0.037512: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 241 loss 0.067648 loss_coord 0.012262 loss_obj 0.002275 loss_noobj 0.032631: 100%|██████████| 128/128 [00:25<00:00,  4.98it/s]\n","[train] Epoch 242 loss 0.062213 loss_coord 0.013311 loss_obj 0.001994 loss_noobj 0.028961: 100%|██████████| 128/128 [00:34<00:00,  3.76it/s]\n","[train] Epoch 243 loss 0.060693 loss_coord 0.017210 loss_obj 0.001802 loss_noobj 0.025465: 100%|██████████| 128/128 [00:16<00:00,  7.88it/s]\n","[train] Epoch 244 loss 0.071393 loss_coord 0.032498 loss_obj 0.001584 loss_noobj 0.023055: 100%|██████████| 128/128 [00:12<00:00, 10.08it/s]\n","[train] Epoch 245 loss 0.099899 loss_coord 0.064399 loss_obj 0.001510 loss_noobj 0.020397: 100%|██████████| 128/128 [00:12<00:00, 10.14it/s]\n","[train] Epoch 246 loss 0.135044 loss_coord 0.102930 loss_obj 0.001323 loss_noobj 0.018886: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 247 loss 0.170456 loss_coord 0.140123 loss_obj 0.001332 loss_noobj 0.017017: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 248 loss 0.170999 loss_coord 0.143431 loss_obj 0.001119 loss_noobj 0.016375: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 249 loss 0.201647 loss_coord 0.175992 loss_obj 0.001058 loss_noobj 0.015077: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 250 loss 0.241591 loss_coord 0.216417 loss_obj 0.001090 loss_noobj 0.014278: 100%|██████████| 128/128 [00:12<00:00, 10.08it/s]\n","[train] Epoch 251 loss 0.321822 loss_coord 0.297500 loss_obj 0.001077 loss_noobj 0.013548: 100%|██████████| 128/128 [00:12<00:00, 10.14it/s]\n","[train] Epoch 252 loss 0.328220 loss_coord 0.303385 loss_obj 0.001266 loss_noobj 0.012178: 100%|██████████| 128/128 [00:12<00:00,  9.87it/s]\n","[train] Epoch 253 loss 0.249088 loss_coord 0.226281 loss_obj 0.001007 loss_noobj 0.012736: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 254 loss 0.251397 loss_coord 0.229607 loss_obj 0.001053 loss_noobj 0.011259: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 255 loss 0.187543 loss_coord 0.167191 loss_obj 0.000972 loss_noobj 0.010634: 100%|██████████| 128/128 [00:12<00:00, 10.13it/s]\n","[train] Epoch 256 loss 0.185413 loss_coord 0.166414 loss_obj 0.000906 loss_noobj 0.009943: 100%|██████████| 128/128 [00:12<00:00, 10.16it/s]\n","[train] Epoch 257 loss 0.203383 loss_coord 0.185669 loss_obj 0.000854 loss_noobj 0.009175: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 258 loss 0.197173 loss_coord 0.180325 loss_obj 0.000832 loss_noobj 0.008527: 100%|██████████| 128/128 [00:12<00:00, 10.16it/s]\n","[train] Epoch 259 loss 0.220224 loss_coord 0.203255 loss_obj 0.000873 loss_noobj 0.008236: 100%|██████████| 128/128 [00:12<00:00, 10.13it/s]\n","[train] Epoch 260 loss 0.231318 loss_coord 0.215357 loss_obj 0.000869 loss_noobj 0.007269: 100%|██████████| 128/128 [00:12<00:00, 10.09it/s]\n","[train] Epoch 261 loss 0.233199 loss_coord 0.215444 loss_obj 0.000926 loss_noobj 0.008494: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 262 loss 0.245798 loss_coord 0.229198 loss_obj 0.000906 loss_noobj 0.007539: 100%|██████████| 128/128 [00:12<00:00, 10.18it/s]\n","[train] Epoch 263 loss 0.249281 loss_coord 0.232876 loss_obj 0.000895 loss_noobj 0.007459: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 264 loss 0.252289 loss_coord 0.225363 loss_obj 0.001180 loss_noobj 0.015131: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 265 loss 0.463517 loss_coord 0.252454 loss_obj 0.013388 loss_noobj 0.077182: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 266 loss 29.848738 loss_coord 2.776541 loss_obj 2.233991 loss_noobj 4.732290: 100%|██████████| 128/128 [00:12<00:00, 10.17it/s]\n","[train] Epoch 267 loss 15.176280 loss_coord 1.595568 loss_obj 0.842106 loss_noobj 5.159657: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 268 loss 2.554721 loss_coord 0.539711 loss_obj 0.084603 loss_noobj 1.168982: 100%|██████████| 128/128 [00:12<00:00, 10.05it/s]\n","[train] Epoch 269 loss 1.296653 loss_coord 0.276217 loss_obj 0.052900 loss_noobj 0.491435: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 270 loss 0.801541 loss_coord 0.203907 loss_obj 0.022304 loss_noobj 0.374589: 100%|██████████| 128/128 [00:12<00:00, 10.09it/s]\n","[train] Epoch 271 loss 0.405295 loss_coord 0.176566 loss_obj 0.009490 loss_noobj 0.133832: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 272 loss 0.194359 loss_coord 0.075827 loss_obj 0.004325 loss_noobj 0.075281: 100%|██████████| 128/128 [00:12<00:00, 10.16it/s]\n","[train] Epoch 273 loss 0.131413 loss_coord 0.037120 loss_obj 0.003675 loss_noobj 0.057545: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 274 loss 0.098970 loss_coord 0.019759 loss_obj 0.003125 loss_noobj 0.047963: 100%|██████████| 128/128 [00:12<00:00, 10.18it/s]\n","[train] Epoch 275 loss 0.080116 loss_coord 0.012449 loss_obj 0.002680 loss_noobj 0.040867: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 276 loss 0.067161 loss_coord 0.008580 loss_obj 0.002328 loss_noobj 0.035297: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 277 loss 0.057383 loss_coord 0.006208 loss_obj 0.002041 loss_noobj 0.030763: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 278 loss 0.049868 loss_coord 0.004792 loss_obj 0.001798 loss_noobj 0.027095: 100%|██████████| 128/128 [00:32<00:00,  3.92it/s]\n","[train] Epoch 279 loss 0.043471 loss_coord 0.003520 loss_obj 0.001601 loss_noobj 0.023937: 100%|██████████| 128/128 [00:29<00:00,  4.36it/s]\n","[train] Epoch 280 loss 0.039425 loss_coord 0.003829 loss_obj 0.001426 loss_noobj 0.021340: 100%|██████████| 128/128 [00:27<00:00,  4.70it/s]\n","[train] Epoch 281 loss 0.036583 loss_coord 0.004720 loss_obj 0.001281 loss_noobj 0.019053: 100%|██████████| 128/128 [00:28<00:00,  4.52it/s]\n","[train] Epoch 282 loss 0.035282 loss_coord 0.006625 loss_obj 0.001159 loss_noobj 0.017069: 100%|██████████| 128/128 [00:30<00:00,  4.17it/s]\n","[train] Epoch 283 loss 0.036232 loss_coord 0.010337 loss_obj 0.001050 loss_noobj 0.015399: 100%|██████████| 128/128 [00:14<00:00,  8.96it/s]\n","[train] Epoch 284 loss 0.045744 loss_coord 0.022240 loss_obj 0.000961 loss_noobj 0.013890: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 285 loss 0.086390 loss_coord 0.064749 loss_obj 0.000866 loss_noobj 0.012985: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 286 loss 0.173482 loss_coord 0.152829 loss_obj 0.000874 loss_noobj 0.011917: 100%|██████████| 128/128 [00:12<00:00, 10.22it/s]\n","[train] Epoch 287 loss 0.288003 loss_coord 0.267583 loss_obj 0.000788 loss_noobj 0.012538: 100%|██████████| 128/128 [00:12<00:00, 10.39it/s]\n","[train] Epoch 288 loss 0.347531 loss_coord 0.325107 loss_obj 0.001053 loss_noobj 0.011897: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 289 loss 0.310586 loss_coord 0.287528 loss_obj 0.000747 loss_noobj 0.015593: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 290 loss 0.288482 loss_coord 0.270204 loss_obj 0.000725 loss_noobj 0.011025: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 291 loss 0.263656 loss_coord 0.245687 loss_obj 0.000809 loss_noobj 0.009876: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 292 loss 0.232534 loss_coord 0.215978 loss_obj 0.000696 loss_noobj 0.009599: 100%|██████████| 128/128 [00:12<00:00, 10.10it/s]\n","[train] Epoch 293 loss 0.166797 loss_coord 0.152094 loss_obj 0.000655 loss_noobj 0.008157: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 294 loss 0.123126 loss_coord 0.109540 loss_obj 0.000616 loss_noobj 0.007427: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 295 loss 0.103402 loss_coord 0.091132 loss_obj 0.000545 loss_noobj 0.006821: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 296 loss 0.096848 loss_coord 0.085015 loss_obj 0.000581 loss_noobj 0.006024: 100%|██████████| 128/128 [00:12<00:00, 10.13it/s]\n","[train] Epoch 297 loss 0.119565 loss_coord 0.108413 loss_obj 0.000535 loss_noobj 0.005806: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 298 loss 0.146259 loss_coord 0.135245 loss_obj 0.000535 loss_noobj 0.005666: 100%|██████████| 128/128 [00:12<00:00, 10.25it/s]\n","[train] Epoch 299 loss 0.171430 loss_coord 0.160701 loss_obj 0.000438 loss_noobj 0.006349: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n"]}]},{"cell_type":"code","source":["lambda_coord = 1\n","lambda_obj = 10\n","lambda_noobj = 1"],"metadata":{"id":"ZYjXPB0ND8zQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 結果確認\n","\n","まぁまぁといったところか\n","- 学習量も少なく、データ拡張も行っていないため、この程度であろう"],"metadata":{"id":"SrGwGrGUseUE"}},{"cell_type":"code","source":["model_path = 'model.pth'\n","model.load_state_dict(torch.load(model_path))\n","model = model.cuda()\n","for path in img_list:\n","  img = cv2.imread(path)\n","  img = cv2.resize(img , (img_size , img_size))\n","  img = Image.fromarray(img)\n","\n","  img = transform(img).unsqueeze(0).cuda()\n","  with torch.no_grad():\n","    preds  = list(model(img))\n","  img = cv2.imread(path)[:,:,::-1]\n","  img = cv2.resize(img , (img_size , img_size))\n","  img = torch.tensor(img.transpose(2,0,1))\n","  for color,pred in zip([\"red\",\"green\",\"blue\"],preds):\n","    bbox_list, obj_list = visualization(pred, anchor, img_size, conf = 0.9)\n","    if len(bbox_list) != 0:\n","      obj_label_list = []\n","      for obj_item in obj_list:\n","        obj_label_list.append(obj_labels[obj_item])\n","      img = draw_bounding_boxes(img, torch.tensor(bbox_list), colors=color, width=1, labels=obj_label_list)\n","  show(img)"],"metadata":{"id":"1rZUOYDvD-_y","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LEZqElLCRsRAHx-s0ZYu0QnO1Mpogt8i"},"executionInfo":{"status":"ok","timestamp":1692098832060,"user_tz":-540,"elapsed":78567,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"fa7ba492-ffb2-4e0c-844f-47c616a91001"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# YoLoのその後\n","\n","YoLoの生みの親、Joseph Redmonは、先に述べた通りYoLo V3で開発をやめ、ソーシャルメディアも閉じている\n","\n","YoLov3の論文も、それまでとはことなり、口語調で記述されており、ある意味雑な論文である\n","- 例えば、次のような文章から論文は始まる\"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry.\"\n","- 訳(DeepL)：YOLO！のいくつかのアップデートを紹介します。より良いものにするために、たくさんの小さなデザイン変更を行いました。また、この新しいネットワークはとても素晴らしいものです。前回より少し大きくなりましたが、より正確になっています。それでもまだ速いので、ご心配なく。\n","\n","開発から退いた理由も、論文の最後に記載されている\n","\n","- \"What are we going to do with these detectors now that we have them?\"\n","- 訳：このような物体検出器を使って何をするんだい？\n","\n","- \"A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and deﬁnitely won't be used to harvest your personal information and sell it to…. wait, you're saying that's exactly what it will be used for?? Oh.\"\n","- 訳(DeepL)：この研究をしている人たちの多くは、GoogleやFacebookにいます。少なくとも私たちは、この技術が良い人たちの手に渡り、あなたの個人情報を採取して売るために使われることはないだろうと思っています。ああ。\n","\n","- \"Well the other people heavily funding vision research are the military and they've never done anything horrible like killing lots of people with new technology oh wait…..\"\n","- 訳(DeepL)：ビジョン研究に多額の資金を提供しているのは軍で、彼らは新しい技術で多くの人を殺すような恐ろしいことはしていません。おっとまてよ...\n","\n","- \"The author is funded by the Ofﬁce of Naval Research and Google.\"\n","- 訳(DeepL)：著者は、Ofﬁce of Naval ResearchとGoogleから資金提供を受けている。\n","\n","これらの記述から、研究成果の悪用が許せなかったと考えられます\n","- YoLoを生み出したことで、その責任が問われるような事態を想定したのかもしれません\n","\n","\n","最後に、次のようなtwitterを投稿している\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/jrtwitter.png\" width=500>\n"],"metadata":{"id":"rUvcL8OH7q54"}},{"cell_type":"markdown","source":["# 課題\n","データオーグメンテーションを実装しなさい\n","- どのような形でも構わないが、ここではBounding Boxも変換することを忘れずに"],"metadata":{"id":"YbUTajSeEzwc"}}]}