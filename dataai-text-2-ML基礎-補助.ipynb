{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["GV_uFRm_UpJK","gnckdnVR0na2","6P6MgxPeUvig","SoJyp4R6Pu_i","P60_7Ddayf_Z","6-03EaDSynJr","-aKQricohxW3","A_C6xqSlRoYO","tebnvOBhzYjX"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"RgY28E12xaQC"},"source":["# 最低限必要な数学的知識"]},{"cell_type":"markdown","metadata":{"id":"5forXuZ30S3x"},"source":["以下は極めて基本的な内容であり、既に知っていることとは思うが、Markdownフォーマットでの記述方法も含めて、ここでまとめておく\n","\n","既学習済であるため、厳密な定義などは述べない"]},{"cell_type":"markdown","metadata":{"id":"iwbqMGhyUhOg"},"source":["## 線形代数\n","線形代数は、複数の変数間の関係をシンプルに記述できるため、機械学習において頻出する"]},{"cell_type":"markdown","metadata":{"id":"vQivtCKj0uW3"},"source":["### スカラー，ベクトル，行列，テンソル"]},{"cell_type":"markdown","metadata":{"id":"GV_uFRm_UpJK"},"source":["#### スカラー\n","\n","スカラーは，1つの値もしくは変数のことで、例えば\n","$$\n","x, y, M, N\n","$$\n","\n","と表す\n","\n","#### ベクトル\n","\n","ベクトルは、複数のスカラーを縦もしくは横方向に集めて並べたもので、\n","\n","$$\n","\\boldsymbol{x}=\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2} \\\\\n","x_{3}\n","\\end{bmatrix}, \\\n","\\boldsymbol{y}=\\begin{bmatrix}\n","y_{1} \\\\\n","y_{2} \\\\\n","\\vdots \\\\\n","y_{N}\\end{bmatrix}\n","$$\n","\n","と表す\n","\n","ベクトルの表記は太文字とすることでスカラーかベクトルかを区別し、縦方向に並べたものを列ベクトル、横方向に並べたものを行ベクトルと呼ぶ\n","\n","一般に、単にベクトルと表現した場合には列ベクトルを指すことが多い\n","\n","#### 行列\n","\n","行列は複数の同じサイズのベクトルを並べたもので、\n","\n","$$\n","\\boldsymbol{X}=\\begin{bmatrix}\n","x_{11} & x_{12} \\\\\n","x_{21} & x_{22} \\\\\n","x_{31} & x_{32}\n","\\end{bmatrix}\n","$$\n","\n","のように表す\n","\n","多くの場合行列は大文字または大文字の太文字で表記する\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gnckdnVR0na2"},"source":["#### テンソル\n","\n","テンソルはベクトルや行列を一般化した概念で、ベクトルは1階のテンソル、行列は2階のテンソルと表現できる\n","\n","図のように行列を奥行き方向にさらに並べた場合3階のテンソルと呼ぶ\n","\n","- 例えば、カラー画像をデジタル表現する場合、画像を構成する各ピクセルはRGBの色空間を用いるのが一般的で、(行番号・列番号・色)の3軸で1つの値(この場合はピクセル)を指定する\n","\n","単にテンソルと表現されている場合は、3階以上のテンソルを指すことが多い\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/1-07.png\">\n","\n","次のような形で表記されることが多いが、決まったルールはない\n","\n","||小文字|大文字|\n","|-|-|-|\n","|細文字|スカラーの変数|スカラーの定数|\n","|太文字|ベクトル|行列 or テンソル |"]},{"cell_type":"markdown","metadata":{"id":"6P6MgxPeUvig"},"source":["#### 転置\n","\n","縦向きのベクトルを横向きのベクトル、横向きのベクトルを縦向きのベクトルに入れ替える演算を**転置 (Transpose）** とよび，$T$で表記する\n","\n","$$\n","\\begin{aligned}\\boldsymbol{x}&=\\begin{bmatrix}\n","1 \\\\\n","2 \\\\\n","3\n","\\end{bmatrix}, \\\n","\\boldsymbol{x}^{T}=\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\\\\n","\\boldsymbol{X}&=\\begin{bmatrix}\n","1 & 4 \\\\\n","2 & 5 \\\\\n","3 & 6\n","\\end{bmatrix}, \\\n","\\boldsymbol{X}^{T}=\\begin{bmatrix}\n","1 & 2 & 3 \\\\\n","4 & 5 & 6\n","\\end{bmatrix}\\end{aligned}\n","$$\n","\n","のようになる\n","\n","行列に対する転置では、サイズが$(N, M)$から$(M, N)$となり、$i$行$j$列目の値が$j$行$i$列目の値になる\n","\n","転置に関して次の公式がある\n","\n","$$\n","\\begin{aligned}\n","&(1) \\ \\left( \\boldsymbol{A}^{T}\\right)^{T}=\\boldsymbol{A}\\\\\n","&(2) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\right) ^{T}=\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\\\\n","&(3) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\boldsymbol{C}\\right) ^{T}=\\boldsymbol{C}^{T}\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\n","\\end{aligned}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"SoJyp4R6Pu_i"},"source":["### 線形結合と二次形式\n","\n","単位行列と逆行列については省略する\n","\n","なお、逆行列について機械学習では巨大な行列の逆行列を求める場合があるが、これらをプログラムすることはなく、基本的にはライブラリで計算する\n","\n","そもそも、ライブラリを用いることで計算速度を数十倍高めることができるためライブラリの利用が推奨される\n","\n","機械学習では、$\\boldsymbol{b}^{T}\\boldsymbol{x}$ と $\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}$ の2つの形式が頻出する\n","\n","- 前者は**線形結合**もしくは**一次結合**、後者は**二次形式**とよぶ\n","\n","スカラーの場合、一次式（$ax+b$）や二次式（$ax^2+bx+c$）があるが、これらをベクトルに拡張した形と理解することができる\n","\n","線形結合の計算の中身を見ると、\n","$$\n","\\begin{aligned}\n","\\boldsymbol{b}&=\\begin{bmatrix}\n","1 \\\\\n","2\\end{bmatrix},\\\n","\\boldsymbol{x}=\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2}\n","\\end{bmatrix}\\\\\n","\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n","1 & 2\n","\\end{bmatrix}\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2}\\end{bmatrix}=x_{1}+2x_{2}\\end{aligned}\n","$$\n","\n","のように $\\boldsymbol{x}$ の要素である $x_{1}$ もしくは $x_{2}$ に関して，一次式となっている\n","\n","二次形式も同様に計算の中身を確認すると、\n","\n","$$\n","\\begin{aligned}\n","\\boldsymbol{A}&=\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4\n","\\end{bmatrix},\\  \n","\\boldsymbol{x}=\n","\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2}\n","\\end{bmatrix}\\\\\n","\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}&=\n","\\begin{bmatrix}\n","x_{1} & x_{2}\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2}\n","\\end{bmatrix}\\\\\n","&=x^{2}_{1}+5x_{1}x_{2}+4x_{2}^{2}\n","\\end{aligned}\n","$$\n","\n","となり，各要素において二次式となっている\n","\n","従って，任意の二次関数を、$c$ をスカラーの定数項とすると、\n","$$\n","\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{b}^{T}\\boldsymbol{x} + c\n","$$\n","\n","の形で表現できる"]},{"cell_type":"markdown","metadata":{"id":"mR7Gb6-QXIWb"},"source":["### ベクトルによる微分と勾配\n","\n","ここで、ベクトルによる微分を考える\n","\n","関数のそれぞれのベクトルの成分毎に偏微分を計算し、それらを並べてベクトルにしたものを**勾配**と呼ぶ\n","\n","例えば、\n","\n","$$\n","\\begin{aligned}\n","\\boldsymbol{b}&=\\begin{bmatrix}\n","3 \\\\\n","4\n","\\end{bmatrix}, \\\n","\\boldsymbol{x}=\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2}\n","\\end{bmatrix}\\\\\n","\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n","3 & 4\n","\\end{bmatrix}\\begin{bmatrix}\n","x_{1} \\\\\n","x_{2}\n","\\end{bmatrix}\n","=3x_{1}+4x_{2}\\end{aligned}\n","$$\n","\n","この$\\boldsymbol{b}^{T}\\boldsymbol{x}$をベクトル$\\boldsymbol{x}$ で微分したものを、\n","\n","$$\n","\\dfrac {\\partial }{\\partial \\boldsymbol{x}}( \\boldsymbol{b}^{T}\\boldsymbol{x})\n","$$\n","\n","と表し、これを**ベクトルで微分する**と言う\n","\n","この例では、\n","\n","$$\n","\\begin{aligned}\n","\\dfrac {\\partial }{\\partial \\boldsymbol{x}}( \\boldsymbol{b}^{T}\\boldsymbol{x}) &=\\dfrac {\\partial }{\\partial \\boldsymbol{x}}( 3x_{1}+4x_{2}) \\\\\n","&=\\begin{bmatrix}\n","\\dfrac {\\partial }{\\partial x_{1}} ( 3x_{1}+4x_{2})  \\\\\n","\\dfrac {\\partial }{\\partial x_{2}} ( 3x_{1}+4x_{2})\n","\\end{bmatrix}\n","\\end{aligned}\n","$$\n","\n","のようになり，計算を進めると\n","\n","$$\n","\\begin{aligned}\\dfrac {\\partial }{\\partial x_{1}}( 3x_{1}+4x_{2}) &=\\dfrac {\\partial }{\\partial x_{1}}( 3x_{1}) +\\dfrac {\\partial }{\\partial x_{1}}( 4x_{2}) \\\\\n","&=3\\end{aligned}\n","$$\n","\n","$$\n","\\begin{aligned}\\dfrac {\\partial }{\\partial x_{2}}( 3x_{1}+4x_{2})&=\\dfrac {\\partial }{\\partial x_{2}}( 3x_{1}) +\\dfrac {\\partial }{\\partial x_{2}}( 4x_{2}) \\\\\n","&= 4\n","\\end{aligned}\n","$$\n","\n","となり、結局\n","\n","$$\n","\\begin{aligned}\n","\\dfrac {\\partial }{\\partial \\boldsymbol{x}}( \\boldsymbol{b}^{T}\\boldsymbol{x})\n","=\n","\\begin{bmatrix}\n","3  \\\\\n","4\n","\\end{bmatrix}\n","= \\boldsymbol{b}\n","\\end{aligned}\n","$$\n","となる\n","\n","通常の微分同様に$\\boldsymbol{x}$の係数として見える$\\boldsymbol{b}^{T}$が転置されて得られる\n","\n","以下、まとめると次のようになる。$(3)$は導出していないが、各自で確認のこと。\n","\n","$$\n","\\begin{aligned}\n","( 1) &\\ \\dfrac {\\partial}{\\partial \\boldsymbol{x}}( \\boldsymbol{c} ) = \\boldsymbol{0}\\\\\n","( 2) &\\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}( \\boldsymbol{b}^{T}\\boldsymbol{x}) = \\boldsymbol{b}\\\\\n","( 3) &\\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}( \\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}) =( \\boldsymbol{A}+\\boldsymbol{A}^{T}) \\boldsymbol{x}\\end{aligned}\n","$$\n","\n","その他、複雑なものも含めて、\n","[MatrixCookBook(リンク)](http://class.west.sd.keio.ac.jp/dataai/text/matrixcookbook.pdf)\n","を参考にするとよい\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8x0cqm-4AAMj"},"source":["## ラグランジュの未定乗数法\n","$g(x,y)=0$の条件下で$f(x,y)$を最大化・最小化したいといった等式制約付き最大最小問題について、\n","\n","$$L(\\boldsymbol{x},\\lambda)=f(\\boldsymbol{x})-\\lambda g(\\boldsymbol{x})$$とすると、\n","$\\boldsymbol{x}=\\boldsymbol{\\alpha}$が極致を与える、つまり、\n","$\\boldsymbol{\\alpha}$が$\\dfrac{\\partial L}{\\partial x_1}=\\dfrac{\\partial L}{\\partial x_2}=\\cdots=\\dfrac{\\partial L}{\\partial x_n}=\\dfrac{\\partial L}{\\partial\\lambda}=0$の解、\\\n","もしくは、\n","$\\dfrac{\\partial g}{\\partial x_1}=\\dfrac{\\partial g}{\\partial x_2}=\\cdots=\\dfrac{\\partial g}{\\partial x_n}=0$の解となる\n","\n","この方法により変数が1つ増えるが、式も1つ増え、$n$変数$n$連立方程式となり解くことができる\n","- 証明は他の授業で扱っているため、ここでは省略するが、上記の勾配を利用する\n","\n","簡単な例題を示す\n","\n","**例題**\n","\n","$x^2+y^2=1$のもとで$f(x,y)=2x+3y$を最大化せよ\n","\n","**解**\n","\n","$$L(x,y,\\lambda)=2x+3y-\\lambda(x^2+y^2-1)$$\n","$$\\tfrac{\\partial L}{\\partial x}=2-2x\\lambda=0$$\n","$$\\tfrac{\\partial L}{\\partial y}=3-2y\\lambda=0$$\n","$$\\tfrac{\\partial L}{\\partial \\lambda}=-x^2-y^2+1=0$$\n","\n","$$\\therefore (x,y)=\\left( \\pm\\dfrac{2}{\\sqrt{13}},\\pm\\dfrac{3}{\\sqrt{13}} \\right) (複合同順)$$が解の候補である\n","\n","$2x+3y$に代入して、$+$側が最大値$\\sqrt 13$を与え、$-$側が最小値$-\\sqrt 13$を与える\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hsLXvQtVhwFC"},"source":["## ヤコビアン"]},{"cell_type":"markdown","metadata":{"id":"CPOotnwJelbs"},"source":["\n","\n","多入力多出力関数の微分であるヤコビ行列（ヤコビアン）もニューラルネットワークの誤差逆伝播法を理解する際に必要となる\n","- 殆どの場合、行列を掛けた場合のヤコビアンは、その転置行列であると考えれば十分である場合が多い"]},{"cell_type":"markdown","metadata":{"id":"P60_7Ddayf_Z"},"source":["### 状況設定\n","\n","$\\boldsymbol{x} = (x_1, x_2, \\cdots , x_n)^T$を決めると\n","$\\boldsymbol{y} = (y_1, y_2, \\cdots , y_m)^T$が定まる\n","\n","また、$n$変数の$m$次元ベクトル値関数があり、関数が連続であれば、各$y_i$は$x_j$で偏微分可能である\n","\n","なお、機械学習において連続でない関数を扱うことはない\n","- 実際には**一部で微分不可能であっても実用上問題がない**"]},{"cell_type":"markdown","metadata":{"id":"6-03EaDSynJr"},"source":["### 定義\n","\n","#### ヤコビ行列の定義\n","\n","$$\n","\\frac{\\partial y_1}{\\partial x_1}\n","$$\n","をij成分とする$m\\times n$ 行列$\\mathbf{J}$ をヤコビ行列という。\n","\n","例：$i=j=2$のヤコビ行列\n","\n","$$\n","J_{i=m=2, j=n=2} = \\begin{pmatrix}\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n","\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2}\n","\\end{pmatrix}\n","$$\n","\n","#### ヤコビアンの定義\n","\n","ヤコビ行列$J$の行列式$|J|$のことを「ヤコビ行列式」と呼ぶ\n","- ヤコビアンとは「ヤコビの」という意味であり、ヤコビ行列のことも、ヤコビ行列式のこともヤコビアンと呼ぶことがあるが、一般的にはヤコビ行列式を指す\n","\n","ヤコビアンを$|J|$と書く代わりに次のように表記することもある\n","\n","$$\n","|J| ≡ \\left|\\frac{\\partial (x,y,z)}{\\partial (r,\\theta,\\phi)}\\right|\n","$$\n","\n","これはヤコビアンが偏微分の拡張だというイメージを強調するような記法である\n","\n","ヤコビアンは変換の「拡大率」を表すといえ、重積分での変数変換でも使用する\n","\n","$$\n","\\iint_D f(x, y) dxdy\n","= \\iint_D f(\\varphi_1(u, v), \\varphi_2(u, v))\\left|\\frac{(\\partial \\varphi_1(u, v), \\varphi_2(u, v))}{(\\partial u, \\partial v)}\\right|du dv\n","$$"]},{"cell_type":"markdown","metadata":{"id":"-aKQricohxW3"},"source":["### ヤコビ行列の意味\n","\n","- 1変数の場合：微分係数は接線の傾きで関数の1次近似を表せる\n","\n","$x_0$周りでの1次近似$$\n","y(x) \\simeq y(x_0) + y'(x_0)(x-x_0)\\\\\n","$$\n","\n","- 多変数の場合：ヤコビ行列は接線の傾きに相当するため、同様に次のように表せる\n","\n","$$y'(x_0) \\to \\mathbf{J}(\\boldsymbol{x_0})$$\n","\n","$$\n","\\boldsymbol{y}(\\boldsymbol{x}) \\simeq \\boldsymbol{y}(\\boldsymbol{x_0})\n","+ \\mathbf{J}(\\boldsymbol{x_0})(\\boldsymbol{x}-\\boldsymbol{x_0})\\\\\n","$$"]},{"cell_type":"markdown","metadata":{"id":"mZm_MbRpPVuv"},"source":["## 確率\n","\n","機械学習において、パラメータ推定では確率が利用される\n","\n","$x$を**確率変数(stochastic variable / random variable)**としたときの確率$p(x)$について、確率変数$x$が値$u$のときの確率を$p(x=u)$もしくは$p(u)$と表し、次の関係が成り立つ\n","\n","$$\n","\\begin{align}\n","\\sum_{x } p(x) &= 1 \\\\\n","p(x) & \\geq  0\n","\\end{align}\n","$$\n","\n","2つの事象が同時に起きる確率を**同時確率(joint propability)**とよび$p(x, y)$と表す\n","- なお、独立事象であればよい\n","- 例えば、サイコロを2回振り，1回目の目が$1$，2回目の目が$5$となる確率は同時確率となり$p(1,5)$と表現される\n","\n","同時確率のうち、ある確率変数のみに注目し、それ以外の確率変数について和を取って消去する操作を**周辺化(marginalization)**とよぶ\n","\n","周辺化の結果は、注目した確率変数の確率に一致する\n","\n","$$\n","p(x) = \\sum_y p(x, y) \\\\\n","p(y) = \\sum_x p(x, y)\n","$$\n","\n","片方の確率変数を固定した場合の確率分布を**条件付き確率(conditional probability)**とよび$p(y|x)$と表す\n","- 条件付き確率は、同時確率を条件の確率で割った値と一致する\n","\n","$$\n","p(y|x) = \\frac{p(x, y)}{p(x)}\n","$$\n","\n","ここで$p(y|x)p(x) = p(x, y)$に注意すると\n","\n","$$\n","p(x|y) = \\frac{p(x, y)}{p(y)} = \\frac{p(y | x)p(x)}{p(y)}\n","$$\n","\n","が得られ、これを**ベイズの定理**と呼ぶ。\n","- さらに$p(x)$を**事前確率**、$p(x|y)$が**事後確率**と呼ぶ\n","\n","$p(y)$は事象が発生する平均的確率を意味し、$p(y|x)$ある状況でそのデータが得られる確率を意味する\n","- よって、事前確率は「その状況が正しい確率」であり、事後確率は「データが手に入った後に、その状況が正しいといえる確率」を意味する\n","- さらに、式としては、事前確率を含む結果の確率である$p(x), p(y|x), p(y)$がわかれば、その結果となった原因の確率である事後確率$p(x|y)$がわかる\n","\n","より平易に例を挙げて説明する\n","\n","- ある街で、火事が起こる確率を$P(Fire)=0.01$、煙が上がっているのを見る確率を$P(Smoke)=0.1$とする\n","- 火事の90%で煙が上る、つまり$P(Smoke|Fire)=0.9$とする\n","- 煙が見えたときにそれが火事である事後確率は、ベイズの定理より\n","$P(Fire|Smoke)=\\frac{P(Fire)P(Smoke|Fire)}{P(Smoke)}$として求まり、$\\frac{0.01\\times 0.9}{0.1}=0.09$ となる\n","\n","ベイズ理論は初期のスパム判定においてよく利用された\n","\n","メールが単語$i$を含むか否かを表す確率変数を $x_{i}$、\n","メールがスパムであるか否かを表す確率変数を $y$ とおくと， $p(x_{i})$\n","は「メールが単語$i$を含む確率」、$p(y)$\n","は「メールがスパムである確率」、$p(x_{i}|y)$\n","は「メールがスパムであった場合に，その中に単語$i$が含まれる確率」となる\n","\n","受信済みの大量のメールからそれぞれの割合を集計して求め、ベイズの定理を適用することで、 $p(y|x_{i})$ として，「メールに単語$i$が出現した場合に，そのメールがスパムである確率」を求めることができる"]},{"cell_type":"markdown","metadata":{"id":"Wc_YhfGQrbll"},"source":["## 尤度と最尤推定\n","\n","パラメータ$\\theta$で特徴付けられたパラメトリックな確率モデル$p(x; \\theta)$について、事象$u$が観測される確率$p(x=u; \\theta)$を事象$u$の **尤度** と呼ぶ\n","- 尤度（ゆうど）の尤は「尤（もっと）もらしい」という意味であり、その事象の起きやすさを表す\n","\n","ここで，$N$個のデータについて$X = ( x^{(1)}, x^{(2)}, \\ldots, x^{(N)})$が与えられ、そのデータ$X$を生成するような確率分布を推定する問題を考える\n","\n","この場合、**最尤（さいゆう）推定**とよばれる手法がよく利用される\n","- 最尤推定は観測データ$X$を最も生成しそうなパラメータ$\\theta$を推定する手法のこと\n","\n","観測するデータがそれぞれ独立に生成されている場合、その尤度は\n","\n","$$\n","L(\\theta) = p(X; \\theta) = \\prod_{i=1}^N p(x^{(i)}; \\theta)\n","$$\n","\n","複数データに対する尤度は、$1$より小さな値の積となるため非常に小さな数になりコンピュータ上で扱うことが困難となる\n","\n","また尤度を最大化したい場合、積の形の式の最大化は比較的困難なため、尤度の代わりにその対数をとった対数尤度とし、和の形式として扱う\n","\n","$$\n","\\log L(\\theta) = \\log p(X; \\theta) = \\sum_{i=1}^N \\log p(x^{(i)}; \\theta)\n","$$\n","\n","この対数尤度を最大化するパラメータ$\\theta$を求めることができれば，その値がデータ$X$を最も生成しそうな確率モデルのパラメータとなる\n","\n","簡単な例としてコインの表・裏が出る確率を推定する問題を考える\n","\n","- コインの表・裏を表す確率変数を$x$とおき、$x = 1$であれば表、$x = 0$であれば裏とする\n","\n","- また、表($x = 1$)となる確率を表すパラメータを$\\theta$とおく。コインを$10$回投げた結果、以下の観測結果$X$が得られたとする。\n","\n","$$\n","X = (1, 0, 1, 1, 1, 0, 0, 1, 0, 0)\n","$$\n","\n","この場合の尤度は，\n","\n","$$\n","\\begin{aligned}\n","L(\\theta) &= \\theta \\cdot (1 - \\theta) \\cdot  \\ldots  \\cdot (1 - \\theta) \\cdot (1 - \\theta) \\\\\n","&= \\theta^{5} \\cdot (1 - \\theta)^{5}\n","\\end{aligned}\n","$$\n","\n","となり、対数尤度は、\n","\n","$$\n","\\log L(\\theta) = 5 \\log \\theta + 5 \\log \\left( 1 - \\theta \\right)\n","$$\n","\n","となる\n","\n","これを$\\theta$で微分して$0$になる条件を求めると\n","\n","$$\n","\\frac{5}{\\theta} - \\frac{5}{\\left( 1 - \\theta \\right)} = 0\n","$$\n","\n","ともとまる\n","\n","従って、$\\theta = 0.5$が最尤推定により得られるが、これは当然であり、表裏それぞれ50\\%正確に出現する場合、Xの出現を最大化できることを意味する\n","\n","回帰モデルの目的関数として真値と予測値の二乗誤差の和を使う場合を特に**最小二乗法(Least-squares method)**と呼ぶ\\\n","この場合、モデルの出力値に正規分布の誤差を仮定した最尤推定を行っているのと等価であることが知られている"]},{"cell_type":"markdown","metadata":{"id":"A9zvcxaVzQuo"},"source":["### 事後確率最大化推定(MAP推定)\n","\n","最尤推定は多くの場合有効だが、求めるパラメータに何らかの事前情報がある場合、その事前情報を扱うことができないため、試行回数が少ない中でパラメータを推定しようとすると、最尤推定ではうまくいかない場合がある\n","\n","- 例えば、コインの表・裏が出る確率を推定する例においても、コインを$5$回投げ、たまたま$5$回とも表($x = 1$)が出たとすると、最尤推定では、表が出る確率が$100$%（裏が出る確率が$0$%）であると推定する。しかしながら、裏が出る確率は$0$よりも大きいという事前情報があれば、異なる推定ができるであろう。\n","\n","このように、事前情報も考慮しつつ観測データに基づいてパラメータを推定する方法として **事後確率最大化(Maximum A Posteriori, MAP)推定**が知られている\n","\n","MAP推定では、パラメータ$\\theta$も確率変数であり、**事前確率**である$\\theta$の分布$p\\left( \\theta \\right)$を考慮に入れて、観測データ$X$が与えられた条件での**事後確率**であるパラメータ$\\theta$の条件付き確率$p\\left( \\theta|X\\right)$ を最大化する$\\theta$を求める\n","\n","ベイズの定理により事後確率は、\n","\n","$$\n","p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{p(X)}\n","$$\n","\n","である。これをパラメータについて最大化すると、$P(X)$はパラメータとは無関係のため、\n","\n","$$\n","p(X|\\theta) p(\\theta)\n","$$\n","\n","を最大化するパラメータを求めればよい\n","\n","$p(X|\\theta)$は最尤推定と同じであるが、MAP推定ではさらにパラメータの事前確率$p(\\theta)$を掛けた確率を最大化する\n","\n","機械学習においてパラメータを最適化する際、パラメータの値が大きい場合にペナルティを与える正則化を採用することがあるが、これはパラメータの事前確率とみなすことができ、パラメータをMAP推定していると解釈できる\n","\n","但し、$p(\\theta)$を正しく得ることは難しく、あくまでも「人間が与えるこうであろう・こうであってほしい」という値を与えることとなる"]},{"cell_type":"markdown","metadata":{"id":"_UfhUQBvBlU4"},"source":["ここで、具体的な例を使いながら理解を深める\n","\n","- 例えば、勝負に10回挑んで、8回勝ったとすると、勝率8割という形で強さを表現できるが、確率的な話であるとすると実際と得られた勝率との乖離が気になることがある\n","  - 本当の実力は勝率1割であるが、たまたま10戦中8勝した\n","  - 本当の実力は勝率9割であるが、たまたま10戦中8勝しかできなかった\n","\n","- さて、何が正しいのか？そもそも、こういうことは気にしないという言い方もあるが、例えば、藤井聡太二冠はデビュー以来29連勝を達成したが、同様に考えると、\n","\n","  - 29連勝する確率は藤井聡太二冠の本来の勝率を0.95としても、22.6%程度の確率\n","  - 将棋対局の仕組みを考えず、単純にデータだけを見るとデビューの2016年と翌年2017年の合計では、55勝10敗と勝率は84.6%程度である\n","\n","- このように、実力もずば抜けていたことは事実であるが、相当な運もあったことが伺える\n","\n","さて、最尤推定を考えると、\n","\n","$$\n","L(\\theta) = p(X; \\theta) = \\prod_{i=1}^N p(x^{(i)}; \\theta)\n","$$\n","\n","であるが、$\\theta$を本来の実力としての勝率とするならば、29連勝は、$p(29連勝; \\theta)$となり、この確率は\n","$\\theta^{29}$\n","となる\\\n","この$p$を最大化する$\\theta$は\n","$1$\n","となり、あり得ない値になっていることがわかる\n","\n","さて、$n$回戦って$v$回勝利するとすると、\n","$$p(v|n, \\theta)={}_n\\mathrm{C}_v\\theta^v(1-\\theta)^{n-v}$$\n","となる\n","\n","これを最大化する$\\theta$を求めるため微分すると、\n","\n","$$\\frac{dp}{d\\theta}={}_n\\mathrm{C}_v(v\\theta^{n-1}(1-\\theta)^{n-v}-(n-v)\\theta^v(1-\\theta)^{n-v-1})={}_n\\mathrm{C}_vv\\theta^{n-1}(1-\\theta)^{n-v-1}(v(1-\\theta)-(n-v)\\theta)$$\n","\n","となるが、これを$0$にするには、結局$\\theta=\\frac{v}{n}$となり、勝率を求めることに他ならない\n","\n","データ数が多ければ、当然の結果といえるが、データの数が少ないとき、例えば4戦だけして4勝した場合、たまたまである可能性も高く、だからといって勝率$1$とは言いにくいであろう\n","\n","ここで、MAP推定を用いる\n","\n","次の試合結果をDとすると、ベイズの定理により、\n","\n","$$p(\\theta|D)=\\frac{p(D|\\theta)p(\\theta)}{p(D)}$$\n","\n","と表現できる\n","\n","$p(\\theta)$は次の試合結果がわかる前の確率分布であり事前分布、$p(\\theta|D)$は次の試合結果が分かった後の確率分布で事後分布である\n","- この事後分布が最大になる$\\theta$を選ぶのが**MAP推定**である\n","\n","このMAP推定を数式で表現すると、\n","$$\\mathrm{argmax}_\\theta p(\\theta|D) = \\mathrm{argmax}_\\theta \\frac{p(D|\\theta)p(\\theta)}{p(D)}$$\n","となり、$\\mathrm{argmax} f(x)$は$f(x)$を最大にする$x$の集合を意味する\n","\n","分母の$p(D)$は$\\theta$に無関係であるため、結局MAP推定は、\n","$$\\mathrm{argmax}_\\theta p(\\theta|D) = \\mathrm{argmax}_\\theta p(D|\\theta)p(\\theta)$$\n","となる\n","\n","事前分布$p(\\theta)$は繰り返しになるが、人間が最初に設定する必要がある\n","- しかしながら、なるべく計算を楽にするために、事前分布と事後分布が同じ形になるように設定するのが望ましく、そのように定めた事前分布を**共役事前分布**と呼ぶ\n","\n","勝敗を議論する場合には、β分布が用いられる\n","- β分布は$\\beta$をハイパーパラメータとして、\n","$$\n","p(\\theta) = \\mathrm{Beta}(\\theta|\\beta)=\\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}\\theta^{\\beta-1}(1-\\theta)^{\\beta-1}\n","$$\n"," となる\n","\n","なお、積分値を1にするため、作為的に$\\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}$を掛けている\n","\n","実際にMAP推定を求めると、\n","\n","$$\\mathrm{argmax}_\\theta p(\\theta|D)=\\mathrm{argmax}_\\theta {}_n\\mathrm{C}_v\\theta^v(1-\\theta)^{n-v}\\cdot \\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}\\theta^{\\beta-1}(1-\\theta)^{\\beta-1} $$\n","\n","$\\theta$に関係するところのみ抽出すると、\n","\n","$$\\mathrm{argmax}_\\theta p(\\theta|D)=\\mathrm{argmax}_\\theta \\theta^{n+\\beta-1}(1-\\theta)^{n-v+\\beta-1}$$\n","\n","となる\n","\n","微分し極を求めると、\n","\n","$$\\theta=\\frac{v+\\beta-1}{n+2(\\beta-1)}$$\n","\n","となる\n","\n","これで、偶然性も考慮して、本来の勝率を推定する\n","\n","例えば、$\\beta = 2$とした場合はラプラススムージングと呼ばれ、29連勝は、\n","$$\\frac{29+2-1}{29+2(2-1)}=0.9677$$となる\n","\n","$\\beta = 10$とした場合は同様に$0.8088$となり、勝率推定の値が小さくなる\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AwndYk_Kp7th"},"source":["### ベイズ推定\n","このMAP推定には、当然ながら不満も出てくる\n","\n","298戦199勝であっても、4戦3勝であっても、\n","$$\\theta=2/3$$と計算できるが、298戦199勝の方が、確度が高くより評価されるべきと考えるであろう\n","\n","まさにその通りであり、これを考慮したのがベイズ推定といえる\n","\n","$\\mathrm{argmax}$を求めず、$p(\\theta|D)$をそのまま利用する\n","\n","ベイズの定理から、\n","\n","$$p(\\theta|D)=\\frac{p(D|\\theta)p(\\theta)}{p(D)}$$\n","\n","であるが、$p(D)$を無視せずに求めると、\n","\n","$$\n","\\begin{aligned}\n","p(D)&=\\int^1_0p(D,\\theta)d\\theta\\\\\n","&=\\int^1_0p(D|\\theta)p(\\theta)d\\theta\\\\\n","&=\\int^1_0{}_n\\mathrm{C}_v\\theta^v(1-\\theta)^{n-v} \\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}\\theta^{\\beta-1}(1-\\theta)^{\\beta-1}d\\theta\\\\\n","&={}_n\\mathrm{C}_v\\theta^v(1-\\theta)^{n-v} \\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}\\int^1_0\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}d\\theta\n","\\end{aligned}$$\n","\n","となる\n","\n","改めて、$p(\\theta|D)$を求めると、\n","\n","$$\n","\\begin{align*}\n","P(\\theta\\ |\\ D) &= \\frac{P(D\\ |\\ \\theta)P(\\theta)}{P(D)}\\\\\n","&= \\frac{{}_n\\mathrm{C}_v\\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}}{{}_n\\mathrm{C}_v\\frac{\\Gamma(2\\beta)}{\\Gamma(\\beta)^2}\\int^1_0\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}d\\theta}\\\\\n","&= \\frac{\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}}{\\int^1_0\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}d\\theta}\n","\\end{align*}\n","$$\n","\n","となり、\n","\n","$$\n","\\int_0^1\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}d\\theta\n","= \\frac{\\Gamma(v+\\beta)\\Gamma(n-v+\\beta)}{\\Gamma(n+2\\beta)}\n","$$\n","\n","であることから、\n","\n","$$\n","P(\\theta\\ |\\ D) = \\frac{\\Gamma(n+2\\beta)}{\\Gamma(v+\\beta)\\Gamma(n-v+\\beta)}\\theta^{v+\\beta-1}(1-\\theta)^{n-v+\\beta-1}\n","$$\n","\n","となる\n","\n","ここで、$v$連勝した場合を順に求めると次のグラフのようになり、連勝を重ねるほど、$\\theta$の確率分布が右に移動することが見て取れる\n","\n","![](http://class.west.sd.keio.ac.jp/dataai/text/bayes.gif)\n"]},{"cell_type":"markdown","metadata":{"id":"A_C6xqSlRoYO"},"source":["### 統計量\n","\n","**分散**について、標本分散と不偏分散について述べる。分散は、\n","\n","$$\n","\\begin{aligned}\\sigma ^{2}=\\dfrac {1}{N}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\\end{aligned}\n","$$\n","\n","と与えられ、各サンプルの平均 $\\bar{x}$ からの差分 $x- \\bar{x}$ を計算し、それらの二乗誤差の平均の値を計算すればよい\n","- これを**標本分散**と呼ぶ\n","\n","分散にはもう一つ定義があり、\n","\n","$$\n","\\begin{aligned}\n","\\sigma ^{2}=\\dfrac {1}{N-1}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\n","\\end{aligned}\n","$$\n","- これを**不偏分散**と呼ぶ\n","\n","データ解析において、用いる全データ、つまり**母集団**に対する解析を行うのか、その一部の標本データ、つまり**標本集団**に対する解析を行うのかを意識する必要がある\n","- 母集団は、解析対象の想定範囲におけるすべてのデータが揃っている場合に用い、標本集団はそのうちの一部を抽出する場合に用いる\n","\n","一般に、母集団のデータを集めるのが大変である場合は、標本集団から母集団の分布を推定するため、標本集団で利用する不偏分散を利用する\n","- サンプル数$N$が多い場合には母分散と不偏分散の差が小さく問題とならないが、サンプル数が小さい場合は差が顕著となるため注意が必要である\n","- 分散によりデータのばらつきを定量的に評価でき、データのばらつき具合にもよるが、スケールの違いも評価できる\n","\n","**標準偏差**もしばしば利用される\n","\n","基本的であるが、$X=(-2, -1, 0, 1, 2)$についての、平均、分散、標準偏差は、\n","$$\n","\\begin{aligned}\n","\\bar{x}&=\\dfrac {1}{5}\\left( -2-1+0+1+2\\right) =0\\\\\n","\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -2-0\\right) ^{2}+\\left( -1-0\\right) ^{2}+(0-0)^{2}+(1-0)^{2}+(2-0)^{2}\\right\\} \\\\\n","&=\\dfrac {1}{5}\\times 10=2\\\\\n","\\sigma &=\\sqrt {2}\n","\\end{aligned}\n","$$\n","\n","となる"]},{"cell_type":"markdown","metadata":{"id":"tebnvOBhzYjX"},"source":["### 正規分布と正規化\n","\n","**正規分布**は**ガウス分布**ともよばれ、機械学習ではしばしば用いられる\n","\n","平均$\\mu$、標準偏差$\\sigma$を持つ正規分布は以下のような形状をしている\n","\n","![](http://class.west.sd.keio.ac.jp/dataai/text/1-14.png)\n","\n","正規分布は、\n","\n","- 独立で多数の因子の和で表される確率変数は正規分布に近似的に従うことが知られている\n","- 多くの統計的データが正規分布に従う\n","- 数式が比較的扱いやすい\n","\n","一方で、必ずしもデータが正規分布に従うとは限らない\n","\n","必ずデータの分布を図示化し、正規分布として扱ってよいか常に考えることが必要である\n","\n","正規分布では平均 $\\mu$ と標準偏差 $\\sigma$ に対して、何%がその分布に入るかという議論が行われる\n","\n","例えば、$\\mu \\pm 3\\sigma$ の範囲内にデータの全体の99.7%が入るため、この $\\mu \\pm 3 \\sigma$ に入らない領域を外れ値（他の値から大きく外れた値）として定義することが多い\n","\n","なお、正規分布には加法性があり、平均$\\mu$、分散$\\sigma^2$の正規分布を$N(\\mu, \\sigma^2)$と表現すると、$N(\\mu_1, \\sigma_1^2)$と$N(\\mu_2, \\sigma_2^2)$の2つの正規分布について、それぞれの母集団からランダムに要素を取り出してその和を求めた時の分布は、やはり正規分布になる\n","- このようにして新たに作った正規分布は、$N(\\mu_1+\\mu_2, \\sigma_1^2+\\sigma_2^2)$となる\n"]},{"cell_type":"markdown","metadata":{"id":"AKW5zhowYbPx"},"source":["### 標準偏差を利用したスケーリング\n","\n","機械学習アルゴリズムにおける前処理として頻繁にスケーリングが行われる。例えば、スケールが異なる変数 $x_{1}, x_{2}$、$x_1=(100,0.1), x_2=(1000,1)$とする\n","- ここで、縦軸と横軸のスケールが大きく異なっていることに注意する\n","\n","この２点間の距離 $d$ は、\n","\n","$$\n","\\begin{aligned}\n","d&=\\sqrt {\\left( 100-1000\\right) ^{2}+\\left( 0.1-1\\right) ^{2}}\\\\\n","&= \\sqrt {810000.81}\n","\\end{aligned}\n","$$\n","\n","距離$d$において$x_{1}$の影響量が大きく$x_{2}$ は殆ど影響を与えていない\n","\n","すると、$x_{2}$ のデータとしての意味が薄れ、考慮することが困難となる\\\n","そこで、**スケーリング**を行う\n","\n","- **最小値0**，**最大値1**にスケーリング\n","\n"," 最小値 $x_{\\min}$ と最大値 $x_{\\max}$ を求め、全データについて、\n","$$\n","\\widetilde{x} = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n","$$\n","を求めればスケーリングが行われる\n","  - 計算が単純であるが、外れ値の影響が大きくなる\n","\n","- **平均0**，**標準偏差1** にスケーリング\n","\n"," この方法は、**標準化（正規化）** とよばれ、全てのデータから平均を引くと平均$0$になり，標準偏差で割ると標準偏差は$1$となる\n","$$\n","\\widetilde{x}  = \\dfrac{x - \\bar{x}}{\\sigma}\n","$$\n","としてデータが変換される\n","  - 外れ値に強いスケーリングが行われる"]}]}