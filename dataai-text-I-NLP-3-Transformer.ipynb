{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1F4ifdHNnrAUsDv2pj0EXDImHsM_x_E9M","timestamp":1628197853609}],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b71bdf5f159c4d42b6c418a04cd3e862":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e5c3ae58281a47d6bc4cc8fda8f2e7c2","IPY_MODEL_d07f7fbdb15543dcae170df6aa36a393","IPY_MODEL_be3ed39c14af4765b8a41fe004494a5f"],"layout":"IPY_MODEL_2837e0768e8943d8898ef8cd2c655a2b"}},"e5c3ae58281a47d6bc4cc8fda8f2e7c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed9409ebb09b4c3095728286338c886e","placeholder":"​","style":"IPY_MODEL_ef93f5fe589d4090b473d40b4632fcc3","value":"Downloading (…)okenizer_config.json: 100%"}},"d07f7fbdb15543dcae170df6aa36a393":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e292015b82eb4185807927c90ebe7a0f","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eef0eb2d380f4fe2a5662d7cba605585","value":28}},"be3ed39c14af4765b8a41fe004494a5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eb68b03eaf94d92bcda98fae3548151","placeholder":"​","style":"IPY_MODEL_bd7aa322253649f9884acabb5a9a6847","value":" 28.0/28.0 [00:00&lt;00:00, 1.44kB/s]"}},"2837e0768e8943d8898ef8cd2c655a2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9409ebb09b4c3095728286338c886e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef93f5fe589d4090b473d40b4632fcc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e292015b82eb4185807927c90ebe7a0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eef0eb2d380f4fe2a5662d7cba605585":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0eb68b03eaf94d92bcda98fae3548151":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7aa322253649f9884acabb5a9a6847":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4e347d25f974571bea02fb5362b0409":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6890593b1de4ac899e17eb97d3a836a","IPY_MODEL_e27a3b073dc94b98977d12e73c9eccd7","IPY_MODEL_09cca75ce77c4343ae0315254a307ed9"],"layout":"IPY_MODEL_a9f2c0b86dcf46d4b62b0556beeeb6f9"}},"c6890593b1de4ac899e17eb97d3a836a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78548bde7c0949588eac6a9e61c9396b","placeholder":"​","style":"IPY_MODEL_f9a3b7636e114730ae0d6b6f1d9a70b8","value":"Downloading (…)lve/main/config.json: 100%"}},"e27a3b073dc94b98977d12e73c9eccd7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06b60d9cd55d4e2dbe041b503ede0b36","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f5f4f45e17245c89c977dfa5f6b9750","value":570}},"09cca75ce77c4343ae0315254a307ed9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9746cfffd154e37bf29aeb46c8e6226","placeholder":"​","style":"IPY_MODEL_66fb652463c54ee3bd19feb4e561a0e6","value":" 570/570 [00:00&lt;00:00, 24.7kB/s]"}},"a9f2c0b86dcf46d4b62b0556beeeb6f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78548bde7c0949588eac6a9e61c9396b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9a3b7636e114730ae0d6b6f1d9a70b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06b60d9cd55d4e2dbe041b503ede0b36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f5f4f45e17245c89c977dfa5f6b9750":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9746cfffd154e37bf29aeb46c8e6226":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66fb652463c54ee3bd19feb4e561a0e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"315f17b8acfe445181351f9f346f7824":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02c779ba4446447a8fc21b889ee9f726","IPY_MODEL_4c412e5988574898804da52e82a31702","IPY_MODEL_533559a9002749feb107944482272627"],"layout":"IPY_MODEL_31e1272dd3c3440c8a857f527030fe86"}},"02c779ba4446447a8fc21b889ee9f726":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8adac3798c6440ea83bac7699a0e66f","placeholder":"​","style":"IPY_MODEL_b0a07c739ea942ef83a597aaa6dd044c","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"4c412e5988574898804da52e82a31702":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50c49698f24443aa956ff8bf13a4d04d","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f34c698b0c544115aaa544fa57fcad79","value":231508}},"533559a9002749feb107944482272627":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e068202d4f21430694c061e8b4f18c19","placeholder":"​","style":"IPY_MODEL_e0ab63bd2ab74cd1aa934bef62615551","value":" 232k/232k [00:00&lt;00:00, 1.44MB/s]"}},"31e1272dd3c3440c8a857f527030fe86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8adac3798c6440ea83bac7699a0e66f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0a07c739ea942ef83a597aaa6dd044c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50c49698f24443aa956ff8bf13a4d04d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f34c698b0c544115aaa544fa57fcad79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e068202d4f21430694c061e8b4f18c19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ab63bd2ab74cd1aa934bef62615551":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6b6f339eeab4ec98514b16a4340a039":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88b7085b4dcd487c92d1ea2718c9a5de","IPY_MODEL_e0cdfb00d7a64d5092ee12320b93fbe6","IPY_MODEL_30fb06bb4fb24c4eacbbecae604d2779"],"layout":"IPY_MODEL_560ceaf4d1ee482386c722b55e17c1ae"}},"88b7085b4dcd487c92d1ea2718c9a5de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dae048a55a1483888c49fa18a7540b0","placeholder":"​","style":"IPY_MODEL_38e9c48375f44fe590d2f1e026ea57ab","value":"Downloading (…)/main/tokenizer.json: 100%"}},"e0cdfb00d7a64d5092ee12320b93fbe6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14348811134d4b1899e6e684952905a3","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdbc7e329a9746b4bbf9a11b1652c4b4","value":466062}},"30fb06bb4fb24c4eacbbecae604d2779":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebcec7e18b054f87893d57b749815fb0","placeholder":"​","style":"IPY_MODEL_aaa81565dcac4f11a81418852a2751f0","value":" 466k/466k [00:00&lt;00:00, 1.92MB/s]"}},"560ceaf4d1ee482386c722b55e17c1ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dae048a55a1483888c49fa18a7540b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38e9c48375f44fe590d2f1e026ea57ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14348811134d4b1899e6e684952905a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdbc7e329a9746b4bbf9a11b1652c4b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebcec7e18b054f87893d57b749815fb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa81565dcac4f11a81418852a2751f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c68dfcfe51442e3a191fec84645be8b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_172828cc83384387938c608cc1427f28","IPY_MODEL_3b818c115d9b457e8c8f2df532e2d149","IPY_MODEL_160e06bf0ee84a18926a60fdee13b4cd"],"layout":"IPY_MODEL_f7acacebafd948c1b1455900ead30d18"}},"172828cc83384387938c608cc1427f28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58509191945c46789e87489706468c9e","placeholder":"​","style":"IPY_MODEL_fb8780679ad74c56a88bbb2ee6372520","value":"Downloading: 100%"}},"3b818c115d9b457e8c8f2df532e2d149":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5510ad50514c4b248aaa34159f5a2a45","max":4556,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1254657cf114861b21d60c768f14bef","value":4556}},"160e06bf0ee84a18926a60fdee13b4cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0d67b87e09b4673bac813f9371cb443","placeholder":"​","style":"IPY_MODEL_a7f08528cf6e4affb443f6d5dbd1fde7","value":" 4.56k/4.56k [00:00&lt;00:00, 244kB/s]"}},"f7acacebafd948c1b1455900ead30d18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58509191945c46789e87489706468c9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb8780679ad74c56a88bbb2ee6372520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5510ad50514c4b248aaa34159f5a2a45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1254657cf114861b21d60c768f14bef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0d67b87e09b4673bac813f9371cb443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f08528cf6e4affb443f6d5dbd1fde7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff371dce560845a5a5b0f552561477e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7b3f2197358405480cf2f6e976eb5f6","IPY_MODEL_c821c3e595994bd2a5605b80c8d0a26c","IPY_MODEL_3d814c1adf6c47669cd1985c46575480"],"layout":"IPY_MODEL_3cdd4a5db5f747ffbab94b4399c51da1"}},"c7b3f2197358405480cf2f6e976eb5f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e6dc3759bb942f5bdae00143976a393","placeholder":"​","style":"IPY_MODEL_e97317f78b554ff88edb5d7ade3f3cfa","value":"Downloading: 100%"}},"c821c3e595994bd2a5605b80c8d0a26c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f98b9f4acb44a04824da6fc04cecede","max":2071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a46b23b3f8774870883f5cc486b0b0ed","value":2071}},"3d814c1adf6c47669cd1985c46575480":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acdb430f632d4b8f9401cd9482fea549","placeholder":"​","style":"IPY_MODEL_b5ba958e5fe9403f8e38c5811a038b4f","value":" 2.07k/2.07k [00:00&lt;00:00, 106kB/s]"}},"3cdd4a5db5f747ffbab94b4399c51da1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e6dc3759bb942f5bdae00143976a393":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e97317f78b554ff88edb5d7ade3f3cfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f98b9f4acb44a04824da6fc04cecede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46b23b3f8774870883f5cc486b0b0ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acdb430f632d4b8f9401cd9482fea549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5ba958e5fe9403f8e38c5811a038b4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50ae47e678b74e92a4cdb590efc81f2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac610ca4a49f4063bb96619866bd651e","IPY_MODEL_ec8132fa2fec4272999018153c71dce5","IPY_MODEL_4e0970e3bef64991a79467573d927a79"],"layout":"IPY_MODEL_6bfef77978a9457cadda3d830ae4f533"}},"ac610ca4a49f4063bb96619866bd651e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_855efa0f279141e39ecc44d51809c62d","placeholder":"​","style":"IPY_MODEL_c0083a25a93a44dbaca8ff4592856512","value":"Downloading: 100%"}},"ec8132fa2fec4272999018153c71dce5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bb5f61a9fe448df89bd90a76b999997","max":84125825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93683b7bc6de4d55a0743c0aec90a781","value":84125825}},"4e0970e3bef64991a79467573d927a79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53b1b78e4ae342fbad6471425a27fd84","placeholder":"​","style":"IPY_MODEL_a6f490d9dfb14822b531e9bd54bf6197","value":" 84.1M/84.1M [00:09&lt;00:00, 16.9MB/s]"}},"6bfef77978a9457cadda3d830ae4f533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855efa0f279141e39ecc44d51809c62d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0083a25a93a44dbaca8ff4592856512":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bb5f61a9fe448df89bd90a76b999997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93683b7bc6de4d55a0743c0aec90a781":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53b1b78e4ae342fbad6471425a27fd84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6f490d9dfb14822b531e9bd54bf6197":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f11e4e6459e45309675623534d4a365":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_299048368be64c1296293482fcd1f958","IPY_MODEL_81bb01647f2a471c84571723f86fc68e","IPY_MODEL_efc17f28e1a24377911d57902b7e8e81"],"layout":"IPY_MODEL_0849069658734d66944b84ae104da927"}},"299048368be64c1296293482fcd1f958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_002d58f5f58f4f99b9fe1c0db68685b9","placeholder":"​","style":"IPY_MODEL_34a660ece7ef4c868b3f4fcbc28ddd9a","value":""}},"81bb01647f2a471c84571723f86fc68e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_1639a01b58dc4b9aa36c5c8a11896d78","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76db9a4e7cb144f58b03779a26685928","value":1}},"efc17f28e1a24377911d57902b7e8e81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6761c7f590a0441a814e3728420f0268","placeholder":"​","style":"IPY_MODEL_7290cbc7a69b4cc885ce4418396de44c","value":" 24726/0 [00:01&lt;00:00, 21204.81 examples/s]"}},"0849069658734d66944b84ae104da927":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"002d58f5f58f4f99b9fe1c0db68685b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34a660ece7ef4c868b3f4fcbc28ddd9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1639a01b58dc4b9aa36c5c8a11896d78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"76db9a4e7cb144f58b03779a26685928":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6761c7f590a0441a814e3728420f0268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7290cbc7a69b4cc885ce4418396de44c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ba9280221934939b8d49c10fa21ebab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abd44d31cc174c0a9e01f4f7e5459734","IPY_MODEL_b317c892913b4d288aae9e8db4d5ec97","IPY_MODEL_08bacf3c6f394dcb9856a42d9da5c21e"],"layout":"IPY_MODEL_691fced080254b0eb7f678799cadb8e2"}},"abd44d31cc174c0a9e01f4f7e5459734":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c1e5c28db5e48ce8c6e5962c1c616b5","placeholder":"​","style":"IPY_MODEL_4147ee702d844310bdea47759c35c625","value":""}},"b317c892913b4d288aae9e8db4d5ec97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee8e428dac9e49d8b901c7344a05f82b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f30ed79035f3469b87c38df8afd08c82","value":1}},"08bacf3c6f394dcb9856a42d9da5c21e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28b70bda63c048d1a75be1c1ac84ee51","placeholder":"​","style":"IPY_MODEL_8e02a57aafc849aa9e7da6211b43feab","value":" 24316/0 [00:01&lt;00:00, 21765.62 examples/s]"}},"691fced080254b0eb7f678799cadb8e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"3c1e5c28db5e48ce8c6e5962c1c616b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4147ee702d844310bdea47759c35c625":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee8e428dac9e49d8b901c7344a05f82b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f30ed79035f3469b87c38df8afd08c82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28b70bda63c048d1a75be1c1ac84ee51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e02a57aafc849aa9e7da6211b43feab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab213f00ff504fe2a4484b49673e7e2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d3e1c1866a74667aa749e8dd461c18f","IPY_MODEL_68595da0f0344b9cb5a0792fe8f012e4","IPY_MODEL_35cba0cc4bfe4562b68f585c340630e2"],"layout":"IPY_MODEL_e1da967df41f4b0ba3ac97b1bee59e92"}},"7d3e1c1866a74667aa749e8dd461c18f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edb660600d134959b2511256806a7339","placeholder":"​","style":"IPY_MODEL_679c7d36626b457db0d69c1391c9b29f","value":""}},"68595da0f0344b9cb5a0792fe8f012e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe886707ef59491aa56dd33f31a4b71e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca111cbf5a2140549987a1f3b6b86fde","value":1}},"35cba0cc4bfe4562b68f585c340630e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c84fd1ee900e4cf8bca6c5e869034b65","placeholder":"​","style":"IPY_MODEL_4a3b7d4a289644b5a61637c55803cf28","value":" 50000/0 [00:02&lt;00:00, 23556.34 examples/s]"}},"e1da967df41f4b0ba3ac97b1bee59e92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"edb660600d134959b2511256806a7339":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"679c7d36626b457db0d69c1391c9b29f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe886707ef59491aa56dd33f31a4b71e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ca111cbf5a2140549987a1f3b6b86fde":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c84fd1ee900e4cf8bca6c5e869034b65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a3b7d4a289644b5a61637c55803cf28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2d9c9e0a47e4583acd3f7fef3a18ae8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6a41831fd2c41b89eb183f378c67502","IPY_MODEL_de402f2f531446c9b43223f28ef62fcc","IPY_MODEL_1cc0c17bbc8943ea8aaf7e00e23bb38c"],"layout":"IPY_MODEL_6eef4d8a86dc44429738d5175efeb957"}},"d6a41831fd2c41b89eb183f378c67502":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68ad25aa06c44e3886715c79016f5beb","placeholder":"​","style":"IPY_MODEL_c88d6f9cd394453386b1b0f92de88885","value":"100%"}},"de402f2f531446c9b43223f28ef62fcc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9848cd435d484043a483e93dd20fe7d7","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2325307cef984c1aa5112a5d117747b9","value":25}},"1cc0c17bbc8943ea8aaf7e00e23bb38c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_198f15593b0946339bcd5209c86e6a86","placeholder":"​","style":"IPY_MODEL_0dc04f0695b84ea6a4bfb30a6a9f6be5","value":" 25/25 [00:41&lt;00:00,  1.83s/it]"}},"6eef4d8a86dc44429738d5175efeb957":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ad25aa06c44e3886715c79016f5beb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c88d6f9cd394453386b1b0f92de88885":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9848cd435d484043a483e93dd20fe7d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2325307cef984c1aa5112a5d117747b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"198f15593b0946339bcd5209c86e6a86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dc04f0695b84ea6a4bfb30a6a9f6be5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4de7d208fa342b19f129ef7822cff0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5423caacb0d04eb6a6f3ccf5a103fc90","IPY_MODEL_ef9207a286de4892a453f0c5f69de1e3","IPY_MODEL_710161159199426a90d49cdf435fbe11"],"layout":"IPY_MODEL_0602d660e3e641f78ece4251f40e0ef1"}},"5423caacb0d04eb6a6f3ccf5a103fc90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a2479fa8aa546f68fc0c5a96f87c90a","placeholder":"​","style":"IPY_MODEL_d5a0bd19fb6a46f283c25bda60e8c1e6","value":"100%"}},"ef9207a286de4892a453f0c5f69de1e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b15eaab59f344fc0b2f1f0976ab4f063","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d677db8900974cf38ca1b18659fc5243","value":25}},"710161159199426a90d49cdf435fbe11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c01fa01a4994cf78bb717b657d50013","placeholder":"​","style":"IPY_MODEL_0975de5ba8504695a9bebf0b30da399e","value":" 25/25 [00:39&lt;00:00,  1.43s/it]"}},"0602d660e3e641f78ece4251f40e0ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a2479fa8aa546f68fc0c5a96f87c90a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5a0bd19fb6a46f283c25bda60e8c1e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b15eaab59f344fc0b2f1f0976ab4f063":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d677db8900974cf38ca1b18659fc5243":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c01fa01a4994cf78bb717b657d50013":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0975de5ba8504695a9bebf0b30da399e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"gU-Arpc9zElH"},"source":["---\n",">「ドライバーが車を選ぶんじゃない。車がドライバーを選ぶんだ。人間と機械の間には、神秘的な絆があるんだ」 \\\n",">（ボビー:映画トランスフォーマーより）\n","---"]},{"cell_type":"markdown","metadata":{"id":"D1PpwmcMJFRB"},"source":["# Transformerモデルを用いた文章分類\n","\n","シンプルなクラス分類のTransformerモデルをフルスクラッチで実装する\n","- 映画の英語レビューがポジテイプな内容かネガテイプな内容かを判定させる\n","- どのような単語に注目して判定したのかをSelf-Attentionの結果から可視化する\n","\n","**<font color=\"red\">+++注意+++</font>**\n","\n","ライブラリの更新が速いため、バージョン違いによる動作エラーが発生する場合があります\n","- 現在把握している問題については、回避方法を記述していますが、再起動が必要となるなど、単純に実行しただけでは結果を得ることができない場合があります"]},{"cell_type":"markdown","source":["## なぜTransformerが重要なのか\n","\n","Transformerは、自然言語処理に適する方法として見出されたが、これにはどのような経緯があったのか、簡単に説明する\n"],"metadata":{"id":"_eDIB5jStfZc"}},{"cell_type":"markdown","source":["### 自然言語処理はなぜ困難なのか\n","\n","まず、言語データは画像データと本質的に扱い方が異なる\n","\n","- 画像データは画素の集まりであり、CNNでは画像全体から近隣の画素の特徴をとらえて処理する\n","- 言語データは単語を逐次的に聞いて処理する\n","  - つまり、過去の単語の入力情報を保持し、文脈を理解する必要がある\n","  - 例えば、「まいった」だけではわからないが、「失敗してまいった」「神社にまいった」となればわかる\n","\n","言語データの性質から、逐次的に処理する仕組みが必要\n","- 従来は、この性質から、RNN (Recurrent Neural Network)やLSTM (Long short-term memory)などが利用されてきた\n","  - 内部状態として過去の状態を記録することができるため\n","- 一方で、RNN、LSTMは学習時間が長く大きなモデルを構築するのが困難であった\n","  - 文章の単語を1stepに1単語ずつモデルに投入するため、バッチにより並列的に大量に処理できるCNNと異なり時間がかかる\n","\n","処理速度を稼ぐため、CNNやFCを利用する試みもなされた\n","- 処理速度は向上するが、やはり文章の離れた単語間の関係性を考慮できないため、精度の向上が困難であった\n","  - 主語と述語は文章の最初と最後であり、この主述関係を理解しようとするとCNNや勾配消失の大きいRNNでは困難であることは容易に想像できる"],"metadata":{"id":"rVw9AeeL5sI2"}},{"cell_type":"markdown","source":["### Attentionの登場\n","\n","既に説明済みであるがTransformerではAttentionという新しい概念が提案、利用されている\n","\n","- Attentionの演算は内積演算であり、同じ向きを向いた大きな要素間では大きな値となるため、これを類似度として扱う\n","  - これを値の類似度としてとらえるには抵抗を感じるかもしれないが、ひとまずここではこのようにして計算した値を類似度とする\n","- この掛け算の値をAttention Mapと呼ぶ\n","\n","- Self-AttentionおよびSource-Target-Attentionを利用することで、言語データでも離れた位置の単語間の関係を考慮できる可能性がある"],"metadata":{"id":"Ciq_J_jV5x1e"}},{"cell_type":"markdown","metadata":{"id":"3IBOjXDeJFRE"},"source":["## 事前準備\n","\n","今回はフルスクラッチで記述するため、シンプルである\n","- このあと機能に特化したライブラリは個別に読み込んでいる\n","- なお、PyTorchのTransformerライブラリを用いるなどして、個別モジュールの設計を避けてパーツを組み合わせることで実装することを推奨する\n","- 提供されるライブラリは下記の記述よりも実行速度が速い、最適化されている、より優れた実装が採用されている、なによりも精度が高くなるなど、良いことばかりであり、そもそも利用するという立場では一から設計する意味はほとんどない"]},{"cell_type":"code","metadata":{"id":"w1kMABqAJFRF"},"source":["import math\n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchtext\n","from torch.utils.data import DataLoader\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["nlpライブラリは様々なデータセットを提供している\n","- 映画評論データセットを入手するために利用する"],"metadata":{"id":"6Yp9AeCnpwLj"}},{"cell_type":"code","metadata":{"id":"7qg6t5nnBjqs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692262001108,"user_tz":-540,"elapsed":16113,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f1291aed-0ab2-42d9-f1a0-d26fd5473162"},"source":["!pip install nlp\n","!pip install --force-reinstall dill==0.3.5.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlp\n","  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nlp) (1.23.5)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (9.0.0)\n","Collecting dill (from nlp)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nlp) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from nlp) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from nlp) (3.12.2)\n","Collecting xxhash (from nlp)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n","Installing collected packages: xxhash, dill, nlp\n","Successfully installed dill-0.3.7 nlp-0.4.0 xxhash-3.3.0\n","Collecting dill==0.3.5.1\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.7\n","    Uninstalling dill-0.3.7:\n","      Successfully uninstalled dill-0.3.7\n","Successfully installed dill-0.3.5.1\n"]}]},{"cell_type":"markdown","source":["transformersライブラリを読み込んでいるが、transformerのモデルを利用するわけではない\n","- ここでは、AutoTokenizerを利用するために読み込んでいる\n","- AutoTokenizerはHuggingfaceが提供している有用性の高いライブラリであり、今後主流となる予感がする\n","- AutoTokenizerはDataLoaderとの相性がよいため安心して利用できる\n","  - `BertForSequenceClassification.from_pretrained`などを用いることもできるが、バッチ処理がかなり面倒になるであろう\n","  - BertTokenizerFastに引けをとらない処理速度を有している\n","\n","**重要な点**\n","今回は学習させるため、どのようなTokenizerを用いても構わない\n","- なんなら自作でも構わない\n","\n","事前学習済みモデルを利用する場合は、そのモデルが用いたTokenizerを用いなければ正しい結果を得ることができない\n","- 当然であるが、「私」を10に変換していたのが、変わって20に変換されては精度が落ちて当然\n","- 危険なのは、この件に限らず、間違えても頑張って学習する結果、それなりに精度が出るため、「誤りを、誤りと気づきにくい」点に注意が必要である"],"metadata":{"id":"a6N8vPoOp4-Z"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrkJBTwtoGXN","executionInfo":{"status":"ok","timestamp":1692262022142,"user_tz":-540,"elapsed":21045,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c7607c6d-b84c-45b5-ab24-6bdb3374bd55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZsgQNMJxpBnW"},"source":["### モデルとTokenizerの読み込み\n","事前学習済みのモデルと、これと紐づいたTokenizerを読み込む\n","- いつもと同様、bert-base-uncased 事前学習モデルを読み込む"]},{"cell_type":"code","metadata":{"id":"9R0HK29fHrf3","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["b71bdf5f159c4d42b6c418a04cd3e862","e5c3ae58281a47d6bc4cc8fda8f2e7c2","d07f7fbdb15543dcae170df6aa36a393","be3ed39c14af4765b8a41fe004494a5f","2837e0768e8943d8898ef8cd2c655a2b","ed9409ebb09b4c3095728286338c886e","ef93f5fe589d4090b473d40b4632fcc3","e292015b82eb4185807927c90ebe7a0f","eef0eb2d380f4fe2a5662d7cba605585","0eb68b03eaf94d92bcda98fae3548151","bd7aa322253649f9884acabb5a9a6847","a4e347d25f974571bea02fb5362b0409","c6890593b1de4ac899e17eb97d3a836a","e27a3b073dc94b98977d12e73c9eccd7","09cca75ce77c4343ae0315254a307ed9","a9f2c0b86dcf46d4b62b0556beeeb6f9","78548bde7c0949588eac6a9e61c9396b","f9a3b7636e114730ae0d6b6f1d9a70b8","06b60d9cd55d4e2dbe041b503ede0b36","3f5f4f45e17245c89c977dfa5f6b9750","f9746cfffd154e37bf29aeb46c8e6226","66fb652463c54ee3bd19feb4e561a0e6","315f17b8acfe445181351f9f346f7824","02c779ba4446447a8fc21b889ee9f726","4c412e5988574898804da52e82a31702","533559a9002749feb107944482272627","31e1272dd3c3440c8a857f527030fe86","c8adac3798c6440ea83bac7699a0e66f","b0a07c739ea942ef83a597aaa6dd044c","50c49698f24443aa956ff8bf13a4d04d","f34c698b0c544115aaa544fa57fcad79","e068202d4f21430694c061e8b4f18c19","e0ab63bd2ab74cd1aa934bef62615551","c6b6f339eeab4ec98514b16a4340a039","88b7085b4dcd487c92d1ea2718c9a5de","e0cdfb00d7a64d5092ee12320b93fbe6","30fb06bb4fb24c4eacbbecae604d2779","560ceaf4d1ee482386c722b55e17c1ae","7dae048a55a1483888c49fa18a7540b0","38e9c48375f44fe590d2f1e026ea57ab","14348811134d4b1899e6e684952905a3","bdbc7e329a9746b4bbf9a11b1652c4b4","ebcec7e18b054f87893d57b749815fb0","aaa81565dcac4f11a81418852a2751f0"]},"executionInfo":{"status":"ok","timestamp":1692262024265,"user_tz":-540,"elapsed":2132,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"bf94983c-cb48-4593-bc6c-3a7f56e47d74"},"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b71bdf5f159c4d42b6c418a04cd3e862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e347d25f974571bea02fb5362b0409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"315f17b8acfe445181351f9f346f7824"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b6f339eeab4ec98514b16a4340a039"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"aWCmm2TjqToE"},"source":["### データセットの読み込み\n","\n","nlpライブラリに含まれるIMDbデータセットを利用する\n","- IMDbデータセットは、ポジティブかネガティブの好悪感情を表すラベルが付与された25000の映画レビューコメントデータセット\n","- 好意的なレビューは1、否定的なレビューは0が振られている\n","- 感情分析用では鉄板のデータセット\n","\n","https://www.imdb.com/interfaces/"]},{"cell_type":"code","metadata":{"id":"rfEnNpv9HuXI","colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["6c68dfcfe51442e3a191fec84645be8b","172828cc83384387938c608cc1427f28","3b818c115d9b457e8c8f2df532e2d149","160e06bf0ee84a18926a60fdee13b4cd","f7acacebafd948c1b1455900ead30d18","58509191945c46789e87489706468c9e","fb8780679ad74c56a88bbb2ee6372520","5510ad50514c4b248aaa34159f5a2a45","e1254657cf114861b21d60c768f14bef","f0d67b87e09b4673bac813f9371cb443","a7f08528cf6e4affb443f6d5dbd1fde7","ff371dce560845a5a5b0f552561477e4","c7b3f2197358405480cf2f6e976eb5f6","c821c3e595994bd2a5605b80c8d0a26c","3d814c1adf6c47669cd1985c46575480","3cdd4a5db5f747ffbab94b4399c51da1","3e6dc3759bb942f5bdae00143976a393","e97317f78b554ff88edb5d7ade3f3cfa","4f98b9f4acb44a04824da6fc04cecede","a46b23b3f8774870883f5cc486b0b0ed","acdb430f632d4b8f9401cd9482fea549","b5ba958e5fe9403f8e38c5811a038b4f","50ae47e678b74e92a4cdb590efc81f2e","ac610ca4a49f4063bb96619866bd651e","ec8132fa2fec4272999018153c71dce5","4e0970e3bef64991a79467573d927a79","6bfef77978a9457cadda3d830ae4f533","855efa0f279141e39ecc44d51809c62d","c0083a25a93a44dbaca8ff4592856512","8bb5f61a9fe448df89bd90a76b999997","93683b7bc6de4d55a0743c0aec90a781","53b1b78e4ae342fbad6471425a27fd84","a6f490d9dfb14822b531e9bd54bf6197","6f11e4e6459e45309675623534d4a365","299048368be64c1296293482fcd1f958","81bb01647f2a471c84571723f86fc68e","efc17f28e1a24377911d57902b7e8e81","0849069658734d66944b84ae104da927","002d58f5f58f4f99b9fe1c0db68685b9","34a660ece7ef4c868b3f4fcbc28ddd9a","1639a01b58dc4b9aa36c5c8a11896d78","76db9a4e7cb144f58b03779a26685928","6761c7f590a0441a814e3728420f0268","7290cbc7a69b4cc885ce4418396de44c","4ba9280221934939b8d49c10fa21ebab","abd44d31cc174c0a9e01f4f7e5459734","b317c892913b4d288aae9e8db4d5ec97","08bacf3c6f394dcb9856a42d9da5c21e","691fced080254b0eb7f678799cadb8e2","3c1e5c28db5e48ce8c6e5962c1c616b5","4147ee702d844310bdea47759c35c625","ee8e428dac9e49d8b901c7344a05f82b","f30ed79035f3469b87c38df8afd08c82","28b70bda63c048d1a75be1c1ac84ee51","8e02a57aafc849aa9e7da6211b43feab","ab213f00ff504fe2a4484b49673e7e2e","7d3e1c1866a74667aa749e8dd461c18f","68595da0f0344b9cb5a0792fe8f012e4","35cba0cc4bfe4562b68f585c340630e2","e1da967df41f4b0ba3ac97b1bee59e92","edb660600d134959b2511256806a7339","679c7d36626b457db0d69c1391c9b29f","fe886707ef59491aa56dd33f31a4b71e","ca111cbf5a2140549987a1f3b6b86fde","c84fd1ee900e4cf8bca6c5e869034b65","4a3b7d4a289644b5a61637c55803cf28"]},"executionInfo":{"status":"ok","timestamp":1692262071994,"user_tz":-540,"elapsed":47731,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c2d40913-2ae4-4fcc-a176-420aa75669dc"},"source":["from nlp import load_dataset\n","raw_train_data, raw_test_data = load_dataset(\"imdb\", split=[\"train\", \"test\"]) # 訓練用と検証用データに分けて読み込む"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c68dfcfe51442e3a191fec84645be8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff371dce560845a5a5b0f552561477e4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown sizetotal: 207.28 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ae47e678b74e92a4cdb590efc81f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f11e4e6459e45309675623534d4a365"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba9280221934939b8d49c10fa21ebab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab213f00ff504fe2a4484b49673e7e2e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743. Subsequent calls will reuse this data.\n"]}]},{"cell_type":"markdown","metadata":{"id":"7t_nwDeOX2Ok"},"source":["例としてデータを表示する\n","- 英語です、がっかりしましたか？"]},{"cell_type":"code","metadata":{"id":"EAB_DeeTX1uu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692262071994,"user_tz":-540,"elapsed":12,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"35c592a6-1a98-404e-bf1d-c562ae2369d5"},"source":["print(raw_train_data[\"label\"][0], raw_train_data[\"text\"][0])  # 好意的なコメントの例\n","print(raw_train_data[\"label\"][20000], raw_train_data[\"text\"][20000])  # 否定的なコメントの例"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n","0 This movie tries hard, but completely lacks the fun of the 1960s TV series, that I am sure people do remember with fondness. Although I am 17, I watched some of the series on YouTube a long time ago and it was enjoyable and fun. Sadly, this movie does little justice to the series.<br /><br />The special effects are rather substandard, and this wasn't helped by the flat camera-work. The script also was dull and lacked any sense of wonder and humour. Other films with under-par scripting are Home Alone 4, Cat in the Hat, Thomas and the Magic Railroad and Addams Family Reunion.<br /><br />Now I will say I liked the idea of the story, but unfortunately it was badly executed and ran out of steam far too early, and I am honestly not sure for this reason this is something for the family to enjoy. And I was annoyed by the talking suit, despite spirited voice work from Wayne Knight.<br /><br />But the thing that angered me most about this movie was that it wasted the talents of Christopher Lloyd, Jeff Daniels and Daryl Hannah, all very talented actors. Jeff Daniels has pulled off some good performances before, but he didn't seem to have a clue what he was supposed to be doing, and Elizabeth Hurley's character sadly came across as useless. Daryl Hannah is a lovely actress and generally ignored, and I liked the idea of her being the love interest, but sadly you see very little of her,(not to mention the Monster attack is likely to scare children than enthrall them) likewise with Wallace Shawn as some kind of government operative. Christopher Lloyd acquits himself better, and as an actor I like Lloyd a lot(he was in two of my favourite films Clue and Who Framed Roger Rabbit, and I am fond of Back To The Future) but he was given little to work with, and had a tendency to overact quite wildly.<br /><br />Overall, as much I wanted to like this movie, I was left unimpressed. Instead of being fun, it came across as pointless, and that is a shame because it had a lot of potential, with some talented actors and a good idea, but wasted with poor execution. 1/10 Bethany Cox\n"]}]},{"cell_type":"markdown","metadata":{"id":"CFIcwJVIaAs-"},"source":["DeepLで訳してみると次のような感じです\n","\n","> 1 ブロムウェル・ハイ」は、カートゥーン・コメディです。ブロムウェル・ハイ』は、『ティーチャーズ』のような学校生活を描いた番組と同時期に放送されていました。私の35年間の教師生活を振り返ると、「ブロムウェル・ハイ」の風刺は「ティーチャーズ」よりもはるかに現実に近いものだと思います。経済的に生き残るために奔走する姿、哀れな教師たちの虚勢を見抜く洞察力のある生徒たち、そしてすべての状況の情けなさは、私が知っている学校とその生徒たちを思い出させてくれます。生徒が何度も学校を燃やそうとしたエピソードを見たとき、すぐに ......... .......... のことを思い出しました。高いですね。古典的なセリフです。検閲官：あなた方の先生の一人をクビにするために来ました。生徒：Bromwell Highへようこそ。私と同年代の大人の多くは、「ブロムウェルハイ」を奇想天外なものだと思っているのではないでしょうか。そうでないのが残念です。\n","\n","> 0 この映画は努力していますが、1960年代のテレビシリーズの面白さが完全に欠けています。私は17歳ですが、ずいぶん前にYouTubeでこのシリーズを見たことがあり、楽しくて仕方がありませんでした。特殊効果は標準的ではなく、平板なカメラワークによって助けられていませんでした。また、「ホームアローン4」、「帽子をかぶった猫」、「きかんしゃトーマス」、「アダムス・ファミリー・リユニオン」などの作品があります。さて、ストーリーのアイデアは良かったのですが、残念ながら出来が悪く、早々に力尽きてしまったので、正直、家族で楽しめる作品ではないと思います。また、ウェイン・ナイトが気合を入れて演じたにもかかわらず、しゃべるスーツにも腹が立ちました。しかし、この映画で最も腹が立ったのは、クリストファー・ロイド、ジェフ・ダニエルズ、ダリル・ハンナという才能ある俳優を無駄にしてしまったことです。ジェフ・ダニエルズはこれまでも良い演技をしてきましたが、彼は何をすべきかわからないようでしたし、エリザベス・ハーリーのキャラクターも残念ながら役立たずでした。ダリル・ハンナは素敵な女優だが、一般的には無視されており、私は彼女が愛の対象になるというアイデアが好きだったが、残念ながら彼女の姿はほとんど見られない。（モンスターの攻撃は、子供たちを魅了するというよりも、怖がらせる可能性が高いのは言うまでもない）同様に、ウォレス・ショーンもある種の政府の工作員として登場する。        1/10 ベサニー・コックス"]},{"cell_type":"markdown","source":["試しに一文をTokenizerで単語IDに変換する"],"metadata":{"id":"qBcib8lrsMTG"}},{"cell_type":"code","source":["torch.tensor(tokenizer(raw_train_data['text'][0], max_length=10)['input_ids'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlwUvUuYAKRq","executionInfo":{"status":"ok","timestamp":1692262071995,"user_tz":-540,"elapsed":10,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c5cb1542-528a-4200-e09d-c55ee091f80f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([  101, 22953,  2213,  4381,  2152,  2003,  1037,  9476,  4038,   102])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["必要なパラメタを定義する\n","- 各データセットの文章数\n","- バッチサイズ\n","- 最大の文章長さ(これ以上の長さは切られる)"],"metadata":{"id":"dmbl19bhsUo4"}},{"cell_type":"code","source":["num_train_data = raw_train_data.num_rows\n","num_test_data = raw_test_data.num_rows\n","batch_size = 32\n","max_seq_len = 256"],"metadata":{"id":"uOMmUJuzINJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tz0UpoNQYIzs"},"source":["mapメソッドを利用して各データに前処理を施す\n","- ここではtokenizeを定義し、このtokenizeを全データに施す\n","- tokenizeは読み込んだIMDbのデータをTokenizerで処理し、語句IDに変換する関数である\n","- バッチサイズはデータ全体、つまり全データに対して一気に処理している(順番に取り出して何かするのではないため、これでよい)\n","- \"input_ids\", \"attention_mask\", \"label\"の順番にデータを並べて、PyTorchで利用できるようにPyTorchのDataLoaderと同様の形で出力させる\n","- max_lengthで長い文章をここで制限しておく\n","  - つけないと512になる\n","\n","なお、set_formatのtype=\"torch\"は、torch.tensorで出力する指定である\n","- だが、set_format自体が変換するわけではなく、tokenizerに渡して変換する仕様のようだ\n","- 従って、指定のtokenizerを利用しなければ変換されないので注意\n","  - これがAutoTokenizerを使う大きな理由の一つであり、バッチ化を簡単にできる\n","\n","本来、テストデータはシャッフルする必要はないが、最後に乱雑に確認したいため、シャッフルしている\n","- **普通ではないので注意**"]},{"cell_type":"code","metadata":{"id":"Z2UD6DkjXzto","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["b2d9c9e0a47e4583acd3f7fef3a18ae8","d6a41831fd2c41b89eb183f378c67502","de402f2f531446c9b43223f28ef62fcc","1cc0c17bbc8943ea8aaf7e00e23bb38c","6eef4d8a86dc44429738d5175efeb957","68ad25aa06c44e3886715c79016f5beb","c88d6f9cd394453386b1b0f92de88885","9848cd435d484043a483e93dd20fe7d7","2325307cef984c1aa5112a5d117747b9","198f15593b0946339bcd5209c86e6a86","0dc04f0695b84ea6a4bfb30a6a9f6be5","f4de7d208fa342b19f129ef7822cff0a","5423caacb0d04eb6a6f3ccf5a103fc90","ef9207a286de4892a453f0c5f69de1e3","710161159199426a90d49cdf435fbe11","0602d660e3e641f78ece4251f40e0ef1","2a2479fa8aa546f68fc0c5a96f87c90a","d5a0bd19fb6a46f283c25bda60e8c1e6","b15eaab59f344fc0b2f1f0976ab4f063","d677db8900974cf38ca1b18659fc5243","7c01fa01a4994cf78bb717b657d50013","0975de5ba8504695a9bebf0b30da399e"]},"executionInfo":{"status":"ok","timestamp":1692262152958,"user_tz":-540,"elapsed":80969,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"50ba7598-d4e5-498e-bd44-17cd395923d7"},"source":["train_data = raw_train_data.map(lambda e: tokenizer(e['text'], truncation=True, max_length=max_seq_len, padding='max_length'), batched=True)\n","train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","test_data = raw_test_data.map(lambda e: tokenizer(e['text'], truncation=True, max_length=max_seq_len, padding='max_length'), batched=True)\n","test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True) # ここのシャッフルは意味がないが最後の試行を乱雑にしたい"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d9c9e0a47e4583acd3f7fef3a18ae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4de7d208fa342b19f129ef7822cff0a"}},"metadata":{}}]},{"cell_type":"markdown","source":["train_dataの中身は次の通り"],"metadata":{"id":"Zkt-yrSqs49X"}},{"cell_type":"code","source":["train_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-OJW-XGyt8Dp","executionInfo":{"status":"ok","timestamp":1692262152959,"user_tz":-540,"elapsed":14,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"853c93d6-36b7-49e0-b7e8-3de381f86823"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset(features: {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None), 'text': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, num_rows: 25000)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["train_dataloaderから試しにデータを取得する"],"metadata":{"id":"ES7ekokus76v"}},{"cell_type":"code","source":["next(iter(train_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8M7xA7-ktYvo","executionInfo":{"status":"ok","timestamp":1692262153757,"user_tz":-540,"elapsed":808,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c87985a5-27a2-43d2-bf3d-ebab0b1cd3ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n","         0, 1, 0, 0, 0, 1, 0, 0]),\n"," 'input_ids': tensor([[ 101, 2066, 2070,  ...,    0,    0,    0],\n","         [ 101, 2023, 2003,  ...,    0,    0,    0],\n","         [ 101, 5674, 2008,  ..., 2000, 1037,  102],\n","         ...,\n","         [ 101, 1045, 1005,  ...,    0,    0,    0],\n","         [ 101, 2026, 2564,  ..., 1026, 7987,  102],\n","         [ 101, 2242, 2000,  ..., 7472, 2007,  102]]),\n"," 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 1, 1, 1],\n","         ...,\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 1, 1, 1],\n","         [1, 1, 1,  ..., 1, 1, 1]])}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["語彙数、つまりTokenizerが知っている単語の種類の数をパラ目として設定する"],"metadata":{"id":"Y-izvBgJvpbb"}},{"cell_type":"code","source":["vocab_size = tokenizer.vocab_size\n","vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJ00Kxq20qSZ","executionInfo":{"status":"ok","timestamp":1692262153758,"user_tz":-540,"elapsed":8,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"676f7c7a-a155-48d3-c97d-f0d185d5b2eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Transformerのネットワーク構造\n"],"metadata":{"id":"Revn7i8dCLlO"}},{"cell_type":"markdown","source":["### 各層の結合とデータサイズ\n","\n","入カはミニバッチ数$M(=256)$、一文の単語数$W(=256)$とすると、$M\\times W$\n","\n","処理は次の通りとなる\n","\n","- 内部の単語の表現ベクトル$V(=300)$とすると、Embedderにより、単語一つがV次元のベクトル表現になり、その出力は$M\\times W \\times V$となる\n","  - Embedding層は内部で、まず入力記号OneHotベクトルに変換し、そのOneHotベクトルをより低次のベクトル空間上に線形写像している\n","- Embedderの後、PositionalEncoder、TransformerBlockへと処理が映るが、これらは全て入出力で次元を変更していない\n","- 最後のTransformerBlockの出力がClassificationHeadにおいて、クラス数$C(=2)$に変換され、結果的に$M\\times C$となる"],"metadata":{"id":"ik1knMVh9mBD"}},{"cell_type":"markdown","source":["### 各層の動作内容\n","\n","- Embedding\n","  - ここでは、PyTorchが提供するnn.Embeddingを用いており、誤差逆伝搬により更新される\n","  - その他、fasttextや、Word2Vecなどによる事前学習に基づいた分散表現変換も想定される\n","- PositionalEncoder\n","  - 入カデータに位置情報を足し算する\n","  - Self-Attentionを利用するため、各単語がどの単語と関係するかはAttentionで計算、獲得できる\n","  - すると、入力文章の単語の順番がシャッフルされた場合、同様に処理すると、語順という概念が欠落して同じ結果が出る可能性がある、すなわち語順が考慮されない、という問題を解決する\n","  - 今回のように文章の構造を判断材料に入れたいという場合に導入している\n","\n","- TransformerBlock\n","  - 任意の回数繰り返して利用する\n","    - Transformerの図で$\\times n$と記載されている通り\n","  - ここでは2段構成となっている\n","  - 入力のmaskはAttention Mapの一部の値を0に置き換える\n","  - 文章がmax_lengthの256文字よりも短くパディング、つまり<pad>が埋め込まれている部分についてAttentionを求めないように、その重みを0とする\n","  - 翻訳タスクなどのデコーダ側では、マスクされた単語を補完する、マスク位置を次々とずらすことで文章を完成させるといったタスクを達成するために利用する\n","\n","- ClassificationHead\n","  - 今回のタスクがクラス分けであるため、Transformer標準ではないが、最後に設けて次元数2の出力に変換する"],"metadata":{"id":"RcquWAj4CYWr"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1Gr2vIJ1JK8p"}},{"cell_type":"markdown","source":["### Embedder\n","\n","既述は次の通り\n","\n","主なオプションは次の通り\n","- `num_embeddings（int）`: 埋め込み辞書のサイズ\n","- `embedding_dim（int）`: 各埋め込みベクトルのサイズ\n","- `freeze=True`: ここでは利用していないが、誤差逆伝搬において内部重みの更新を阻止する"],"metadata":{"id":"qMSfC0FSJPZX"}},{"cell_type":"code","metadata":{"id":"iAB9spQJJFRM"},"source":["class Embedder(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim):\n","        super(Embedder, self).__init__()\n","        self.embeddings = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n","\n","    def forward(self, x):\n","        x_vec = self.embeddings(x)\n","        return x_vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Embedderの動作を確認する"],"metadata":{"id":"zWxZYSxo3rAn"}},{"cell_type":"code","source":["# モデル構築\n","net1 = Embedder(vocab_size,300)\n","# 入出力\n","test_batch = next(iter(train_loader))\n","x = test_batch['input_ids']\n","x1 = net1(x)  # 単語をベクトルに\n","print(\"入力のテンソルサイズ：\", x.shape)\n","print(\"出力のテンソルサイズ：\", x1.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1r9QL3S0vJa5","executionInfo":{"status":"ok","timestamp":1692262153758,"user_tz":-540,"elapsed":5,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"70e1b1ea-a10b-4628-d9ef-25018dd7e768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256])\n","出力のテンソルサイズ： torch.Size([32, 256, 300])\n"]}]},{"cell_type":"markdown","source":["### PositionalEncoder\n","\n","入力された単語の位置を示すベクトル情報`pe`を付加する\n","- 位置の計算式はTransfomerの論文のままの標準的な方法\n","- 文章が短く、単語ベクトルがPositional Encodingよりも小さい場合に対応するため、$\\sqrt{V}$を掛けて大きさをある程度そろえる処理が加わっている\n","- `pe`は何度も計算するわけではなく、コンストラクタにおいて、テーブルとして保持している\n","- `pe`は勾配計算の対象外であるため、`requires_grad = False`を忘れないように\n","  - 計算しても動作するがpeが更新され変更される\n","  - 結果として実行速度低下を招く\n","  - 精度低下を招くかどうかはなんともいえない\n","    - 課題としてトライしてみるとよいだろう"],"metadata":{"id":"6goQ4l5B1-md"}},{"cell_type":"code","metadata":{"id":"S-R3xrLWJFRU"},"source":["class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model=300, max_seq_len=max_seq_len, devname='cpu'):\n","        super().__init__()\n","        self.d_model = d_model  # 単語ベクトルの次元数\n","        pe = torch.zeros(max_seq_len, d_model)\n","        pe = pe.to(devname)\n","        for pos in range(max_seq_len):\n","            for i in range(0, d_model, 2):\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\n","        self.pe = pe.unsqueeze(0) # 表peの先頭に、ミニバッチ次元となる次元を足す\n","        self.pe.requires_grad = False # 勾配を計算しないようにする\n","    def forward(self, x):\n","        # 入力xとPositonal Encodingを足し算する\n","        # xがpeよりも小さいので、大きくする\n","        ret = math.sqrt(self.d_model)*x + self.pe\n","        return ret"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PositonalEncoderの動作確認を行う"],"metadata":{"id":"CZ4ZKoA6Mi_W"}},{"cell_type":"code","metadata":{"id":"SZoqYmiiJFRX","executionInfo":{"status":"ok","timestamp":1692262154109,"user_tz":-540,"elapsed":353,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"005cf084-6c7e-40fb-d41b-f32a0dae1e22"},"source":["# モデル構築\n","net2 = PositionalEncoder(d_model=300, max_seq_len=max_seq_len)\n","# 入出力\n","x = test_batch['input_ids']\n","x1 = net1(x)  # 単語をベクトルに\n","x2 = net2(x1)\n","print(\"入力のテンソルサイズ：\", x1.shape)\n","print(\"出力のテンソルサイズ：\", x2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256, 300])\n","出力のテンソルサイズ： torch.Size([32, 256, 300])\n"]}]},{"cell_type":"markdown","source":["### TransformerBlock\n","\n","LayerNormalization、Dropout、Attention、FeedForwardで構成\n","\n","- LayerNormalization: 各単語が持つ$V$個の特徴量に対し、その特徴量毎に正規化を行う\n","  - 各特徴量が持つ$V$次元の要素の平均と標準偏差が、それぞれ0と1になるように正規化\n","\n","- Attentionにおいて特徴量が変換\n","- その出力にDropoutしたベクトルとLayerNormalizationの入力のベクトルを足し算する\n","- FeedForwardにより特徴量変換を行う\n","\n","なお、オリジナルのTransformerはMulti-Head Attentionであるが、ここではSingle Attentionで実装している\n","- Single Attentionを複数並列するとMulti-Headになる\n","- Milti-headについては演習で扱う\n","\n","- テキストの隙間埋めパディング`<pad>`の部分のmask値は0であるが、Attentionにおいてはこの部分を-le9というマイナス無限大に近い値に置き換える\n","- 結果的に、その後のSofmax計算で邪魔をしなくなる\n","  - Attention Mapにおいて0になるようにするため\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/mytransformerblock.png\" width=500>"],"metadata":{"id":"v6vSrSptNX8m"}},{"cell_type":"code","metadata":{"id":"i8wcSoeDJFRa"},"source":["class Attention(nn.Module):\n","    def __init__(self, d_model=300):\n","        super().__init__()\n","        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n","        self.q_linear = nn.Linear(d_model, d_model)\n","        self.v_linear = nn.Linear(d_model, d_model)\n","        self.k_linear = nn.Linear(d_model, d_model)\n","        self.out = nn.Linear(d_model, d_model)  # 出力時に使用する全結合層\n","        self.d_k = d_model  # Attentionの大きさ調整の変数\n","\n","    def forward(self, q, k, v, mask):\n","        # 全結合層で特徴量を変換\n","        k = self.k_linear(k)\n","        q = self.q_linear(q)\n","        v = self.v_linear(v)\n","        # Attentionの値を計算する\n","        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)  # 値が大きくならないようroot(d_k)で割って調整\n","        mask = mask.unsqueeze(1) # maskを計算\n","        weights = weights.masked_fill(mask == 0, -1e9)\n","        normlized_weights = F.softmax(weights, dim=-1)  # softmaxで正規化\n","        output = torch.matmul(normlized_weights, v)  # AttentionをValueとかけ算\n","        output = self.out(output)  # 全結合層で特徴量を変換\n","        return output, normlized_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention層から出力を全結合層2つで特徴量を変換する"],"metadata":{"id":"eOog06Sxcr4T"}},{"cell_type":"code","metadata":{"id":"e1AcDAfjJFRe"},"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(d_model, d_ff)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_2 = nn.Linear(d_ff, d_model)\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.dropout(F.relu(x))\n","        x = self.linear_2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQ5Y2sDDJFRh"},"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, dropout=0.1):\n","        super().__init__()\n","        # LayerNormalization層\n","        self.norm_1 = nn.LayerNorm(d_model)\n","        self.norm_2 = nn.LayerNorm(d_model)\n","        # Attention層\n","        self.attn = Attention(d_model)\n","        # Attentionのあとの全結合層\n","        self.ff = FeedForward(d_model)\n","        # Dropout\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        x_normlized = self.norm_1(x)  # 正規化\n","        output, normlized_weights = self.attn(  # Attention\n","            x_normlized, x_normlized, x_normlized, mask)\n","        x2 = x + self.dropout_1(output)\n","        x_normlized2 = self.norm_2(x2)  # 正規化\n","        output = x2 + self.dropout_2(self.ff(x_normlized2)) # 全結合層\n","        return output, normlized_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 動作確認"],"metadata":{"id":"t2Yf3QtkNmmh"}},{"cell_type":"code","metadata":{"id":"KNaVWuNJJFRk","executionInfo":{"status":"ok","timestamp":1692262155001,"user_tz":-540,"elapsed":894,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47d56c21-cca5-459c-e8aa-8f8dc7b46a4c"},"source":["net3 = TransformerBlock(d_model=300)  # モデル構築\n","# maskの作成\n","x = test_batch['input_ids']\n","input_pad = 101  # 単語のIDにおいて、'<pad>': 1 なので\n","input_mask = (x != input_pad)\n","# 入出力\n","x1 = net1(x)  # 単語をベクトルに\n","x2 = net2(x1)  # Positon情報を足し算\n","x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n","print(\"入力のテンソルサイズ：\", x2.shape)\n","print(\"出力のテンソルサイズ：\", x3.shape)\n","print(\"Attentionのサイズ：\", normlized_weights.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256, 300])\n","出力のテンソルサイズ： torch.Size([32, 256, 300])\n","Attentionのサイズ： torch.Size([32, 256, 256])\n"]}]},{"cell_type":"markdown","source":["### ClassificationHead\n","\n","評価のPositive/Negativeの2つのクラス分類を出力する\n","\n","ここで、どこを利用して特徴量を抽出するかについて選択肢がいくつかある\n","- 文章全体を用いて特徴量を抽出する\n","  - もちろん可能であり、こちらの方がよさそうですが、問題として文章の長さが異なるためパディングによる悪影響が回避できるかどうかが疑わしいという問題がある\n","  - これは、課題として比較してみるとよいであろう\n","- どこかしらの1つの特徴量を利用する\n","  - こうなるとどこか、ということであるが、先頭単語の特徴量を利用するという方針を選択している\n","  - これは、先頭単語に分類に必要な特徴量が存在するというわけではない\n","  - 学習によって、「そうなるように」能力を獲得させるということである\n","  - これでもうまくいくのだから、DNNはそれなりにミスがあっても、見当違いがあっても、おおらかに、かつ甘んじてそれを受け入れ、その制約の中で頑張って学習する健気な存在である"],"metadata":{"id":"MScWOg5K0RbN"}},{"cell_type":"code","metadata":{"id":"pcrNM1rpJFRn"},"source":["class ClassificationHead(nn.Module):\n","    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n","\n","    def __init__(self, d_model=300, output_dim=2):\n","        super().__init__()\n","\n","        # 全結合層\n","        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n","\n","        # 重み初期化処理\n","        nn.init.normal_(self.linear.weight, std=0.02)\n","        nn.init.normal_(self.linear.bias, 0)\n","\n","    def forward(self, x):\n","        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n","        out = self.linear(x0)\n","\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformerの動作確認"],"metadata":{"id":"CL_p27fp2R2H"}},{"cell_type":"code","metadata":{"id":"WUVS-wxoJFRq","executionInfo":{"status":"ok","timestamp":1692262155407,"user_tz":-540,"elapsed":408,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c0cfcc3-edc7-42fd-a29d-d38662a53be0"},"source":["batch = next(iter(train_loader))  # ミニバッチの用意\n","# モデル構築\n","net3 = TransformerBlock(d_model=300)\n","net4 = ClassificationHead(output_dim=2, d_model=300)\n","# 入出力\n","x =test_batch['input_ids'][0]\n","x1 = net1(x)  # 単語をベクトルに\n","x2 = net2(x1)  # Positon情報を足し算\n","x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n","x4 = net4(x3)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n","print(\"入力のテンソルサイズ：\", x3.shape)\n","print(\"出力のテンソルサイズ：\", x4.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256, 300])\n","出力のテンソルサイズ： torch.Size([32, 2])\n"]}]},{"cell_type":"markdown","source":["最終的なTransformerモデルのクラス"],"metadata":{"id":"xXgFUcrH2doO"}},{"cell_type":"code","metadata":{"id":"k8-NHGc3JFRt"},"source":["class TransformerClassification(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, d_model=300, max_seq_len=max_seq_len, output_dim=2):\n","        super().__init__()\n","        self.net1 = Embedder(num_embeddings, embedding_dim)\n","        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len, devname=device)\n","        self.net3_1 = TransformerBlock(d_model=d_model)\n","        self.net3_2 = TransformerBlock(d_model=d_model)\n","        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n","\n","    def forward(self, x, mask):\n","        x1 = self.net1(x)  # 単語をベクトルに\n","        x2 = self.net2(x1)  # Positon情報を足し算\n","        x3_1, normlized_weights_1 = self.net3_1(\n","            x2, mask)  # Self-Attentionで特徴量を変換\n","        x3_2, normlized_weights_2 = self.net3_2(\n","            x3_1, mask)  # Self-Attentionで特徴量を変換\n","        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n","        return x4, normlized_weights_1, normlized_weights_2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["最終的なTransformerモデルのクラスの動作確認"],"metadata":{"id":"kucURlu72kKc"}},{"cell_type":"code","metadata":{"id":"g2dt7_rvJFRv","executionInfo":{"status":"ok","timestamp":1692262166754,"user_tz":-540,"elapsed":11349,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ddf29671-8df4-4b11-948a-216b9f68040c"},"source":["model = TransformerClassification(  # モデル構築(batchは前の値を利用する)\n","\n","    num_embeddings=vocab_size, embedding_dim=300, d_model=300, max_seq_len=max_seq_len, output_dim=2).to(device)\n","\n","# 入出力\n","x = test_batch['input_ids']\n","x = x.to(device)\n","input_mask = (x != input_pad).to(device)\n","out, normlized_weights_1, normlized_weights_2 = model(x, input_mask)\n","\n","print(\"出力のテンソルサイズ：\", out.shape)\n","print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["出力のテンソルサイズ： torch.Size([32, 2])\n","出力テンソルのsigmoid： tensor([[1.0000e+00, 1.3675e-06],\n","        [1.0000e+00, 1.7600e-06],\n","        [1.0000e+00, 1.7467e-06],\n","        [1.0000e+00, 1.5209e-06],\n","        [1.0000e+00, 1.4390e-06],\n","        [1.0000e+00, 1.3932e-06],\n","        [1.0000e+00, 1.5905e-06],\n","        [1.0000e+00, 1.8700e-06],\n","        [1.0000e+00, 1.7951e-06],\n","        [1.0000e+00, 1.7382e-06],\n","        [1.0000e+00, 1.7375e-06],\n","        [1.0000e+00, 1.7609e-06],\n","        [1.0000e+00, 1.5063e-06],\n","        [1.0000e+00, 1.8945e-06],\n","        [1.0000e+00, 1.7396e-06],\n","        [1.0000e+00, 1.3070e-06],\n","        [1.0000e+00, 1.4250e-06],\n","        [1.0000e+00, 1.7798e-06],\n","        [1.0000e+00, 1.4900e-06],\n","        [1.0000e+00, 1.5721e-06],\n","        [1.0000e+00, 1.7312e-06],\n","        [1.0000e+00, 1.7077e-06],\n","        [1.0000e+00, 1.7074e-06],\n","        [1.0000e+00, 1.6516e-06],\n","        [1.0000e+00, 1.7173e-06],\n","        [1.0000e+00, 1.6300e-06],\n","        [1.0000e+00, 1.5699e-06],\n","        [1.0000e+00, 1.5289e-06],\n","        [1.0000e+00, 1.5863e-06],\n","        [1.0000e+00, 1.4512e-06],\n","        [1.0000e+00, 1.5553e-06],\n","        [1.0000e+00, 1.6155e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xDdsFS7CXmXk"},"source":["### DatasetとDataLoaderの実装"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgmPvIq6XmXk"},"outputs":[],"source":["# 辞書オブジェクトにまとめる\n","dataloaders_dict = {\"train\": train_loader, \"val\": test_loader}"]},{"cell_type":"markdown","source":["ネットワークの初期化として、 He の方法 (一様分布)を用いる\n","- 初期化については後で概要についてまとめる"],"metadata":{"id":"ziBFndlzLAQX"}},{"cell_type":"code","source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Linear') != -1:\n","        # Liner層の初期化\n","        nn.init.kaiming_normal_(m.weight)\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias, 0.0)\n","# 訓練モードに設定\n","model.train()\n","# TransformerBlockモジュールを初期化実行\n","model.net3_1.apply(weights_init)\n","model.net3_2.apply(weights_init)\n","model.to(device)  # モデルをGPUへ"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSvPRAloQTFR","executionInfo":{"status":"ok","timestamp":1692262166754,"user_tz":-540,"elapsed":13,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"6969cd48-a4aa-4e1b-aae3-6008755ff5de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TransformerClassification(\n","  (net1): Embedder(\n","    (embeddings): Embedding(30522, 300)\n","  )\n","  (net2): PositionalEncoder()\n","  (net3_1): TransformerBlock(\n","    (norm_1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (norm_2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (attn): Attention(\n","      (q_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (v_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (k_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (out): Linear(in_features=300, out_features=300, bias=True)\n","    )\n","    (ff): FeedForward(\n","      (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n","    )\n","    (dropout_1): Dropout(p=0.1, inplace=False)\n","    (dropout_2): Dropout(p=0.1, inplace=False)\n","  )\n","  (net3_2): TransformerBlock(\n","    (norm_1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (norm_2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (attn): Attention(\n","      (q_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (v_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (k_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (out): Linear(in_features=300, out_features=300, bias=True)\n","    )\n","    (ff): FeedForward(\n","      (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n","    )\n","    (dropout_1): Dropout(p=0.1, inplace=False)\n","    (dropout_2): Dropout(p=0.1, inplace=False)\n","  )\n","  (net4): ClassificationHead(\n","    (linear): Linear(in_features=300, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"5hd6Ov0xXmXm"},"source":["### 損失関数と最適化手法を定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyeBmPrMXmXm"},"outputs":[],"source":["# 損失関数の設定\n","criterion = nn.CrossEntropyLoss()\n","# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n","\n","# 最適化手法の設定\n","learning_rate = 2e-5\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"yqIzemptXmXm"},"source":["### 学習・検証"]},{"cell_type":"markdown","source":["モデルを学習させる関数を作成"],"metadata":{"id":"0bSozxc_CTso"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq6aBPOTX9cB"},"outputs":[],"source":["def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","    torch.backends.cudnn.benchmark = True  # ネットワークがある程度固定であれば高速化る\n","    for epoch in range(num_epochs):  # epochのループ\n","        for phase in ['train', 'val']:  # epochごとの訓練と検証のループ\n","            if phase == 'train':\n","                model.train()  # モデルを訓練モードに\n","            else:\n","                model.eval()  # モデルを検証モードに\n","            epoch_loss = 0.0  # epochの損失和\n","            epoch_corrects = 0  # epochの正解数\n","            for batch in (dataloaders_dict[phase]):  # データローダーからミニバッチを取り出す\n","                inputs = batch['input_ids'].to(device)  # 文章を可能ならばGPUへ\n","                labels = batch['label'].to(device)  # ラベルを可能ならばGPUへ\n","                optimizer.zero_grad()  # optimizerを初期化\n","                with torch.set_grad_enabled(phase == 'train'):  # 順伝搬の計算\n","                    # mask作成\n","                    input_mask = (inputs != input_pad)\n","                    input_mask = input_mask.to(device)\n","                    outputs, _, _ = model(inputs, input_mask)  # Transformerに入力\n","                    loss = criterion(outputs, labels)  # 損失を計算\n","                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n","                    if phase == 'train':  # 訓練時のみ勾配計算と更新\n","                        loss.backward()\n","                        optimizer.step()\n","                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n","                    epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n","            # epochごとのlossと正解率\n","            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double(\n","            ) / len(dataloaders_dict[phase].dataset)\n","            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n","                phase, epoch_loss, epoch_acc))\n","    return net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80KEK8XJXmXn","outputId":"07bdc280-6538-4ac7-b6ce-36de405617c4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692263245714,"user_tz":-540,"elapsed":1078970,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 | train |  Loss: 0.7789 Acc: 0.5697\n","Epoch 1/10 |  val  |  Loss: 0.6304 Acc: 0.6430\n","Epoch 2/10 | train |  Loss: 0.6217 Acc: 0.6538\n","Epoch 2/10 |  val  |  Loss: 0.5998 Acc: 0.6763\n","Epoch 3/10 | train |  Loss: 0.5895 Acc: 0.6848\n","Epoch 3/10 |  val  |  Loss: 0.5795 Acc: 0.6913\n","Epoch 4/10 | train |  Loss: 0.5417 Acc: 0.7272\n","Epoch 4/10 |  val  |  Loss: 0.5277 Acc: 0.7352\n","Epoch 5/10 | train |  Loss: 0.5042 Acc: 0.7528\n","Epoch 5/10 |  val  |  Loss: 0.5229 Acc: 0.7376\n","Epoch 6/10 | train |  Loss: 0.4827 Acc: 0.7654\n","Epoch 6/10 |  val  |  Loss: 0.4888 Acc: 0.7639\n","Epoch 7/10 | train |  Loss: 0.4645 Acc: 0.7780\n","Epoch 7/10 |  val  |  Loss: 0.4879 Acc: 0.7635\n","Epoch 8/10 | train |  Loss: 0.4506 Acc: 0.7881\n","Epoch 8/10 |  val  |  Loss: 0.4739 Acc: 0.7726\n","Epoch 9/10 | train |  Loss: 0.4390 Acc: 0.7957\n","Epoch 9/10 |  val  |  Loss: 0.4681 Acc: 0.7755\n","Epoch 10/10 | train |  Loss: 0.4223 Acc: 0.8040\n","Epoch 10/10 |  val  |  Loss: 0.4569 Acc: 0.7845\n"]}],"source":["# 学習・検証を実行する 15分ほどかかります\n","num_epochs = 10\n","net_trained = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"vpzS-7IqXmXn"},"source":["正解率の計算と表示"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xXiZX_UXmXn"},"outputs":[],"source":["net_trained.eval()   # モデルを検証モードに\n","net_trained.to(device)\n","epoch_corrects = 0  # epochの正解数\n","for batch in (test_loader):  # testデータのDataLoader\n","    inputs = batch['input_ids'].to(device)\n","    labels = batch['label'].to(device)\n","    with torch.set_grad_enabled(False):  # 順伝搬のみ計算\n","        input_mask = (inputs != input_pad)  # mask作成\n","        outputs, _, _ = net_trained(inputs, input_mask)  # Transformerに入力\n","        _, preds = torch.max(outputs, 1)  # ラベルを予測\n","        epoch_corrects += torch.sum(preds == labels.data)  # 結果の計算"]},{"cell_type":"markdown","source":["正解率の出力"],"metadata":{"id":"iNkXv2uL50cq"}},{"cell_type":"code","source":["\n","epoch_acc = epoch_corrects.double() / len(test_loader.dataset)\n","print('テストデータ{}個での正解率：{:.4f}'.format(len(test_loader.dataset),epoch_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEWYEqR-vUtF","executionInfo":{"status":"ok","timestamp":1692263278612,"user_tz":-540,"elapsed":12,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"994ecbc6-e26a-41aa-b9bb-632874a28cfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["テストデータ25000個での正解率：0.7845\n"]}]},{"cell_type":"markdown","metadata":{"id":"ezyhNu5_XmXn"},"source":["## Attentionの可視化と判定根拠の判断\n","\n"]},{"cell_type":"markdown","source":["### HTML作成関数の実装\n","\n","なぜレピュー文章の内容をポジテイプもしくはネガティブとモデルが判定したのか、判定する際に強くAttentionをかけた単語を可視化することで、その判定根拠を探る\n","- XAI(Explainable Artificial Intelligence :説明可能AI)の議論への対応として、説明性を持たせる判定根拠の可視化は重要\n","- 自然言語処理における判定根拠を明確に示す手法は確立していない\n","- Attentionが判定根拠になるかどうかは議論となっている\n","\n","文章の各単語についてAttentionの影響が強い単語の背景(HTMLのbackground-colorスタイル)ほどより赤くハイライトする\n","- Jupyter NotebookはHTML表示に対応するため、HTMLデータとして作成して表示する\n","- 文章の1単語目に埋め込まれているである<cls>の特徴量が分類の判断材料であるため、この特徴量を作成するために利用したSelf-Attentionをnormlized_weights から取り出して仕様する\n","  - TransformerBlockモジュールが2つあるため、1つ目と2つ目のttention\n","が存在する\n","\n","関数は次の2つ\n","- highlight\n","  Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\n","- mk_html\n","  実際にHTMLデータを作成する\n"],"metadata":{"id":"kRk0d28JHpfQ"}},{"cell_type":"code","source":["def highlight(word, attn):\n","    if (word == input_pad or word == 0):\n","        return ''\n","    wordc = tokenizer.convert_ids_to_tokens([word])[0]\n","    html_color = '#%02X%02X%02X' % (\n","        255, int(255*(1 - attn)), int(255*(1 - attn)))\n","    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, wordc)\n","\n","def mk_html(index, batch, preds, normlized_weights_1, normlized_weights_2):\n","    # indexの結果を抽出\n","    sentence = batch['input_ids'][index]  # 文章\n","    label = batch['label'][index]  # ラベル\n","    pred = preds[index]  # 予測\n","    # indexのAttentionを抽出と規格化\n","    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n","    attens1 /= attens1.max()\n","    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n","    attens2 /= attens2.max()\n","    # ラベルと予測結果を文字に置き換え\n","    if label == 0:\n","        label_str = \"Negative\"\n","    else:\n","        label_str = \"Positive\"\n","    if pred == 0:\n","        pred_str = \"Negative\"\n","    else:\n","        pred_str = \"Positive\"\n","    # 表示用のHTMLを作成する\n","    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n","    # 1段目のAttention\n","    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n","    for word, attn in zip(sentence, attens1):\n","        html += highlight(word, attn)\n","    html += \"<br><br>\"\n","    # 2段目のAttention\n","    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n","    for word, attn in zip(sentence, attens2):\n","        html += highlight(word, attn)\n","    html += \"<br><br>\"\n","    return html"],"metadata":{"id":"F0_auT52jZRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["実行するたびに異なる文章を評価できる\n","\n","- Positiveな文章ではPositiveな単語が、Negativeでは逆の単語に注目していることがわかる\n","- 結果を見てどのような文章で誤解しているのかなどを解析し、さらなる工夫を施すことが考えられる"],"metadata":{"id":"IVm70SXWqfpp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5yH0WycXmXo","outputId":"da1509d2-f133-440f-bdda-3be611b1a237","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"ok","timestamp":1692263278613,"user_tz":-540,"elapsed":10,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["正解ラベル：Positive<br>推論ラベル：Positive<br><br>[TransformerBlockの1段目のAttentionを可視化]<br><span style=\"background-color: #FFFCFC\"> hello</span><span style=\"background-color: #FFFEFE\"> everyone</span><span style=\"background-color: #FFFCFC\"> ,</span><span style=\"background-color: #FFFCFC\"> all</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFCFC\"> say</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFFAFA\"> human</span><span style=\"background-color: #FFFEFE\"> traffic</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFCFC\"> all</span><span style=\"background-color: #FFFCFC\"> of</span><span style=\"background-color: #FFFBFB\"> its</span><span style=\"background-color: #FFFEFE\"> characters</span><span style=\"background-color: #FFFEFE\"> are</span><span style=\"background-color: #FFF7F7\"> so</span><span style=\"background-color: #FFFEFE\"> real</span><span style=\"background-color: #FFFBFB\"> its</span><span style=\"background-color: #FFFCFC\"> funny</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFE3E3\"> live</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFEFE\"> australia</span><span style=\"background-color: #FFFAFA\"> (</span><span style=\"background-color: #FFFCFC\"> melbourne</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFFDFD\"> '</span><span style=\"background-color: #FFFEFE\"> m</span><span style=\"background-color: #FFF9F9\"> finally</span><span style=\"background-color: #FFFBFB\"> out</span><span style=\"background-color: #FFFCFC\"> of</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFEFE\"> club</span><span style=\"background-color: #FFFEFE\"> ##bing</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFEFE\"> staying</span><span style=\"background-color: #FFFBFB\"> out</span><span style=\"background-color: #FFFCFC\"> all</span><span style=\"background-color: #FFDFDF\"> weekend</span><span style=\"background-color: #FFFEFE\"> lifestyle</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFF8F8\"> explains</span><span style=\"background-color: #FFFDFD\"> everything</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFF5F5\"> currently</span><span style=\"background-color: #FFF6F6\"> going</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFEEEE\"> world</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF6F6\"> so</span><span style=\"background-color: #FFF5F5\"> exactly</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFF0F0\"> i</span><span style=\"background-color: #FFF1F1\"> can</span><span style=\"background-color: #FFC8C8\"> ##t</span><span style=\"background-color: #FFFDFD\"> stop</span><span style=\"background-color: #FFFEFE\"> watching</span><span style=\"background-color: #FFCBCB\"> it</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF0F0\"> i</span><span style=\"background-color: #FFFEFE\"> used</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFBFB\"> be</span><span style=\"background-color: #FFF6F6\"> exactly</span><span style=\"background-color: #FFF6F6\"> like</span><span style=\"background-color: #FFFCFC\"> mo</span><span style=\"background-color: #FFF9F9\"> ##ff</span><span style=\"background-color: #FFFCFC\"> ,</span><span style=\"background-color: #FFF6F6\"> so</span><span style=\"background-color: #FFFBFB\"> my</span><span style=\"background-color: #FFFDFD\"> friends</span><span style=\"background-color: #FFFEFE\"> said</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFF0F0\"> i</span><span style=\"background-color: #FFFAFA\"> hadn</span><span style=\"background-color: #FFFDFD\"> '</span><span style=\"background-color: #FFA7A7\"> t</span><span style=\"background-color: #FFFBFB\"> even</span><span style=\"background-color: #FFC6C6\"> seen</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFF8F8\"> left</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFE2E2\"> weekend</span><span style=\"background-color: #FFFEFE\"> party</span><span style=\"background-color: #FFFBFB\"> ##ing</span><span style=\"background-color: #FFFCFC\"> behind</span><span style=\"background-color: #FFFAFA\"> about</span><span style=\"background-color: #FFFAFA\"> 3</span><span style=\"background-color: #FFE7E7\"> months</span><span style=\"background-color: #FFEAEA\"> ago</span><span style=\"background-color: #FFD1D1\"> after</span><span style=\"background-color: #FFFEFE\"> 4</span><span style=\"background-color: #FF0000\"> years</span><span style=\"background-color: #FFFCFC\"> of</span><span style=\"background-color: #FFFCFC\"> intense</span><span style=\"background-color: #FFFEFE\"> party</span><span style=\"background-color: #FFFBFB\"> ##ing</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FF9595\"> change</span><span style=\"background-color: #FFFCFC\"> my</span><span style=\"background-color: #FF4E4E\"> life</span><span style=\"background-color: #FF8080\"> around</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFF3F3\"> was</span><span style=\"background-color: #FFFEFE\"> at</span><span style=\"background-color: #FFEAEA\"> a</span><span style=\"background-color: #FFA8A8\"> dvd</span><span style=\"background-color: #FFFEFE\"> store</span><span style=\"background-color: #FFF1F1\"> when</span><span style=\"background-color: #FFF2F2\"> i</span><span style=\"background-color: #FFFDFD\"> saw</span><span style=\"background-color: #FFFAFA\"> human</span><span style=\"background-color: #FFFEFE\"> traffic</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFD1D1\"> remember</span><span style=\"background-color: #FFFCFC\"> my</span><span style=\"background-color: #FFE7E7\"> old</span><span style=\"background-color: #FFFDFD\"> friends</span><span style=\"background-color: #FFF7F7\"> going</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFAFA\"> an</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFAFA\"> about</span><span style=\"background-color: #FFCCCC\"> it</span><span style=\"background-color: #FFFCFC\"> ,</span><span style=\"background-color: #FFF6F6\"> so</span><span style=\"background-color: #FFF0F0\"> i</span><span style=\"background-color: #FFFEFE\"> bought</span><span style=\"background-color: #FFCDCD\"> it</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFEFE\"> see</span><span style=\"background-color: #FFFEFE\"> what</span><span style=\"background-color: #FFFCFC\"> all</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFEFE\"> fuss</span><span style=\"background-color: #FFF4F4\"> was</span><span style=\"background-color: #FFFBFB\"> about</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFF3F3\"> was</span><span style=\"background-color: #FFF7F7\"> so</span><span style=\"background-color: #FFFEFE\"> into</span><span style=\"background-color: #FFCECE\"> it</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFEDED\"> watched</span><span style=\"background-color: #FFCCCC\"> it</span><span style=\"background-color: #FFFEFE\"> 4</span><span style=\"background-color: #FFD2D2\"> times</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFE9E9\"> a</span><span style=\"background-color: #FFF6F6\"> row</span><span style=\"background-color: #FFE5E5\"> because</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFECEC\"> couldn</span><span style=\"background-color: #FFFDFD\"> '</span><span style=\"background-color: #FFA6A6\"> t</span><span style=\"background-color: #FFEBEB\"> believe</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFEAEA\"> someone</span><span style=\"background-color: #FFE0E0\"> had</span><span style=\"background-color: #FFFEFE\"> made</span><span style=\"background-color: #FFE9E9\"> a</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFF7F7\"> explains</span><span style=\"background-color: #FFFCFC\"> everything</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFE8E8\"> a</span><span style=\"background-color: #FFA8A8\"> t</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> anyway</span><span style=\"background-color: #FFECEC\"> ##s</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFD4D4\"> by</span><span style=\"background-color: #FFFEFE\"> far</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFEFE\"> best</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFEFE\"> fun</span><span style=\"background-color: #FFFEFE\"> ##nies</span><span style=\"background-color: #FFC9C9\"> ##t</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFF0F0\"> i</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFBBBB\"> ever</span><span style=\"background-color: #FFC6C6\"> seen</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFAFA\"> its</span><span style=\"background-color: #FFFBFB\"> funny</span><span style=\"background-color: #FFE3E3\"> because</span><span style=\"background-color: #FFFAFA\"> its</span><span style=\"background-color: #FFF6F6\"> so</span><span style=\"background-color: #FF4949\"> truth</span><span style=\"background-color: #FFFEFE\"> ##ful</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFDFD\"> everything</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFFCFC\"> goes</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFCFC\"> mo</span><span style=\"background-color: #FFF9F9\"> ##ff</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFE9E9\"> a</span><span style=\"background-color: #FFFBFB\"> legend</span><span style=\"background-color: #FFFEFE\"> !</span><span style=\"background-color: #FFFEFE\"> !</span><span style=\"background-color: #FFFEFE\"> !</span><span style=\"background-color: #FFFCFC\"> that</span><span style=\"background-color: #FFECEC\"> ##s</span><span style=\"background-color: #FFFCFC\"> all</span><span style=\"background-color: #FFF1F1\"> i</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFCFC\"> say</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFBFB\"> :</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFFEFE\"> enjoy</span><span style=\"background-color: #FFFCFC\"> 11</span><span style=\"background-color: #FFFEFE\"> /</span><span style=\"background-color: #FFFEFE\"> 10</span><span style=\"background-color: #FFFEFE\"> blew</span><span style=\"background-color: #FFFDFD\"> [SEP]</span><br><br>[TransformerBlockの2段目のAttentionを可視化]<br><span style=\"background-color: #FF0000\"> hello</span><span style=\"background-color: #FFFDFD\"> everyone</span><span style=\"background-color: #FFF6F6\"> ,</span><span style=\"background-color: #FFFEFE\"> all</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFDFD\"> have</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> say</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FFFEFE\"> human</span><span style=\"background-color: #FFF2F2\"> traffic</span><span style=\"background-color: #FFF5F5\"> and</span><span style=\"background-color: #FFFEFE\"> all</span><span style=\"background-color: #FFF7F7\"> of</span><span style=\"background-color: #FFFEFE\"> its</span><span style=\"background-color: #FFFEFE\"> characters</span><span style=\"background-color: #FFF9F9\"> are</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFDFD\"> real</span><span style=\"background-color: #FFFEFE\"> its</span><span style=\"background-color: #FFFDFD\"> funny</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> live</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFEFE\"> australia</span><span style=\"background-color: #FFF7F7\"> (</span><span style=\"background-color: #FFFEFE\"> melbourne</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFF5F5\"> and</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFFEFE\"> m</span><span style=\"background-color: #FFFEFE\"> finally</span><span style=\"background-color: #FFFEFE\"> out</span><span style=\"background-color: #FFF7F7\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> club</span><span style=\"background-color: #FFFDFD\"> ##bing</span><span style=\"background-color: #FFF5F5\"> and</span><span style=\"background-color: #FFCDCD\"> staying</span><span style=\"background-color: #FFFEFE\"> out</span><span style=\"background-color: #FFFEFE\"> all</span><span style=\"background-color: #FFFDFD\"> weekend</span><span style=\"background-color: #FFFEFE\"> lifestyle</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> this</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFDFD\"> explains</span><span style=\"background-color: #FFFEFE\"> everything</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> currently</span><span style=\"background-color: #FFFDFD\"> going</span><span style=\"background-color: #FFFDFD\"> on</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> world</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFEFE\"> exactly</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFCFC\"> can</span><span style=\"background-color: #FFFEFE\"> ##t</span><span style=\"background-color: #FFFEFE\"> stop</span><span style=\"background-color: #FFFEFE\"> watching</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> used</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> be</span><span style=\"background-color: #FFFEFE\"> exactly</span><span style=\"background-color: #FFFDFD\"> like</span><span style=\"background-color: #FFFEFE\"> mo</span><span style=\"background-color: #FFFDFD\"> ##ff</span><span style=\"background-color: #FFF6F6\"> ,</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFEFE\"> my</span><span style=\"background-color: #FFFEFE\"> friends</span><span style=\"background-color: #FFFEFE\"> said</span><span style=\"background-color: #FFF5F5\"> and</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> hadn</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFFEFE\"> t</span><span style=\"background-color: #FFEBEB\"> even</span><span style=\"background-color: #FFFEFE\"> seen</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> left</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> weekend</span><span style=\"background-color: #FFBCBC\"> party</span><span style=\"background-color: #FFFEFE\"> ##ing</span><span style=\"background-color: #FFFEFE\"> behind</span><span style=\"background-color: #FFFAFA\"> about</span><span style=\"background-color: #FFFEFE\"> 3</span><span style=\"background-color: #FFF0F0\"> months</span><span style=\"background-color: #FFFEFE\"> ago</span><span style=\"background-color: #FFFEFE\"> after</span><span style=\"background-color: #FFFEFE\"> 4</span><span style=\"background-color: #FFFEFE\"> years</span><span style=\"background-color: #FFF7F7\"> of</span><span style=\"background-color: #FFFCFC\"> intense</span><span style=\"background-color: #FFBBBB\"> party</span><span style=\"background-color: #FFFEFE\"> ##ing</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> change</span><span style=\"background-color: #FFFEFE\"> my</span><span style=\"background-color: #FFF5F5\"> life</span><span style=\"background-color: #FFFEFE\"> around</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFF9F9\"> was</span><span style=\"background-color: #FFFDFD\"> at</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFDFD\"> dvd</span><span style=\"background-color: #FFFEFE\"> store</span><span style=\"background-color: #FFFEFE\"> when</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> saw</span><span style=\"background-color: #FFFEFE\"> human</span><span style=\"background-color: #FFF1F1\"> traffic</span><span style=\"background-color: #FFF4F4\"> and</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFF0F0\"> remember</span><span style=\"background-color: #FFFEFE\"> my</span><span style=\"background-color: #FFF5F5\"> old</span><span style=\"background-color: #FFFEFE\"> friends</span><span style=\"background-color: #FFFDFD\"> going</span><span style=\"background-color: #FFFDFD\"> on</span><span style=\"background-color: #FFF7F7\"> an</span><span style=\"background-color: #FFFDFD\"> on</span><span style=\"background-color: #FFF9F9\"> about</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFF5F5\"> ,</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFEFEF\"> bought</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> see</span><span style=\"background-color: #FFFEFE\"> what</span><span style=\"background-color: #FFFEFE\"> all</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> fuss</span><span style=\"background-color: #FFF9F9\"> was</span><span style=\"background-color: #FFF9F9\"> about</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFF9F9\"> was</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFF8F8\"> into</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> watched</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFEFE\"> 4</span><span style=\"background-color: #FFFEFE\"> times</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFBFB\"> row</span><span style=\"background-color: #FFFEFE\"> because</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFEFE\"> couldn</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFFEFE\"> t</span><span style=\"background-color: #FFF9F9\"> believe</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FF9999\"> someone</span><span style=\"background-color: #FFFEFE\"> had</span><span style=\"background-color: #FFFEFE\"> made</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FFFDFD\"> explains</span><span style=\"background-color: #FFFEFE\"> everything</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> t</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> anyway</span><span style=\"background-color: #FFFEFE\"> ##s</span><span style=\"background-color: #FFFEFE\"> this</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFDFD\"> by</span><span style=\"background-color: #FFF8F8\"> far</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF4F4\"> best</span><span style=\"background-color: #FFF5F5\"> and</span><span style=\"background-color: #FFFEFE\"> fun</span><span style=\"background-color: #FF0808\"> ##nies</span><span style=\"background-color: #FFFEFE\"> ##t</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFDFD\"> have</span><span style=\"background-color: #FFFBFB\"> ever</span><span style=\"background-color: #FFFEFE\"> seen</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> its</span><span style=\"background-color: #FFFDFD\"> funny</span><span style=\"background-color: #FFFEFE\"> because</span><span style=\"background-color: #FFFEFE\"> its</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFEFE\"> truth</span><span style=\"background-color: #FFFEFE\"> ##ful</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFEFE\"> everything</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FFDFDF\"> goes</span><span style=\"background-color: #FFFDFD\"> on</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF5F5\"> and</span><span style=\"background-color: #FFFEFE\"> mo</span><span style=\"background-color: #FFFDFD\"> ##ff</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> legend</span><span style=\"background-color: #FFFEFE\"> !</span><span style=\"background-color: #FFFEFE\"> !</span><span style=\"background-color: #FFFEFE\"> !</span><span style=\"background-color: #FFFEFE\"> that</span><span style=\"background-color: #FFFEFE\"> ##s</span><span style=\"background-color: #FFFEFE\"> all</span><span style=\"background-color: #FFFDFD\"> i</span><span style=\"background-color: #FFFDFD\"> have</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> say</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> :</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFFEFE\"> enjoy</span><span style=\"background-color: #FFFEFE\"> 11</span><span style=\"background-color: #FFFEFE\"> /</span><span style=\"background-color: #FFFEFE\"> 10</span><span style=\"background-color: #FFFEFE\"> blew</span><span style=\"background-color: #FFFEFE\"> [SEP]</span><br><br>"]},"metadata":{},"execution_count":33}],"source":["from IPython.display import HTML\n","\n","# Transformerで処理\n","\n","# ミニバッチの用意\n","batch = next(iter(test_loader))\n","\n","# GPUが使えるならGPUにデータを送る\n","inputs = batch['input_ids'].to(device)  # 文章\n","labels = batch['label'].to(device)  # ラベル\n","\n","# mask作成\n","input_mask = (inputs != input_pad)\n","\n","# Transformerに入力\n","outputs, normlized_weights_1, normlized_weights_2 = net_trained(\n","    inputs, input_mask)\n","_, preds = torch.max(outputs, 1)  # ラベルを予測\n","\n","\n","#index = 3  # 出力させたいデータ\n","index = random.randint(0, batch_size-1)\n","html_output = mk_html(index, batch, preds, normlized_weights_1,\n","                      normlized_weights_2)  # HTML作成\n","HTML(html_output)  # HTML形式で出力\n"]},{"cell_type":"markdown","source":["# 課題(Transformer)\n","\n","- 説明文中で言及した次の2つの課題について実際に試しなさい\n","  - Embeddingの値を学習させた場合とさせない場合の結果の違い\n","  - ClassificationHeadにおける特徴量の扱い方における結果の違い\n","    - この場合ハイライティングは言及しなくてよい\n","\n","- 日本語で実験してみよう\n","  - https://github.com/amazon-research/amazon-multilingual-counterfactual-dataset\n","  - こちらのデータセット利用してトライする\n","    - ただ、中身を見るとわかるが、ちょっとつまらないかも\n","\n","- Embeddingによる内部の単語表現ベクトルの次元を変えたとき、精度にどのように影響するかを調査しなさい\n","  - できれば減らせるようにしよう\n","\n","- Transformerの段数を増やし、精度が向上するか確認してみよう\n","\n","- LightGBMと精度を比較してみなさい\n","  - 落胆する結果にならないとよいですが…"],"metadata":{"id":"JniR0P2CQuIz"}},{"cell_type":"markdown","metadata":{"id":"Zd4rO-jl1Xe4"},"source":["# Transformer\n","- 2017年に導入されたディープラーニングモデルの一種\n","  - 主に自然言語処理で利用されている\n","- RNNと同様自然言語などの時系列データ処理向けに設計されているが、再帰や畳み込みは利用していない\n","- Attention層のみで構築されている(後述)\n","- 翻訳やテキスト要約などの各種タスクに利用可能\n","- 並列化が容易で訓練時間を削減できる\n","- 「Attention is All You Need」という論文で著名になった\n","- 機械翻訳タスクにおいてRNNを用いたモデルよりも精度がよく、訓練コストが小さいことから革命的であり、NLPではRNNに印籠を渡し現在の主流の方法である\n","  - この後登場するBERT、ELECTRA、GPTなどすべてTransformerを基本としている\n","\n","RNNとは全く異なるアプローチ\n","- EncoderおよびDecoderのいずれにもRNNのような再帰構造をもたず、Attentionが利用されている\n","- その優れた特徴から、自然言語処理以外の分野でも利用が進む\n","- PyTorchでは既に公式実装が存在しておりそちらを利用するべきであるが、ここでは構造を理解するため全て記述する"]},{"cell_type":"markdown","metadata":{"id":"_ku4J5yu4iy0"},"source":["### Transformerの構造\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer.png\" width=500>\n","\n","Seq2Seq同様EncoderとDecoderで構成\n","\n","### Encoderの構造\n","1. Embedding層により入力文章をベクトルに圧縮、つまり分散表現に変換する\n","1. Positional Encoder層で文章内のどこにあるかという位置情報を加える\n","1. Multi-Head Attention層(後述)\n","1. normalization(正規化)によりデータの偏りを削減する\n","  - batch normalizationではなくlayer normalizationが行なわれる\n","1. Feed Forward層と組み合わせて処理され、実際のモデルでは例えば6回繰り返される\n","  - 出力されたベクトルはDecoderに渡される\n","  - 特にPositionwise fully connected feed-forward networkと呼ばれる\n","\n","- Multi-Head Attention層とFeed Forward層の組み合わせが6回繰り返される\n","\n","以上で、Encoderが構成される\n","\n","### Decoderの構造\n","\n","1. Embedding層により入力文章をベクトルに圧縮(分散表現を獲得)\n","1. Positional Encoder層で位置情報を追加\n","1. Masked Multi-Head Attention層、先ほどと同様であるがAttention内のsoftmax関数を通す直前の値にマスキングが適用されている\n","  - 特定のkeyに対して、Attention weightを0にすることで入力した単語の先読みによる「カンニング」を防ぐ\n","  - 入力に予測すべき結果が入らないようにする\n","1. normalization（正規化）などで先ほどと同様\n","1. Multi-Head Attention層（Encoderの出力を入力として使用）\n","1. normalization（正規化）など\n","1. Positionwise fully connected feed-forward network(先ほどと同じ)\n","1. normalization（正規化）など\n","- 例えば、以上を6回繰り返す\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZCbAQSWygRj8"},"source":["### Transformerの構成要素\n","\n","- Attention\n","  -「文章中のどの単語に注目すればよいかを表すスコア」のこと\n","  - Query、Key、Valueの3つのベクトルで求める\n","    - Query: Inputのうち「検索をかけたいもの」\n","    - Key: 検索対象とQueryの近さ、どれだけ似ているかを測る\n","    - Value: Keyに基づき、適切なValueを出力する\n","  - Self-Attention\n","    - 下図でInputとMemoryが同一のAttention\n","      - 文法の構造や、単語同士の関係性などを獲得するのに使用される\n","  - SourceTarget-Attention\n","    - 下図でInputとMemoryが異なるAttention\n","      - TransformerではDecoderで使用される\n","  - Multi-Head Attention\n","    - Attentionを複数並列して並べたもの(後述)\n","  - Masked Multi-Head Attention\n","    - Multi-Head Attentionにマスクをつけたもの\n","    - 特定の key に対してAttention weight を0にする\n","    - TransformerではDecoderで使われる\n","    - 入力した単語が先読みを防ぐために 情報をマスクで遮断する、言わば「カンニング」を防ぐ\n","  - Attentionは可視化できる\n","    - すでに示したが、attentionは可視化でき、どの単語に注目しているかを知ることができる\n","- Position-wise Fully-connected Feedforward Network\n","  - 2層からなる全結合ニューラルネットワーク\n","  - 単語の位置ごとに個別の順伝播ネットワークとなる\n","    - これにより他単語との影響関係を排除することができる\n","  - パラメータは全てのネットワークで共通\n","$FNN(x) = LeRU(xW_1+b_1)\\cdot W_2+b_2$\n","- Positional Encoding ($PE$)\n","  - 「単語の位置」の情報をベクトルに加える\n","    - 本当に加えるだけで次元を増やさない\n","  - $pos$は位置を表し、$2i$および$2i+1$はEmbeddingの何番目の次元か、$d_{model}$が次元数を示す\n","偶数番目：$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$\n","奇数番目：$ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$"]},{"cell_type":"markdown","metadata":{"id":"Qqqr2QeuiwaR"},"source":["<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/attention2.png\" width=800>\n","\n","- 丸角(緑)がベクトル(テンソル)、四角角(青)が処理を表す\n","- **InputとMemoryはそれぞれ異なる埋め込みベクトルを表し、例えば2つの異なる文章を表す**\n","- Inputについて全結合層で各単語のQueryを作成する\n","- Memoryについても同様に全結合層でKeyを作成しQueryとの内積をとって関連度合い見る\n","  - 同じ向きを向いていれば掛け算となる\n","  - 垂直である、つまり関連しなければ0\n","  - この値を関連度(logit)とする\n","- logitにSoftmaxを適用して0から1の間に調整して出力、この結果が Attention weightとなる\n","  - メモリのどの単語に注意を払うかの重みづけ\n","  - QueryとKeyの関連が大きいとAttention weightが大きくなる\n","    - 正しくMemoryの単語に注意を向けるように,keyが正しくAttentionに向けられるように学習される\n","- Memoryから全結合層を経て、Memoryの各単語に対する埋め込みベクトルであるValueを算出する\n","  - ValueとAttenthion weightとの内積を求める\n","    - Attention weightに従ってValueを選択することを意味する\n","- 最後に全結合層を挟んで出力を得る\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SUEwYu73qKFn"},"source":["### InputとMemory\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/input-memory.png\" width=600>\n","\n","各文章は分かち書きされIDで表現された後、Embeddingにより埋め込みベクトルに変換される\n","\n","### Attention Weightの算出\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/attention-weight.png\" width=600>\n","\n","QueryとKeyの内積を算出してInputとMemoryの各単語の関連度であるlogitを算出、Softmaxを用いてAttention weightとする\n","- Memoryのどの単語に注意を払うかの重み付け\n","\n","例えば、Inputのスポーツという単語に対して、Memoryの「野球」 「が」 「得意」の各単語について正しく注意を向けるように学習する\n","- ここでは野球が高い値になるようになる\n","\n","### valueとの内積\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/value-naiseki.png\" width=600>\n","\n","この内積は、value、ここでは「野球」「が」「得意」の各単語のValueとAttention weightを掛け合わせて総和を計算することになる\n","\n","最も注目するべきvalueの値が算出されているといえるが、他の単語との関連性も考慮した値となっている"]},{"cell_type":"markdown","metadata":{"id":"ooi7J-rSvL7e"},"source":["### Multi-Head Attention\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/mhattention.jpg\" width=200>\n","\n","- Attentionを並列させた構造を持つ\n","- それぞれのAttentionをHeadと呼ぶ\n","- 「Attention Is All You Need」ではMulti-Head化により性能が向上するとされている\n","  - アンサンブル学習に近い\n","  - Dropoutも毎回異なるネットワークを使っており、アンサンブルに通じるところがある"]},{"cell_type":"markdown","source":["## Positional Encoding\n","\n","Postional Encoding層は、系列データ内の各要素に、要素のデータ内における位置情報を付与する\n","- 文章の場合、Positiona Encodingによって、各単語ベクトルに、その単語が文章内で何番目に登場するかという情報を付与する\n","\n","次の式で算出した値を固定値として、入力に加算する\n","\n","$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$\n","\n","$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$\n","\n","このPositional Encodingの値を図示すると次のようになる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/pos_encoding.png\" width=400>\n"],"metadata":{"id":"imYDkjsFTgPT"}},{"cell_type":"markdown","metadata":{"id":"2UbJrC7Fq43T"},"source":["# PyTorch Transformerを用いた単語予測\n","\n","Transformerは複雑な構造をもっており、これそのものをPytorchで記述することも可能であるが、CNNやRNNと同様、PyTorchが提供するライブラリを利用することで簡単に利用できる\n","- ここではPytorch Transformerが提供するTransformerを構築するに必要な層の要素を組み合わせて設計する\n","\n","Pytorh TransformerとTorchTextを用い、先に学んだsequence-to-sequenceモデルを使って機械翻訳モデルを実装する\n","- この内容はPyTorchのチュートリアルドキュメントに準拠する\n","\n","WikiText2から取得した文章を用いて単語系列であるsequenceを入力、次に来る単語の予測を行う\n"]},{"cell_type":"markdown","metadata":{"id":"m5z7ICe5cTN8"},"source":["## モデル定義"]},{"cell_type":"markdown","metadata":{"id":"7QGqYTRpDIbs"},"source":["単語、つまりトークンの並びであるシーケンスがモデルに入力されると、位置エンコーディング層で単語の順序情報が加えられる\n","\n","言語モデルタスクでは、入力シーケンスと共に、アテンション・マスクを利用する\n","- nn.TransformerEncoderのSelf-Attention層では、シーケンスにおけるその単語以前の単語のみ知ることができる\n","  - 普通は、未来に登場するはずの単語は考慮できないため\n","- そこで、言語モデルタスクでは、後で登場するトークンは未知のトークンとして扱う必要があるため、これらをマスクする\n"]},{"cell_type":"markdown","metadata":{"id":"XX10cXx_DWd2"},"source":["nn.TransfomerEncoderについて\n","- nn.TransformerEncoderモデルの出力は、最終的に全結合層に送られlog-Softmax関数を介することで予測結果を得ることができる\n","- nn.TransformerEncoderは、複数のnn.TransformerEncoderLayer層で構成されており並列的に動作できるためRNNよりも計算効率が良い\n","\n","TransformerModelの引き数\n","- src: [seq_len, batch_size]のTensor型\n","- src_mask: [seq_len, seq_len]のTensor型\n","戻り値: [seq_len, batch_size, ntoken]のTensor型\n"]},{"cell_type":"code","metadata":{"id":"vD0wRqmlPPd6"},"source":["import math\n","from typing import Tuple\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import dataset\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n","                 nlayers: int, dropout: float = 0.5):\n","        super().__init__()\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEncoding(d_model, dropout)\n","        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, d_model)\n","        self.d_model = d_model\n","        self.decoder = nn.Linear(d_model, ntoken)\n","\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n","        src = self.encoder(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        output = self.decoder(output)\n","        return output\n","\n","def generate_square_subsequent_mask(sz: int) -> Tensor:\n","    # -infの上三角行列を生成し，対角線上に0を置く。\n","    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hX1-8iMzkrcm"},"source":["## PositionalEncoding\n","\n","PosigionalEncodingモジュールは、シーケンス内のトークンの相対的な位置、もしくは絶対的な位置に関する情報を与える\n","\n","オリジナルの実装と同様、入力と出力は同じ次元である\n","- つまり、もともとの入力$x$に対して、PositionalEncodingの値$p(x)$が得られたとすると、出力は$x+p(x)$となる"]},{"cell_type":"code","metadata":{"id":"B94dp7t9PPd9"},"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        # [seq_len, batch_size, embedding_dim]型Tensorを引数とする\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkurbIT-cbN5"},"source":["## データの読み込みとバッチ処理\n"]},{"cell_type":"markdown","metadata":{"id":"dhPLp8qLmpkS"},"source":["ここでは、torchtextのWikitext-2データセットを利用する\n","\n","vocabはトークン（単語）をテンソル形式の数値に変換する\n","- 訓練データセットを元に構築されており、データセット依存である\n","\n","- batchify()関数は、トークンが左から右に一つずつ並んだシーケンス形式のデータを束ねて、batch処理ができるようにする\n","  - 変換にはデータをbatch_size 変数のサイズで分割し、余ったトークンは廃棄する\n","\n","例えば、アルファベット26文字をシーケンスとしたとき、バッチサイズが4であれば、アルファベットを長さ6の4つのシーケンスに分割することが考えられる\n","\n","この時次のような変換が行なわれる\n","\n","\\begin{align}\\begin{bmatrix}\n","  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n","  \\end{bmatrix}\n","  \\Rightarrow\n","  \\begin{bmatrix}\n","  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n","  \\end{bmatrix}\\end{align}\n","\n","なお、各バッチ、つまり各列はモデル内ではそれぞれ独立しており、その境界を越えて依存関係を学習することはできない\n","- 例えばFとGの依存関係を学習することはできない\n","- それでも大量にデータを入力して学習させるため問題とはならない\n","- バッチ処理を有効に活用した方が計算効率が高く、その方がメリットが大きい\n"]},{"cell_type":"markdown","source":["Google Colaboratoryにはtorchdataがないので、インストール\n","\n","ランタイムを再起動させる必要があるかもしれないので注意"],"metadata":{"id":"g2bddT_ckgOJ"}},{"cell_type":"code","source":["!pip install torchdata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AO-sHgYUkay5","executionInfo":{"status":"ok","timestamp":1692263488822,"user_tz":-540,"elapsed":6835,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"2df69d57-8e20-4272-a48c-efe71ef40460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchdata) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n"]}]},{"cell_type":"markdown","source":["必要なライブラリの読み込み\n","\n"],"metadata":{"id":"840dAvHobWkf"}},{"cell_type":"code","source":["!pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXflwEXKY5Gk","executionInfo":{"status":"ok","timestamp":1692263496622,"user_tz":-540,"elapsed":7803,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"8c3c5f6a-c24f-48d6-ff1c-5cbb2cf60b23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n","Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.0.4)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"]}]},{"cell_type":"markdown","source":["`portalocker.Lock`でエラーが発生するため、torchtext.datasetsを読み込む前にportalockerをインストールする必要がある\n","- こういうノウハウ的なところはひとまず気にしなくてよい\n"],"metadata":{"id":"RgUg9xlpgrkA"}},{"cell_type":"code","source":["from torchtext.datasets import WikiText2\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator"],"metadata":{"id":"xF_hfy6Jkd82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`data_process`は、テキストをテンソル列に変換する\n","\n","`batchfy`は、データをbsz個のシーケンスに分割し，きれいに収まらない余分な要素を削除する\n","- data: Tensor, shape [N]\n","- bsz: int, batch size\n","- 戻り値: Tensor of shape [N // bsz, bsz]"],"metadata":{"id":"T0MO7YeoZi-P"}},{"cell_type":"markdown","source":["**<font color=\"red\">+++注意+++</font>**\n","\n","次のエラーが出力される場合、ランタイムを再起動して対処してください。\n","\n","最初からやり直すと時間がかかるため、「PyTorch Transformerを用いた単語予測」から以降をランタイムを再起動(Ctrl+M .[ピリオド])して、再実行(Ctrl+F10)するとよい\n","\n","\n","```\n","AttributeError: 'NoneType' object has no attribute 'Lock'\n","This exception is thrown by __iter__ of _MemoryCellIterDataPipe(remember_elements=1000, source_datapipe=_ChildDataPipe)\n","```\n","\n"],"metadata":{"id":"udO417RDdtbO"}},{"cell_type":"code","source":["!pip install portalocker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlYcnBLGwfnB","executionInfo":{"status":"ok","timestamp":1692263501792,"user_tz":-540,"elapsed":4473,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"504ab693-81ec-4c60-a6e1-a9c4da264d8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.7.0)\n"]}]},{"cell_type":"code","source":["train_iter = WikiText2(split='train')\n","tokenizer = get_tokenizer('basic_english')\n","vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])\n","\n","def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n","    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n","    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n","\n","# train_iter was \"consumed\" by the process of building the vocab,\n","# so we have to create it again\n","train_iter, val_iter, test_iter = WikiText2()\n","train_data = data_process(train_iter)\n","val_data = data_process(val_iter)\n","test_data = data_process(test_iter)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def batchify(data: Tensor, bsz: int) -> Tensor:\n","    seq_len = data.size(0) // bsz\n","    data = data[:seq_len * bsz]\n","    data = data.view(bsz, seq_len).t().contiguous()\n","    return data.to(device)\n","\n","batch_size = 20\n","eval_batch_size = 10\n","train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n","val_data = batchify(val_data, eval_batch_size)\n","test_data = batchify(test_data, eval_batch_size)"],"metadata":{"id":"ap-_QY18YqBH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lu9vP19sP9W"},"source":["## 入力シーケンスとTargetシーケンスの生成\n","\n","``get_batch()``関数を用いてTransformerモデルの入力シーケンスと、Targetシーケンスを生成する\n","- ソースデータを変数``bptt``の長さのチャンクデータに細分化する\n","- 今回の言語モデルタスクでは、入力シーケンスの続きとなる単語をTargetとして学習させる\n","\n","例えば``bptt``が2の場合は後続する2 つの要素を取得する\n","\n","- ``get_batch()``関数の返り値``data``の0次元目がチャンクの長さでを表し、Transformerモデルの次元Sと一致する\n","- dataの1次元目はバッチサイズを示す次元数Nである\n","\n","- 引数\n","  - source: [full_seq_len, batch_size]のテンソル\n","  - i: 整数\n","- 返り値\n","  - (data, target)\n","  - [seq_len, batch_size]のテンソルであるdataと、[seq_len * batch_size]のテンソルであるtarget"]},{"cell_type":"code","metadata":{"id":"vwlWJxBHPPeE"},"source":["bptt = 35\n","def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n","    seq_len = min(bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].reshape(-1)\n","    return data, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1crAe63wcYAF"},"source":["インスタンスの初期化\n","--------------------"]},{"cell_type":"markdown","metadata":{"id":"UMeu_Zh7cbH7"},"source":["モデルは、以下のハイパーパラメータを使用して設定されます。vocabのサイズは、vocabオブジェクトの長さと同じです。"]},{"cell_type":"code","metadata":{"id":"n8uZqoGWPPeI"},"source":["ntokens = len(vocab)  # 語彙サイズ\n","emsize = 200  # 埋め込み次元数\n","d_hid = 200  # forwardネットワークモデルのnn.TransformerEncoderの次元\n","nlayers = 2  # nn.TransformerEncoderのnn.TransformerEncoderLayerの数\n","nhead = 2  # nn.MultiheadAttentionにおけるheadの数\n","dropout = 0.2  # dropoutの割合\n","model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["語彙数を確認する"],"metadata":{"id":"syHb9YFHmReQ"}},{"cell_type":"code","source":["ntokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aA58Lil2mPNS","executionInfo":{"status":"ok","timestamp":1692263516109,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"ede1b893-8097-4f2a-9017-ce59da13edd5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28782"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"Xx-EHjPUf7bE"},"source":["モデルの実行\n","-------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aoF0qhd7f7nX"},"source":["- CrossEntropyLossとSDGを用いる\n","- 初期の学習率は5.0としている\n","- エポック単位で学習率を調整する\n","  - StepLRを使用して学習率を調整する\n","  - 訓練中は、勾配爆発を防ぐためnn.utils.clip_grad_normを用いて学習率を調整する"]},{"cell_type":"code","metadata":{"id":"RjC4emUHPPeL"},"source":["import copy\n","import time\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = 5.0  # learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","\n","def train(model: nn.Module) -> None:\n","    model.train()  # turn on train mode\n","    total_loss = 0.\n","    log_interval = 200\n","    start_time = time.time()\n","    src_mask = generate_square_subsequent_mask(bptt).to(device)\n","\n","    num_batches = len(train_data) // bptt\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","        data, targets = get_batch(train_data, i)\n","        seq_len = data.size(0)\n","        if seq_len != bptt:  # only on last batch\n","            src_mask = src_mask[:seq_len, :seq_len]\n","        output = model(data, src_mask)\n","        loss = criterion(output.view(-1, ntokens), targets)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        if batch % log_interval == 0 and batch > 0:\n","            lr = scheduler.get_last_lr()[0]\n","            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n","            cur_loss = total_loss / log_interval\n","            ppl = math.exp(cur_loss)\n","            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n","                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n","                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n","    model.eval()  # turn on evaluation mode\n","    total_loss = 0.\n","    src_mask = generate_square_subsequent_mask(bptt).to(device)\n","    with torch.no_grad():\n","        for i in range(0, eval_data.size(0) - 1, bptt):\n","            data, targets = get_batch(eval_data, i)\n","            seq_len = data.size(0)\n","            if seq_len != bptt:\n","                src_mask = src_mask[:seq_len, :seq_len]\n","            output = model(data, src_mask)\n","            output_flat = output.view(-1, ntokens)\n","            total_loss += seq_len * criterion(output_flat, targets).item()\n","    return total_loss / (len(eval_data) - 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgE9j1KumZTK"},"source":["エポックを繰り返す\n","\n","- 検証データの損失がそれまでの実行のなかで最も良い(低い)場合は当該モデルを保存する\n","- 各エポックの後に学習率を調整する(小さくする)"]},{"cell_type":"code","metadata":{"id":"4Ws6fSWbPPeO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1c4109c-0cd0-40fa-9bf8-d0f365ce6f50","executionInfo":{"status":"ok","timestamp":1692263973026,"user_tz":-540,"elapsed":456919,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["best_val_loss = float('inf')\n","epochs = 10\n","best_model = None\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train(model)\n","    val_loss = evaluate(model, val_data)\n","    val_ppl = math.exp(val_loss)\n","    elapsed = time.time() - epoch_start_time\n","    print('-' * 89)\n","    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n","          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n","    print('-' * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = copy.deepcopy(model)\n","\n","    scheduler.step()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 19.07 | loss  8.11 | ppl  3335.74\n","| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 15.81 | loss  6.88 | ppl   972.47\n","| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 20.63 | loss  6.44 | ppl   626.90\n","| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 18.04 | loss  6.30 | ppl   544.71\n","| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 15.14 | loss  6.19 | ppl   485.81\n","| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 16.92 | loss  6.16 | ppl   473.52\n","| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 23.76 | loss  6.11 | ppl   451.98\n","| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 18.03 | loss  6.11 | ppl   449.36\n","| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 15.56 | loss  6.03 | ppl   414.02\n","| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 14.10 | loss  6.02 | ppl   410.74\n","| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 14.88 | loss  5.89 | ppl   362.14\n","| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 14.85 | loss  5.97 | ppl   390.64\n","| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 14.31 | loss  5.96 | ppl   386.12\n","| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 14.40 | loss  5.87 | ppl   355.91\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 50.93s | valid loss  5.76 | valid ppl   316.18\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 15.28 | loss  5.87 | ppl   354.30\n","| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 14.89 | loss  5.85 | ppl   348.40\n","| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 14.49 | loss  5.67 | ppl   288.74\n","| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 14.59 | loss  5.70 | ppl   299.20\n","| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 14.72 | loss  5.65 | ppl   283.10\n","| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 15.44 | loss  5.67 | ppl   291.33\n","| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 15.03 | loss  5.69 | ppl   295.42\n","| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 14.54 | loss  5.71 | ppl   300.89\n","| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 14.46 | loss  5.65 | ppl   282.88\n","| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 14.53 | loss  5.66 | ppl   288.37\n","| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 15.16 | loss  5.55 | ppl   257.94\n","| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 14.90 | loss  5.64 | ppl   282.18\n","| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 14.27 | loss  5.64 | ppl   281.20\n","| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 14.29 | loss  5.58 | ppl   265.30\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time: 45.17s | valid loss  5.68 | valid ppl   291.49\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 15.18 | loss  5.60 | ppl   270.80\n","| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 14.66 | loss  5.62 | ppl   275.20\n","| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 14.27 | loss  5.42 | ppl   225.39\n","| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 14.29 | loss  5.47 | ppl   238.58\n","| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 14.30 | loss  5.44 | ppl   230.96\n","| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 15.11 | loss  5.48 | ppl   239.73\n","| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 14.84 | loss  5.49 | ppl   242.23\n","| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 14.27 | loss  5.51 | ppl   246.92\n","| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 17.52 | loss  5.45 | ppl   233.76\n","| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 14.54 | loss  5.47 | ppl   238.61\n","| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 15.36 | loss  5.35 | ppl   210.73\n","| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 14.85 | loss  5.45 | ppl   233.46\n","| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 14.47 | loss  5.47 | ppl   236.35\n","| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 14.40 | loss  5.40 | ppl   221.71\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time: 45.50s | valid loss  5.58 | valid ppl   264.34\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |   200/ 2928 batches | lr 4.29 | ms/batch 15.30 | loss  5.43 | ppl   227.35\n","| epoch   4 |   400/ 2928 batches | lr 4.29 | ms/batch 14.67 | loss  5.47 | ppl   236.28\n","| epoch   4 |   600/ 2928 batches | lr 4.29 | ms/batch 14.42 | loss  5.26 | ppl   192.66\n","| epoch   4 |   800/ 2928 batches | lr 4.29 | ms/batch 14.41 | loss  5.32 | ppl   204.84\n","| epoch   4 |  1000/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.28 | ppl   196.41\n","| epoch   4 |  1200/ 2928 batches | lr 4.29 | ms/batch 15.14 | loss  5.33 | ppl   206.16\n","| epoch   4 |  1400/ 2928 batches | lr 4.29 | ms/batch 14.72 | loss  5.35 | ppl   211.05\n","| epoch   4 |  1600/ 2928 batches | lr 4.29 | ms/batch 14.47 | loss  5.38 | ppl   216.59\n","| epoch   4 |  1800/ 2928 batches | lr 4.29 | ms/batch 14.48 | loss  5.33 | ppl   206.14\n","| epoch   4 |  2000/ 2928 batches | lr 4.29 | ms/batch 14.53 | loss  5.35 | ppl   210.67\n","| epoch   4 |  2200/ 2928 batches | lr 4.29 | ms/batch 15.14 | loss  5.21 | ppl   183.82\n","| epoch   4 |  2400/ 2928 batches | lr 4.29 | ms/batch 14.72 | loss  5.33 | ppl   206.22\n","| epoch   4 |  2600/ 2928 batches | lr 4.29 | ms/batch 14.37 | loss  5.34 | ppl   208.41\n","| epoch   4 |  2800/ 2928 batches | lr 4.29 | ms/batch 14.40 | loss  5.27 | ppl   193.95\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time: 45.05s | valid loss  5.56 | valid ppl   260.70\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |   200/ 2928 batches | lr 4.07 | ms/batch 15.30 | loss  5.31 | ppl   202.95\n","| epoch   5 |   400/ 2928 batches | lr 4.07 | ms/batch 14.50 | loss  5.33 | ppl   206.54\n","| epoch   5 |   600/ 2928 batches | lr 4.07 | ms/batch 14.42 | loss  5.14 | ppl   170.20\n","| epoch   5 |   800/ 2928 batches | lr 4.07 | ms/batch 14.47 | loss  5.20 | ppl   182.06\n","| epoch   5 |  1000/ 2928 batches | lr 4.07 | ms/batch 14.58 | loss  5.16 | ppl   174.38\n","| epoch   5 |  1200/ 2928 batches | lr 4.07 | ms/batch 15.19 | loss  5.21 | ppl   182.64\n","| epoch   5 |  1400/ 2928 batches | lr 4.07 | ms/batch 14.53 | loss  5.24 | ppl   187.83\n","| epoch   5 |  1600/ 2928 batches | lr 4.07 | ms/batch 14.28 | loss  5.26 | ppl   192.59\n","| epoch   5 |  1800/ 2928 batches | lr 4.07 | ms/batch 14.59 | loss  5.21 | ppl   183.64\n","| epoch   5 |  2000/ 2928 batches | lr 4.07 | ms/batch 15.67 | loss  5.23 | ppl   186.26\n","| epoch   5 |  2200/ 2928 batches | lr 4.07 | ms/batch 16.32 | loss  5.09 | ppl   162.01\n","| epoch   5 |  2400/ 2928 batches | lr 4.07 | ms/batch 15.02 | loss  5.20 | ppl   182.12\n","| epoch   5 |  2600/ 2928 batches | lr 4.07 | ms/batch 14.36 | loss  5.22 | ppl   185.83\n","| epoch   5 |  2800/ 2928 batches | lr 4.07 | ms/batch 14.45 | loss  5.15 | ppl   172.34\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time: 45.37s | valid loss  5.49 | valid ppl   242.39\n","-----------------------------------------------------------------------------------------\n","| epoch   6 |   200/ 2928 batches | lr 3.87 | ms/batch 15.33 | loss  5.21 | ppl   182.37\n","| epoch   6 |   400/ 2928 batches | lr 3.87 | ms/batch 14.76 | loss  5.23 | ppl   186.65\n","| epoch   6 |   600/ 2928 batches | lr 3.87 | ms/batch 14.36 | loss  5.04 | ppl   154.31\n","| epoch   6 |   800/ 2928 batches | lr 3.87 | ms/batch 14.39 | loss  5.10 | ppl   164.79\n","| epoch   6 |  1000/ 2928 batches | lr 3.87 | ms/batch 14.39 | loss  5.07 | ppl   158.48\n","| epoch   6 |  1200/ 2928 batches | lr 3.87 | ms/batch 15.21 | loss  5.11 | ppl   166.04\n","| epoch   6 |  1400/ 2928 batches | lr 3.87 | ms/batch 14.92 | loss  5.13 | ppl   169.27\n","| epoch   6 |  1600/ 2928 batches | lr 3.87 | ms/batch 14.38 | loss  5.16 | ppl   174.97\n","| epoch   6 |  1800/ 2928 batches | lr 3.87 | ms/batch 14.45 | loss  5.12 | ppl   168.12\n","| epoch   6 |  2000/ 2928 batches | lr 3.87 | ms/batch 14.33 | loss  5.13 | ppl   169.13\n","| epoch   6 |  2200/ 2928 batches | lr 3.87 | ms/batch 15.08 | loss  4.99 | ppl   147.02\n","| epoch   6 |  2400/ 2928 batches | lr 3.87 | ms/batch 15.18 | loss  5.12 | ppl   167.94\n","| epoch   6 |  2600/ 2928 batches | lr 3.87 | ms/batch 14.40 | loss  5.13 | ppl   169.82\n","| epoch   6 |  2800/ 2928 batches | lr 3.87 | ms/batch 14.36 | loss  5.06 | ppl   157.60\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time: 44.91s | valid loss  5.50 | valid ppl   244.93\n","-----------------------------------------------------------------------------------------\n","| epoch   7 |   200/ 2928 batches | lr 3.68 | ms/batch 15.27 | loss  5.12 | ppl   166.53\n","| epoch   7 |   400/ 2928 batches | lr 3.68 | ms/batch 14.86 | loss  5.14 | ppl   170.71\n","| epoch   7 |   600/ 2928 batches | lr 3.68 | ms/batch 14.43 | loss  4.94 | ppl   140.12\n","| epoch   7 |   800/ 2928 batches | lr 3.68 | ms/batch 14.42 | loss  5.02 | ppl   151.19\n","| epoch   7 |  1000/ 2928 batches | lr 3.68 | ms/batch 14.41 | loss  4.98 | ppl   145.14\n","| epoch   7 |  1200/ 2928 batches | lr 3.68 | ms/batch 15.10 | loss  5.03 | ppl   153.37\n","| epoch   7 |  1400/ 2928 batches | lr 3.68 | ms/batch 14.95 | loss  5.04 | ppl   154.84\n","| epoch   7 |  1600/ 2928 batches | lr 3.68 | ms/batch 14.40 | loss  5.08 | ppl   160.14\n","| epoch   7 |  1800/ 2928 batches | lr 3.68 | ms/batch 14.41 | loss  5.04 | ppl   154.43\n","| epoch   7 |  2000/ 2928 batches | lr 3.68 | ms/batch 14.33 | loss  5.04 | ppl   153.80\n","| epoch   7 |  2200/ 2928 batches | lr 3.68 | ms/batch 15.15 | loss  4.91 | ppl   135.80\n","| epoch   7 |  2400/ 2928 batches | lr 3.68 | ms/batch 15.03 | loss  5.03 | ppl   152.97\n","| epoch   7 |  2600/ 2928 batches | lr 3.68 | ms/batch 14.43 | loss  5.04 | ppl   154.90\n","| epoch   7 |  2800/ 2928 batches | lr 3.68 | ms/batch 14.46 | loss  4.98 | ppl   144.78\n","-----------------------------------------------------------------------------------------\n","| end of epoch   7 | time: 44.98s | valid loss  5.49 | valid ppl   242.82\n","-----------------------------------------------------------------------------------------\n","| epoch   8 |   200/ 2928 batches | lr 3.49 | ms/batch 15.30 | loss  5.03 | ppl   152.82\n","| epoch   8 |   400/ 2928 batches | lr 3.49 | ms/batch 14.75 | loss  5.06 | ppl   157.05\n","| epoch   8 |   600/ 2928 batches | lr 3.49 | ms/batch 14.44 | loss  4.87 | ppl   130.08\n","| epoch   8 |   800/ 2928 batches | lr 3.49 | ms/batch 14.41 | loss  4.94 | ppl   139.59\n","| epoch   8 |  1000/ 2928 batches | lr 3.49 | ms/batch 14.48 | loss  4.90 | ppl   133.85\n","| epoch   8 |  1200/ 2928 batches | lr 3.49 | ms/batch 15.21 | loss  4.96 | ppl   142.05\n","| epoch   8 |  1400/ 2928 batches | lr 3.49 | ms/batch 14.86 | loss  4.96 | ppl   143.26\n","| epoch   8 |  1600/ 2928 batches | lr 3.49 | ms/batch 14.44 | loss  5.00 | ppl   147.69\n","| epoch   8 |  1800/ 2928 batches | lr 3.49 | ms/batch 14.42 | loss  4.96 | ppl   143.20\n","| epoch   8 |  2000/ 2928 batches | lr 3.49 | ms/batch 14.44 | loss  4.96 | ppl   142.81\n","| epoch   8 |  2200/ 2928 batches | lr 3.49 | ms/batch 15.18 | loss  4.83 | ppl   125.05\n","| epoch   8 |  2400/ 2928 batches | lr 3.49 | ms/batch 14.92 | loss  4.95 | ppl   141.32\n","| epoch   8 |  2600/ 2928 batches | lr 3.49 | ms/batch 14.40 | loss  4.97 | ppl   143.33\n","| epoch   8 |  2800/ 2928 batches | lr 3.49 | ms/batch 14.38 | loss  4.91 | ppl   135.16\n","-----------------------------------------------------------------------------------------\n","| end of epoch   8 | time: 45.00s | valid loss  5.54 | valid ppl   254.37\n","-----------------------------------------------------------------------------------------\n","| epoch   9 |   200/ 2928 batches | lr 3.32 | ms/batch 15.33 | loss  4.95 | ppl   141.40\n","| epoch   9 |   400/ 2928 batches | lr 3.32 | ms/batch 14.89 | loss  4.98 | ppl   145.62\n","| epoch   9 |   600/ 2928 batches | lr 3.32 | ms/batch 14.50 | loss  4.79 | ppl   120.23\n","| epoch   9 |   800/ 2928 batches | lr 3.32 | ms/batch 14.46 | loss  4.87 | ppl   130.25\n","| epoch   9 |  1000/ 2928 batches | lr 3.32 | ms/batch 14.56 | loss  4.83 | ppl   125.58\n","| epoch   9 |  1200/ 2928 batches | lr 3.32 | ms/batch 15.18 | loss  4.88 | ppl   132.20\n","| epoch   9 |  1400/ 2928 batches | lr 3.32 | ms/batch 14.81 | loss  4.90 | ppl   133.92\n","| epoch   9 |  1600/ 2928 batches | lr 3.32 | ms/batch 14.37 | loss  4.93 | ppl   137.84\n","| epoch   9 |  1800/ 2928 batches | lr 3.32 | ms/batch 14.38 | loss  4.90 | ppl   134.64\n","| epoch   9 |  2000/ 2928 batches | lr 3.32 | ms/batch 14.43 | loss  4.89 | ppl   133.15\n","| epoch   9 |  2200/ 2928 batches | lr 3.32 | ms/batch 15.11 | loss  4.76 | ppl   116.54\n","| epoch   9 |  2400/ 2928 batches | lr 3.32 | ms/batch 14.84 | loss  4.88 | ppl   131.15\n","| epoch   9 |  2600/ 2928 batches | lr 3.32 | ms/batch 14.41 | loss  4.89 | ppl   133.36\n","| epoch   9 |  2800/ 2928 batches | lr 3.32 | ms/batch 14.40 | loss  4.83 | ppl   125.51\n","-----------------------------------------------------------------------------------------\n","| end of epoch   9 | time: 45.05s | valid loss  5.52 | valid ppl   250.31\n","-----------------------------------------------------------------------------------------\n","| epoch  10 |   200/ 2928 batches | lr 3.15 | ms/batch 15.34 | loss  4.88 | ppl   131.66\n","| epoch  10 |   400/ 2928 batches | lr 3.15 | ms/batch 14.76 | loss  4.91 | ppl   136.21\n","| epoch  10 |   600/ 2928 batches | lr 3.15 | ms/batch 14.49 | loss  4.72 | ppl   112.67\n","| epoch  10 |   800/ 2928 batches | lr 3.15 | ms/batch 14.40 | loss  4.80 | ppl   121.67\n","| epoch  10 |  1000/ 2928 batches | lr 3.15 | ms/batch 14.58 | loss  4.77 | ppl   118.20\n","| epoch  10 |  1200/ 2928 batches | lr 3.15 | ms/batch 15.23 | loss  4.82 | ppl   124.24\n","| epoch  10 |  1400/ 2928 batches | lr 3.15 | ms/batch 14.69 | loss  4.83 | ppl   125.60\n","| epoch  10 |  1600/ 2928 batches | lr 3.15 | ms/batch 14.42 | loss  4.86 | ppl   129.37\n","| epoch  10 |  1800/ 2928 batches | lr 3.15 | ms/batch 14.37 | loss  4.83 | ppl   125.28\n","| epoch  10 |  2000/ 2928 batches | lr 3.15 | ms/batch 14.60 | loss  4.83 | ppl   125.02\n","| epoch  10 |  2200/ 2928 batches | lr 3.15 | ms/batch 15.16 | loss  4.70 | ppl   109.64\n","| epoch  10 |  2400/ 2928 batches | lr 3.15 | ms/batch 14.86 | loss  4.82 | ppl   123.77\n","| epoch  10 |  2600/ 2928 batches | lr 3.15 | ms/batch 14.40 | loss  4.83 | ppl   125.79\n","| epoch  10 |  2800/ 2928 batches | lr 3.15 | ms/batch 14.31 | loss  4.77 | ppl   118.44\n","-----------------------------------------------------------------------------------------\n","| end of epoch  10 | time: 45.06s | valid loss  5.52 | valid ppl   249.93\n","-----------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"kVn3Ydd3nLWm"},"source":["テストデータセットでモデルを評価する\n","-------------------------------------\n","\n","結果を確認するために、ベストモデルでテスト用データセットを評価する\n"]},{"cell_type":"code","metadata":{"id":"2sL5nPiHPPeU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"71d57c06-2a67-4799-ad28-ce599aa23b8b","executionInfo":{"status":"ok","timestamp":1692263975968,"user_tz":-540,"elapsed":2952,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["test_loss = evaluate(best_model, test_data)\n","test_ppl = math.exp(test_loss)\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | '\n","      f'test ppl {test_ppl:8.2f}')\n","print('=' * 89)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=========================================================================================\n","| End of training | test loss  5.40 | test ppl   221.88\n","=========================================================================================\n"]}]},{"cell_type":"code","source":["evaltext = \"I have enjoyed learning about machine learning so much that I want to continue learning even after the class is over.\"\n","evaltext = tokenizer(evaltext)\n","evaltext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzutwiIEqrN3","executionInfo":{"status":"ok","timestamp":1692263975969,"user_tz":-540,"elapsed":15,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"05e5ce3c-a7cd-4432-cee4-1cba9b05b9ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'have',\n"," 'enjoyed',\n"," 'learning',\n"," 'about',\n"," 'machine',\n"," 'learning',\n"," 'so',\n"," 'much',\n"," 'that',\n"," 'i',\n"," 'want',\n"," 'to',\n"," 'continue',\n"," 'learning',\n"," 'even',\n"," 'after',\n"," 'the',\n"," 'class',\n"," 'is',\n"," 'over',\n"," '.']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["evalvcab = vocab(evaltext)\n","evallen = len(evalvcab)\n","evalvcab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMS0q6rqqt8F","executionInfo":{"status":"ok","timestamp":1692263975969,"user_tz":-540,"elapsed":13,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f46e8906-89c7-4d45-b31c-c716e940f822"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[64,\n"," 49,\n"," 2267,\n"," 2849,\n"," 73,\n"," 1016,\n"," 2849,\n"," 137,\n"," 210,\n"," 16,\n"," 64,\n"," 1315,\n"," 7,\n"," 1259,\n"," 2849,\n"," 249,\n"," 45,\n"," 1,\n"," 526,\n"," 23,\n"," 65,\n"," 3]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["evalmask = torch.zeros(evallen)\n","evalmask = generate_square_subsequent_mask(len(evalvcab)).to(device)"],"metadata":{"id":"oxSTq8By34vC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","evalout = model(torch.tensor(evalvcab).to(device), evalmask.to(device))"],"metadata":{"id":"eF3DlgIKq-UN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evalcode = list()\n","for i in range(evallen):\n","  evalcode.append(torch.argmax(evalout[i], dim=1)[i])"],"metadata":{"id":"TMTdtTYgwt8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","evalcode = list()\n","#np.argsort(evalout[0][0])\n","for i in range(evallen):\n","  evalline = evalout[i][i].to('cpu').detach().numpy().copy()\n","  evalsort = np.argsort(-evalline)\n","  evalcode.append(evalsort)\n","evalcode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h1tW6Q5K0rsh","executionInfo":{"status":"ok","timestamp":1692263975971,"user_tz":-540,"elapsed":12,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e67c24d6-2d5d-4a94-b23f-59b0847b25e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([  11, 1250,  548, ..., 4030, 3103, 1965]),\n"," array([  51,    8,  152, ..., 5881, 9602, 3080]),\n"," array([  19,    1,    6, ..., 2814, 2199, 5244]),\n"," array([   7,    4,   16, ..., 6678, 2725, 6764]),\n"," array([  27,   46,    1, ..., 1165, 3872, 5244]),\n"," array([ 550,  651,    3, ..., 2580, 1965, 2904]),\n"," array([   7,    4,   16, ..., 6678, 2725, 6764]),\n"," array([   2,   16,   42, ..., 2199, 2598, 8846]),\n"," array([    4,     7,     3, ...,  8837, 12919,  5004]),\n"," array([   28,     1,   147, ...,  5853,  4926, 10066]),\n"," array([  11, 1250,  548, ..., 4030, 3103, 1965]),\n"," array([   7,   17,    2, ..., 6575, 4470, 6360]),\n"," array([   33,     1,     0, ..., 10212, 19401,  4356]),\n"," array([   7,   36,    6, ...,  439, 7670, 8943]),\n"," array([   7,    4,   16, ..., 6678, 2725, 6764]),\n"," array([   6,   57,    3, ..., 9433, 7088, 2000]),\n"," array([   1,    8,   27, ..., 1363, 4260, 6001]),\n"," array([  37,   95,    0, ...,  385, 2640,  857]),\n"," array([   3,    4,  436, ..., 6334, 8539, 1574]),\n"," array([   8,    1,    6, ..., 9170, 2796, 1697]),\n"," array([   1,    8,    3, ..., 2966,  614, 6280]),\n"," array([   22,     1,     6, ...,  4904, 11149,  9660])]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["実はあまりうまくいっていない\n","- 気持ちはわからなくもない\n","- 一般にTransformerは大量のデータを用いて時間をかけて学習することで精度が良くなることが知られている"],"metadata":{"id":"ogkE2yYJypcm"}},{"cell_type":"code","source":["import pandas as pd\n","tbl = list()\n","for i, id in enumerate(evalcode):\n","  tbl.append([evalvcab[i], vocab.lookup_token(evalvcab[i]),\n","    id[0].tolist(), vocab.lookup_token(id[0]),\n","    id[1].tolist(), vocab.lookup_token(id[1]),\n","    id[2].tolist(), vocab.lookup_token(id[2]),\n","    id[3].tolist(), vocab.lookup_token(id[3]),\n","    id[4].tolist(), vocab.lookup_token(id[4])])\n","df = pd.DataFrame(tbl)\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739},"id":"t4L_LDsgxjt3","executionInfo":{"status":"ok","timestamp":1692263975971,"user_tz":-540,"elapsed":9,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"7b06f566-2f9f-4e19-8447-cc1251850757"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      0         1    2      3     4       5    6      7     8         9   \\\n","0     64         i   11      '  1250   think  548    see   769       don   \n","1     49      have   51   been     8       a  152  since    83        no   \n","2   2267   enjoyed   19     by     1     the    6     in     3         .   \n","3   2849  learning    7     to     4      of   16   that     2         ,   \n","4     73     about   27    his    46     her    1    the     8         a   \n","5   1016   machine  550   guns   651     gun    3      .     2         ,   \n","6   2849  learning    7     to     4      of   16   that     2         ,   \n","7    137        so    2      ,    16    that   42   they    24        it   \n","8    210      much    4     of     7      to    3      .     2         ,   \n","9     16      that   28     he     1     the  147  could    53       she   \n","10    64         i   11      '  1250   think  548    see   769       don   \n","11  1315      want    7     to    17     for    2      ,    24        it   \n","12     7        to   33     be     1     the    0  <unk>    36     their   \n","13  1259  continue    7     to    36   their    6     in     1       the   \n","14  2849  learning    7     to     4      of   16   that     2         ,   \n","15   249      even    6     in    57    when    3      .    45     after   \n","16    45     after    1    the     8       a   27    his    28        he   \n","17     1       the   37  first    95  second    0  <unk>  1170  republic   \n","18   526     class    3      .     4      of  436  ships     2         ,   \n","19    23        is    8      a     1     the    6     in    39       not   \n","20    65      over    1    the     8       a    3      .     2         ,   \n","21     3         .   22      @     1     the    6     in     9         =   \n","\n","      10         11  \n","0    286         do  \n","1      1        the  \n","2      8          a  \n","3     73      about  \n","4     77        him  \n","5    696      yards  \n","6     73      about  \n","7      3          .  \n","8     61       more  \n","9     24         it  \n","10   286         do  \n","11     6         in  \n","12    49       have  \n","13    17        for  \n","14    73      about  \n","15     2          ,  \n","16  1723  returning  \n","17   370      field  \n","18     5        and  \n","19    30         an  \n","20    27        his  \n","21    28         he  "],"text/html":["\n","\n","  <div id=\"df-8f86c8ee-1d44-44c5-b5e7-a8fbac2a59b1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>64</td>\n","      <td>i</td>\n","      <td>11</td>\n","      <td>'</td>\n","      <td>1250</td>\n","      <td>think</td>\n","      <td>548</td>\n","      <td>see</td>\n","      <td>769</td>\n","      <td>don</td>\n","      <td>286</td>\n","      <td>do</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>49</td>\n","      <td>have</td>\n","      <td>51</td>\n","      <td>been</td>\n","      <td>8</td>\n","      <td>a</td>\n","      <td>152</td>\n","      <td>since</td>\n","      <td>83</td>\n","      <td>no</td>\n","      <td>1</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2267</td>\n","      <td>enjoyed</td>\n","      <td>19</td>\n","      <td>by</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>6</td>\n","      <td>in</td>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>8</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2849</td>\n","      <td>learning</td>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>4</td>\n","      <td>of</td>\n","      <td>16</td>\n","      <td>that</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>73</td>\n","      <td>about</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>73</td>\n","      <td>about</td>\n","      <td>27</td>\n","      <td>his</td>\n","      <td>46</td>\n","      <td>her</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>8</td>\n","      <td>a</td>\n","      <td>77</td>\n","      <td>him</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1016</td>\n","      <td>machine</td>\n","      <td>550</td>\n","      <td>guns</td>\n","      <td>651</td>\n","      <td>gun</td>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>696</td>\n","      <td>yards</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2849</td>\n","      <td>learning</td>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>4</td>\n","      <td>of</td>\n","      <td>16</td>\n","      <td>that</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>73</td>\n","      <td>about</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>137</td>\n","      <td>so</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>16</td>\n","      <td>that</td>\n","      <td>42</td>\n","      <td>they</td>\n","      <td>24</td>\n","      <td>it</td>\n","      <td>3</td>\n","      <td>.</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>210</td>\n","      <td>much</td>\n","      <td>4</td>\n","      <td>of</td>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>61</td>\n","      <td>more</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>16</td>\n","      <td>that</td>\n","      <td>28</td>\n","      <td>he</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>147</td>\n","      <td>could</td>\n","      <td>53</td>\n","      <td>she</td>\n","      <td>24</td>\n","      <td>it</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>64</td>\n","      <td>i</td>\n","      <td>11</td>\n","      <td>'</td>\n","      <td>1250</td>\n","      <td>think</td>\n","      <td>548</td>\n","      <td>see</td>\n","      <td>769</td>\n","      <td>don</td>\n","      <td>286</td>\n","      <td>do</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1315</td>\n","      <td>want</td>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>17</td>\n","      <td>for</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>24</td>\n","      <td>it</td>\n","      <td>6</td>\n","      <td>in</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>33</td>\n","      <td>be</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>0</td>\n","      <td>&lt;unk&gt;</td>\n","      <td>36</td>\n","      <td>their</td>\n","      <td>49</td>\n","      <td>have</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1259</td>\n","      <td>continue</td>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>36</td>\n","      <td>their</td>\n","      <td>6</td>\n","      <td>in</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>17</td>\n","      <td>for</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2849</td>\n","      <td>learning</td>\n","      <td>7</td>\n","      <td>to</td>\n","      <td>4</td>\n","      <td>of</td>\n","      <td>16</td>\n","      <td>that</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>73</td>\n","      <td>about</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>249</td>\n","      <td>even</td>\n","      <td>6</td>\n","      <td>in</td>\n","      <td>57</td>\n","      <td>when</td>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>45</td>\n","      <td>after</td>\n","      <td>2</td>\n","      <td>,</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>45</td>\n","      <td>after</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>8</td>\n","      <td>a</td>\n","      <td>27</td>\n","      <td>his</td>\n","      <td>28</td>\n","      <td>he</td>\n","      <td>1723</td>\n","      <td>returning</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>37</td>\n","      <td>first</td>\n","      <td>95</td>\n","      <td>second</td>\n","      <td>0</td>\n","      <td>&lt;unk&gt;</td>\n","      <td>1170</td>\n","      <td>republic</td>\n","      <td>370</td>\n","      <td>field</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>526</td>\n","      <td>class</td>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>4</td>\n","      <td>of</td>\n","      <td>436</td>\n","      <td>ships</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>5</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>23</td>\n","      <td>is</td>\n","      <td>8</td>\n","      <td>a</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>6</td>\n","      <td>in</td>\n","      <td>39</td>\n","      <td>not</td>\n","      <td>30</td>\n","      <td>an</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>65</td>\n","      <td>over</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>8</td>\n","      <td>a</td>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>2</td>\n","      <td>,</td>\n","      <td>27</td>\n","      <td>his</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>3</td>\n","      <td>.</td>\n","      <td>22</td>\n","      <td>@</td>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>6</td>\n","      <td>in</td>\n","      <td>9</td>\n","      <td>=</td>\n","      <td>28</td>\n","      <td>he</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f86c8ee-1d44-44c5-b5e7-a8fbac2a59b1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-58a0e8ce-4a3a-401b-a3a3-441ec13dc556\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-58a0e8ce-4a3a-401b-a3a3-441ec13dc556')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-58a0e8ce-4a3a-401b-a3a3-441ec13dc556 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8f86c8ee-1d44-44c5-b5e7-a8fbac2a59b1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8f86c8ee-1d44-44c5-b5e7-a8fbac2a59b1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["---\n","> The language of poetry is so dense, so multivalent, that it demands a concentrated act of attention\n","> — and offers its greatest rewards only to those who reread.\n",">\n","> (Ezra Pound)\n",">\n","> 詩の言葉はあまりに密集し、様々な意味を持つため、集中的な意識を払うことが求められる。そして、\n","> 再読をする者にだけ最大の報酬を提供するのである。\n",">\n","> (エズラ・パウンド)\n","---"],"metadata":{"id":"wA0Xs4PhR43f"}},{"cell_type":"markdown","source":["# 課題(Multi-Head Attention)\n","次のMulti-Head Attentionのコードを参考に、Single-Head AttentionをMulti-Head Attentionに入れ替えて実行させて制度などの変化を確認しなさい"],"metadata":{"id":"S-8tc9Wzy5Xv"}},{"cell_type":"code","source":["def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention\n","\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension cannot be equal to the number of heads as modulo.\"\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","        self._reset_parameters()\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        qkv = self.qkv_proj(x)\n","        # Separate Q, K, V from linear output\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n","        q, k, v = qkv.chunk(3, dim=-1)\n","        # Determine value outputs\n","        values, attention = scaled_dot_product(q, k, v, mask=mask)\n","        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n","        values = values.reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o"],"metadata":{"id":"XYkoQ03Qy7F4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 課題(PyTorch Transformerを用いた単語予測)\n","\n","- 日本語による単語補完にトライしてみよう\n"],"metadata":{"id":"U1Px28MVROMY"}}]}